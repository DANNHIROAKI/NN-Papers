#!/usr/bin/env python
import argparse

import h5py
import numpy
import theano
from blocks.graph import ComputationGraph
from blocks.filter import VariableFilter
from blocks.roles import OUTPUT
from blocks.select import Selector
from blocks.serialization import load
from fuel.converters.base import fill_hdf5_file
from fuel.datasets import SVHN
from fuel.streams import DataStream
from fuel.schemes import SequentialScheme
from theano import tensor


def preprocess_svhn(main_loop, save_path):
    h5file = h5py.File(save_path, mode='w')

    ali, = Selector(main_loop.model.top_bricks).select('/ali').bricks
    x = tensor.tensor4('features')
    y = tensor.imatrix('targets')
    params = ali.encoder.apply(x)
    mu = params[:, :ali.encoder._nlat]
    acts = []
    acts += [mu]
    acts += VariableFilter(
        bricks=[ali.encoder.layers[-9], ali.encoder.layers[-6],
                ali.encoder.layers[-3]],
        roles=[OUTPUT])(ComputationGraph([mu]).variables)
    output = tensor.concatenate([act.flatten(ndim=2) for act in acts], axis=1)
    preprocess = theano.function([x, y], [output.flatten(ndim=2), y])

    train_set = SVHN(2, which_sets=('train',), sources=('features', 'targets'))
    train_stream = DataStream.default_stream(
        train_set,
        iteration_scheme=SequentialScheme(train_set.num_examples, 100))
    train_features, train_targets = map(
        numpy.vstack,
        list(zip(*[preprocess(*batch) for batch in
                   train_stream.get_epoch_iterator()])))

    test_set = SVHN(2, which_sets=('test',), sources=('features', 'targets'))
    test_stream = DataStream.default_stream(
        test_set,
        iteration_scheme=SequentialScheme(test_set.num_examples, 100))
    test_features, test_targets = map(
        numpy.vstack,
        list(zip(*[preprocess(*batch) for batch in
                   test_stream.get_epoch_iterator()])))

    data = (('train', 'features', train_features),
            ('test', 'features', test_features),
            ('train', 'targets', train_targets),
            ('test', 'targets', test_targets))
    fill_hdf5_file(h5file, data)
    for i, label in enumerate(('batch', 'feature')):
        h5file['features'].dims[i].label = label
    for i, label in enumerate(('batch', 'index')):
        h5file['targets'].dims[i].label = label

    h5file.flush()
    h5file.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Preprocess ALI latent representations on SVHN for "
                    "semi-supervised learning")
    parser.add_argument("main_loop_path", type=str,
                        help="path to the pickled main loop")
    parser.add_argument("save_path", type=str,
                        help="where to save the preprocessed dataset")
    args = parser.parse_args()

    with open(args.main_loop_path, 'rb') as src:
        preprocess_svhn(load(src), args.save_path)
