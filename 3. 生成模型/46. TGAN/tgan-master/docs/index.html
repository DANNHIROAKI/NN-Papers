<html lang="ja">
<head>
    <meta charset="UTF-8" />
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    <title>Temporal Generative Adversarial Nets with Singular Value Clipping</title>

    <style>
	h1 {
		font-weight:300;
	}


    #authors p{
        margin: 10px;
    }
    #authors span{
        margin-right: 10px;
        margin-left: 10px;
    }

    </style>
  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>
<body>
<center>
    <h1 class="header center orange-text">Temporal Generative Adversarial Nets with Singular Value Clipping</h1>
    <p id="authors">
            <span><a href="mailto:msaito@preferred.jp">Masaki Saito</a>&#8224;</span>
            <span><a href="mailto:matsumoto@preferred.jp">Eiichi Matsumoto</a>&#8224;</span>
            <span><a href="mailto:shunta@preferred.jp">Shunta Saito</a></span>
            <br>
            <small>(&#8224; Equal contributions)</small>
        <p>Preferred Networks Inc.</p><p>ICCV 2017</p>
        <span><a href="https://github.com/pfnet-research/tgan">[Code]</a></span>
        <span><a href="https://arxiv.org/abs/1611.06624">[Paper]</a></span>
        <span><a href="bibtex_iccv2017.txt">[Bibtex]</a></span>
    </p>
    <hr>
</center>
<div class="container">
    <h4 class="center">Abstract</h4>
    In this paper, we propose a generative model, Temporal
    Generative Adversarial Nets (TGAN), which can learn a semantic
    representation of unlabeled videos, and is capable of
    generating videos. Unlike existing Generative Adversarial
    Nets (GAN)-based methods that generate videos with a single
    generator consisting of 3D deconvolutional layers, our
    model exploits two different types of generators: a temporal
    generator and an image generator. The temporal generator
    takes a single latent variable as input and outputs a set of
    latent variables, each of which corresponds to an image
    frame in a video. The image generator transforms a set of
    such latent variables into a video. To deal with instability
    in training of GAN with such advanced networks, we adopt
    a recently proposed model, Wasserstein GAN, and propose
    a novel method to train it stably in an end-to-end manner.
    The experimental results demonstrate the effectiveness of our
    methods.
</div>
<hr>

<div class="container">
    <h4 class="center">Results on Video Generation</h4>
    Following results are random samples generated by TGAN (not cherry picked). <br>
    <table>
        <tr>
            <td><a href="http://www.cs.toronto.edu/~nitish/unsupervised_video/">Moving MNIST</a></td>
            <td><image src="images/mm.gif" width="640"></image></td>
        </tr>
        <tr>
            <td><a href="http://crcv.ucf.edu/data/UCF101.php">UCF-101</a></td>
            <td><image src="images/ucf.gif" width="640"></image></td>
        </tr>
        <tr>
            <td>UCF-101 (label conditional)</td>
            <td><image src="images/ucf_cond.gif" width="640"></image></td>
        </tr>
        <tr>
            <td><a href="http://carlvondrick.com/tinyvideo/">Golf</a></td>
            <td><image src="images/golf.gif" width="640"></image></image></td>
        </tr>
    </table>

</div>
<hr>

<div class="container">
    <h4 class="center">Our Model</h4>
    Our model consists of Generator and Discriminator alike usual GANs.
    Unlike previous video generating GANs using 3D convolution layers,
    we decompose the Generator into 1D convolution + 2D convolution as illustrates below:
    <center><image src="images/diagram.png" width="750"></image></center>
    <br>
    The video generator consists of two generators, the temporal generator \(G_0\) and the image generator \(G_1\).
    The temporal generator \(G_0\) yields a set of latent variables \(z^t_1(t = 1, . . . , T)\) from \(z_0\).
    The image generator \(G_1\) transforms those latent variables \(z^t_1(t = 1, . . . , T)\) and \(z_0\)
    into a video data which has \(T\) frames.
    The discriminator consists of three-dimensional convolutional layers,
    and evaluates whether these frames are from the dataset or the video generator.
</div>
<hr>

<div class="container">
    <h4 class="center">Singular Value Clipping</h4>
    <p>
        We use <a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a> objective to train the model.
        WGAN requires the discriminator to fulfill the K-Lipschitz constraint, and the authors employed
        a parameter clipping method that clamps the weights in the discriminator to [−c, c].
        However, we empirically observed that the tuning of hyper parameter c is severe, and it
        frequently fails in learning under a different situation like our proposed model.
    </p>
    <p>
        Instead of the original WGAN, we add a constraint to all linear and convolutional layers in the discriminator that satisfies
        the spectral norm of weight parameter W is equal or less than one to satisfy K-Lipschitz constraint.
        This means that the singular values of weight matrix are all one or less.
        To implement this, we perform singular value decomposition after parameter update,
        replace all the singular values larger than one with one,
        and reconstruct the parameter with them, which we call Singular Value Clipping (SVC).
    </p>
    <center><image src="images/svc.png" width="300"></image></center>
</div>
<hr>

<div class="container">
    <h4 class="center">Related Work</h4>
    <ul>
        <li><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a>, Goodfellow et al., 2014</li>
        <li><a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a>, Radford et al., 2015</li>
        <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a>, Arjovsky et al., 2017</li>
        <li><a href="https://arxiv.org/abs/1704.00028">Improved Training of Wasserstein GANs</a>, Gulrajani et al., 2017</li>
        <li><a href="https://www.researchgate.net/profile/Takeru_Miyato/publication/318572189_Spectral_Normalization_for_Generative_Adversarial_Networks/links/597218e3a6fdcc83487e0616/Spectral-Normalization-for-Generative-Adversarial-Networks.pdf">Spectral Normalization for Generative Adversarial Networks</a>, Miyato et al., 2017</li>
        <li><a href="https://arxiv.org/abs/1610.00527">Video Pixel Networks</a>, Kalchbrenner et al., 2016</li>
        <li><a href="http://papers.nips.cc/paper/6194-generating-videos-with-scene-dynamics">Generating Videos with Scene Dynamics</a>, Vondrick et al., 2016</li>
    </ul>
</div>
<hr>
<!--
<div class="container">
    <h4 class="center">Data</h4>
    いらないかも？
</div>
<hr>
-->
<div class="container">
    <h4 class="center">Acknowledgements</h4>
        We would like to thank Brian Vogel,
    Jethro Tan, Tommi Kerola, and Zornitsa Kostadinova for
    helpful discussions.
</div>


</body>
</html>
