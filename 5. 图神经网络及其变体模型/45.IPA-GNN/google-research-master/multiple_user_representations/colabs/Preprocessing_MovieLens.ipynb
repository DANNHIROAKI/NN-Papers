{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXCiFaU484VZ"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA14k-JN7e68"
      },
      "source": [
        "Author: Nikhil Mehta  \n",
        "Description: Preprocessing MovieLens Dataset for Training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rNmFklq7cW3"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import html\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import string\n",
        "from typing import Any, Dict, List, Text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from colabtools import adhoc_import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_8iE8GsfUMV"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'movielens/100k'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqr7uxN5lk1J"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load(f\"{dataset_name}-ratings\", split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load(f\"{dataset_name}-movies\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCp4q5nD7g61"
      },
      "outputs": [],
      "source": [
        "movies_np_iterator = list(movies.as_numpy_iterator())\n",
        "ratings_np_iterator = list(ratings.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ0m19xDmcRS"
      },
      "outputs": [],
      "source": [
        "len(ratings_np_iterator), len(movies_np_iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpuC2D-1I51P"
      },
      "outputs": [],
      "source": [
        "ratings_np_iterator[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72jIRkFv8nse"
      },
      "outputs": [],
      "source": [
        "movie_to_genre = dict()\n",
        "genre_to_movie = collections.defaultdict(list)\n",
        "genre_set = []\n",
        "movie_titles = dict()\n",
        "\n",
        "for movie in movies_np_iterator:\n",
        "  movie_genres = list(map(int, movie['movie_genres']))\n",
        "  movie_titles[]\n",
        "  movie_id = int(movie['movie_id'])\n",
        "  \n",
        "  movie_to_genre[movie_id] = movie_genres\n",
        "  genre_set.extend(movie_genres)\n",
        "  \n",
        "  for genre in movie_genres:\n",
        "    genre_to_movie[genre].append(movie_id)\n",
        "\n",
        "genre_set = list(set(genre_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyRte-0iwgOA"
      },
      "outputs": [],
      "source": [
        "genre_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix1wwpcod23i"
      },
      "outputs": [],
      "source": [
        "len(genre_to_movie[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchV44dNmFq1"
      },
      "outputs": [],
      "source": [
        "user_interaction_count = collections.defaultdict(lambda: 0)\n",
        "item_interaction_count = collections.defaultdict(lambda: 0)\n",
        "\n",
        "for rating in ratings.as_numpy_iterator():\n",
        "  \n",
        "  movie_id = int(rating['movie_id'])\n",
        "  user_id = int(rating['user_id'])\n",
        "  \n",
        "  user_interaction_count[user_id] += 1\n",
        "  item_interaction_count[movie_id] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Lh2KsjeX92"
      },
      "outputs": [],
      "source": [
        "# Prepare (user, movie, timestamp) triplets\n",
        "movie_arr = []\n",
        "user_arr = []\n",
        "timestamp_arr = []\n",
        "genre_arr = []\n",
        "user_interests = collections.defaultdict(list)\n",
        "\n",
        "count_ignored = 0\n",
        "\n",
        "MIN_INTERACTION_THRESHOLD = 5\n",
        "for rating in ratings.as_numpy_iterator():\n",
        "\n",
        "  movie_id = int(rating['movie_id'])\n",
        "  user_id = int(rating['user_id'])\n",
        "  min_interaction = min(user_interaction_count[user_id],\n",
        "                        item_interaction_count[movie_id])\n",
        "  \n",
        "  if min_interaction \u003c MIN_INTERACTION_THRESHOLD:\n",
        "    print (f'User interaction count: {user_interaction_count[user_id]}')\n",
        "    print (f'Movie interaction count: {item_interaction_count[movie_id]}')\n",
        "    count_ignored += 1\n",
        "    continue\n",
        "\n",
        "  user_arr.append(user_id)\n",
        "  movie_arr.append(movie_id)\n",
        "  timestamp_arr.append(int(rating['timestamp']))\n",
        "  \n",
        "  # Get genres and user_interests for demonstrating sparsity in data.\n",
        "  genre_arr.extend(rating['movie_genres'])\n",
        "  user_interests[user_id].extend(rating['movie_genres'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYzjTrEktuev"
      },
      "outputs": [],
      "source": [
        "movie_counter = collections.Counter(movie_arr)\n",
        "user_counter = collections.Counter(user_arr)\n",
        "genre_counter = collections.Counter(genre_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU8u_Ccrwk8D"
      },
      "source": [
        "## Item Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wME7hkxxWNo"
      },
      "outputs": [],
      "source": [
        "movie_count = np.array(list(movie_counter.values()))\n",
        "movie_count = movie_count / np.sum(movie_count)\n",
        "plt.bar(range(len(movie_counter)), sorted(movie_count, reverse=True))\n",
        "plt.title('MovieLens: Movie frequency distribution.', fontsize=14)\n",
        "plt.xlabel('Movie ID', fontsize=12)\n",
        "plt.ylabel('Normalized frequency', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyG3TMsbwpYE"
      },
      "source": [
        "# Genre Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovvXhZdmyYp0"
      },
      "outputs": [],
      "source": [
        "genre_count = np.array(list(genre_counter.values()))\n",
        "genre_count = genre_count / np.sum(genre_count)\n",
        "genres = np.arange(len(genre_counter), dtype=int)\n",
        "plt.bar(genres, sorted(genre_count, reverse=True))\n",
        "plt.title('MovieLens: Genre frequency distribution.', fontsize=14)\n",
        "plt.xticks(genres)\n",
        "plt.xlabel('Genre ID', fontsize=12)\n",
        "plt.ylabel('Normalized frequency', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVnYcAf6bdPG"
      },
      "source": [
        "## Check whether top items belong to top genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThtHNumizpcT"
      },
      "outputs": [],
      "source": [
        "N = 100\n",
        "\n",
        "genre_set = list(set(genre_arr))\n",
        "num_genres = 20\n",
        "top_N_movies = movie_counter.most_common(N)\n",
        "\n",
        "ordered_movie_genre_matrix = np.zeros(shape=(num_genres, N))\n",
        "\n",
        "for moview_ix, movie in enumerate(top_N_movies):\n",
        "  movie_id, movie_count = movie\n",
        "  top_movie_genres = movie_to_genre[movie_id]\n",
        "  ordered_movie_genre_matrix[top_movie_genres, moview_ix] = 1\n",
        "\n",
        "fig = plt.figure(figsize=(12, 5))\n",
        "plt.imshow(ordered_movie_genre_matrix)  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rSDBbU2y8Vo"
      },
      "source": [
        "## Distribution of user_interests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B8MGRod8a1-"
      },
      "outputs": [],
      "source": [
        "INTEREST_THRESHOLD = 12\n",
        "\n",
        "# Process user_interetes\n",
        "# User_id -\u003e Genres watched\n",
        "all_interests = []\n",
        "for user_id, interests in user_interests.items():\n",
        "  # Remove genres that occured below a certain threshold\n",
        "  for interest, count in collections.Counter(interests).most_common():\n",
        "    if count \u003c INTEREST_THRESHOLD:\n",
        "      break\n",
        "    else:\n",
        "      all_interests.append(interest)\n",
        "\n",
        "user_interest_counter = collections.Counter(all_interests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQezg7A01pv"
      },
      "outputs": [],
      "source": [
        "user_interest_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9MIEc1UyWkW"
      },
      "outputs": [],
      "source": [
        "plt.bar(range(len(user_interest_counter)), sorted(user_interest_counter.values(), reverse=True))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF4_CywEfZSK"
      },
      "source": [
        "## Check what the distribution of movies look like within a genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9cyUTM_gh93"
      },
      "outputs": [],
      "source": [
        "movie_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0yZjisNwH6_"
      },
      "outputs": [],
      "source": [
        "movie_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Et5p5cIgZTw"
      },
      "outputs": [],
      "source": [
        "genre_ix = 0\n",
        "movies_conditioned_on_genre = []\n",
        "for movie in genre_to_movie[genre_ix]:\n",
        "  movies_conditioned_on_genre.append(movie_counter.get(movie, 0))\n",
        "\n",
        "plt.title(f'MovieLens: Movies in genre {genre_ix+1}', fontsize=14)\n",
        "plt.xlabel('Movies', fontsize=12)\n",
        "plt.ylabel('Normalized frequency', fontsize=12)\n",
        "\n",
        "movies_conditioned_on_genre = movies_conditioned_on_genre / np.sum(movies_conditioned_on_genre)\n",
        "plt.bar(range(len(movies_conditioned_on_genre)),\n",
        "        sorted(movies_conditioned_on_genre, reverse=True))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnu3LW6Iwek0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-zpwwypn3mN"
      },
      "outputs": [],
      "source": [
        "plt.bar(np.arange(10), np.arange(1, 11) ** -(0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmUtVmk7xiR5"
      },
      "outputs": [],
      "source": [
        "interests = np.arange(10)\n",
        "interest_power = 2.0\n",
        "\n",
        "prob = (interests+1) ** (-1.0 * interest_power)\n",
        "prob /= np.sum(prob)\n",
        "\n",
        "plt.bar(interests, prob)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6fWtFXq6mgD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCi_iGe2iZ8b"
      },
      "source": [
        "# Save triplets in txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPwEVbukmRec"
      },
      "outputs": [],
      "source": [
        "root_data = f'/data_path/{dataset_name}'\n",
        "tf.io.gfile.makeDirs(root_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMLkkJOuBjLN"
      },
      "outputs": [],
      "source": [
        "# output_path = os.path.join(root_data,'user_item_time.txt')\n",
        "# print ('Saving at:', output_path)\n",
        "user_movie_time = list(zip(user_arr, movie_arr, timestamp_arr))\n",
        "count_ignored, len(list(user_movie_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Baw3__yIhnTH"
      },
      "outputs": [],
      "source": [
        "def generate_user_item_sequence(user_item_time_triplets) -\u003e Dict[int, List[int]]:\n",
        "  \"\"\"Generates user item sequence from triplets (user, item, time) saved in a file.\n",
        "\n",
        "    Args:\n",
        "      user_item_time_triplets: List of triplets.\n",
        "\n",
        "    Returns:\n",
        "      user_item_seq(Dict): A dictionary that maps from user-id to a list of \n",
        "        items. The item sequence sorted by time.\n",
        "      user_map: A dictionary that maps user to an integer id.\n",
        "      item_map: A dictionary that maps item to Tuple(id, popularity)\n",
        "  \"\"\"\n",
        "\n",
        "  user_map = dict()\n",
        "  user_num = 0\n",
        "  item_map = dict()\n",
        "  item_num = 0\n",
        "  user_item_seq = dict()\n",
        "  \n",
        "  \n",
        "  for line in user_item_time_triplets:\n",
        "    user, item, time = line\n",
        "    \n",
        "    if user in user_map:\n",
        "      user_id = user_map[user]\n",
        "    else:\n",
        "      user_num += 1\n",
        "      user_id = user_num\n",
        "      user_map[user] = user_id\n",
        "      user_item_seq[user_id] = []\n",
        "    \n",
        "    if item in item_map:\n",
        "      item_id = item_map[item][0]\n",
        "      item_map[item][1] += 1\n",
        "    else:\n",
        "      item_num += 1\n",
        "      item_id = item_num\n",
        "      item_map[item] = [item_id, 1]\n",
        "    \n",
        "    user_item_seq[user_id].append([item_id, time])\n",
        "\n",
        "  \n",
        "\n",
        "  print(\"Total number of users with \u003e= {} interactions: {}\".format(\n",
        "      MIN_INTERACTION_THRESHOLD, user_num))\n",
        "  print(\"Total number of items with \u003e= {} interactions: {}\".format(\n",
        "      MIN_INTERACTION_THRESHOLD, item_num))\n",
        "\n",
        "  # Sort reviews in user_item_seq according to time\n",
        "  for user_id in user_item_seq.keys():\n",
        "    user_item_seq[user_id].sort(key=lambda x: x[1])\n",
        "\n",
        "  return user_item_seq, user_map, item_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5nXOtzfjGWA"
      },
      "outputs": [],
      "source": [
        "user_movie_sequences, user_map, movie_map = generate_user_item_sequence(user_movie_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1DtYLkNIY07"
      },
      "outputs": [],
      "source": [
        "user_movie_sequences.values()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I5mcfZNjijF"
      },
      "outputs": [],
      "source": [
        "np.mean([len(seq) for seq in user_movie_sequences.values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGXKcdl3jMkV"
      },
      "outputs": [],
      "source": [
        "def sample_item_negatives(\n",
        "    start, end, num_negative_samples, item_counts, skip_items=None) -\u003e List[int]:\n",
        "  \"\"\"Samples negatives for each item for the standard evaluation protocol. \n",
        "  \n",
        "  Following the standard evaluation protocol followed in the literature, only\n",
        "  the negative samples are used while computing metrics HR@K and NDCG@K. For\n",
        "  negative sampling, only those items are considered which the user has not\n",
        "  interacted with.\n",
        "\n",
        "  Args:\n",
        "    start: Smallest item id to consider while sampling.\n",
        "    end: Largest item id to consider while sampling.\n",
        "    num_negative_samples: Number of negative items to sample.\n",
        "    skip_items: List of items that are to be skipped while sampling. This is\n",
        "      used to skip items that the user has already interacted with.\n",
        "    item_counts: A dictionary mapping from item to their engagement count.\n",
        "    \n",
        "  Returns:\n",
        "    negative_samples: A list consisting of negative sample items.\n",
        "  \"\"\"\n",
        "\n",
        "  negative_samples = []\n",
        "  if skip_items is None:\n",
        "    skip_items = []\n",
        "  \n",
        "  adjusted_item_counts = np.array(\n",
        "      [item_counts[item_ix] if item_ix not in skip_items else 0.0 \n",
        "       for item_ix in range(start, end)])\n",
        "  weights = adjusted_item_counts * 1.0 / sum(adjusted_item_counts)\n",
        "  return list(np.random.choice(\n",
        "      np.arange(start, end), size=num_negative_samples, p=weights, replace=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NIzKWOHt25A"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(root_data, 'user_item_mapped.txt')\n",
        "\n",
        "f = tf.io.gfile.GFile(path, 'w')\n",
        "for user in user_movie_sequences.keys():\n",
        "  for item_time in user_movie_sequences[user]:\n",
        "    f.write('%d %d %s\\n' % (user, item_time[0], str(item_time[1])))\n",
        "f.close()\n",
        "\n",
        "path = os.path.join(root_data, 'item_map.json')\n",
        "with tf.io.gfile.GFile(path, 'w') as f:\n",
        "  json.dump(movie_map, f)\n",
        "\n",
        "path = os.path.join(root_data, 'user_map.json')\n",
        "with tf.io.gfile.GFile(path, 'w') as f:\n",
        "  json.dump(user_map, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbXo3Pl1tlVA"
      },
      "outputs": [],
      "source": [
        "# user_items = [item_time[0] for item_time in user_movie_sequences[user]]\n",
        "\n",
        "# Get item counts\n",
        "item_counts = dict()\n",
        "for item, item_val in movie_map.items():\n",
        "  item_counts[item_val[0]] = item_val[1]\n",
        "\n",
        "# Store neg samples for each user.\n",
        "neg_path = os.path.join(root_data, 'user_neg_items.txt')\n",
        "neg_sample_f = tf.io.gfile.GFile(neg_path, 'w')\n",
        "\n",
        "for user in user_movie_sequences.keys():\n",
        "  user_items = [item_time[0] for item_time in user_movie_sequences[user]]\n",
        "  negative_samples = sample_item_negatives(\n",
        "      1, len(movie_map)+1, 99, item_counts, skip_items=user_items)\n",
        "  neg_sample_f.write(\n",
        "      '%d: %s\\n' % (user, ' '.join(map(str, negative_samples))))\n",
        "neg_sample_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXDzkXPhjv5C"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WH0FqKoj2J-"
      },
      "outputs": [],
      "source": [
        "def load_amazon_category_data(dataset_dir: str,\n",
        "                              max_seq_size: int = 30) -\u003e Dict[Text, Any]:\n",
        "  \"\"\"Loads the Amazon category dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset_dir: Path to preprocessed data directory. The dir should have the\n",
        "      user_item_mapped.txt file and user_neg_items.txt. The user_item_time.txt\n",
        "      file should contain \u003cuser_id\u003e \u003citem_id\u003e \u003ctimestamp\u003e in each line, and the\n",
        "      user_neg_items.txt file should contain negative items for each user, such\n",
        "      that each line contains: \u003cuser_id\u003e: \u003citem_id_1\u003e \u003citem_id_2\u003e...\u003citem_id_N\u003e,\n",
        "        where N is the total negative items sampled.\n",
        "    max_seq_size: Max sequence size. Sequence of length less than max_seq_size,\n",
        "      is padded with zeros from left.\n",
        "\n",
        "  Returns:\n",
        "    dataset: A dictionary containing the dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  logging.info('Loading data from %s.', dataset_dir)\n",
        "  num_users = 0\n",
        "  num_items = 0\n",
        "  user_list = collections.defaultdict(list)\n",
        "  user_neg_item_dict = collections.defaultdict(list)\n",
        "\n",
        "  dataset_path = os.path.join(dataset_dir, 'user_item_mapped.txt')\n",
        "  negative_items_path = os.path.join(dataset_dir, 'user_neg_items.txt')\n",
        "\n",
        "  with tf.io.gfile.GFile(dataset_path, 'r') as fin:\n",
        "    for line in fin:\n",
        "\n",
        "      user_id, item_id, _ = line.rstrip().split(' ')\n",
        "      user_id = int(user_id)\n",
        "      item_id = int(item_id)\n",
        "      num_users = max(user_id, num_users)\n",
        "      num_items = max(item_id, num_items)\n",
        "      user_list[user_id].append(item_id)\n",
        "\n",
        "  logging.info('Num users: %d, Num items: %d', num_users, num_items)\n",
        "  for user in user_list:\n",
        "    item_seq = user_list[user]\n",
        "    item_seq_len = len(item_seq)\n",
        "    if item_seq_len \u003c max_seq_size:\n",
        "      padded_item_seq = [0] * (max_seq_size - item_seq_len)\n",
        "      padded_item_seq.extend(item_seq)\n",
        "    else:\n",
        "      padded_item_seq = item_seq\n",
        "    \n",
        "    user_list[user] = padded_item_seq\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  with tf.io.gfile.GFile(negative_items_path, 'r') as fin:\n",
        "    for line in fin:\n",
        "      user_id, items = line.rstrip().split(':')\n",
        "      user_id = int(user_id)\n",
        "      user_neg_item_dict[user_id] = list(map(int, items.strip().split(' ')))\n",
        "\n",
        "  user_neg_items = []\n",
        "  user_item_sequences = []\n",
        "  for user_id in user_neg_item_dict.keys():\n",
        "    user_neg_items.append(user_neg_item_dict[user_id])\n",
        "    item_seq = user_list[user_id]\n",
        "\n",
        "    if len(item_seq) \u003e max_seq_size:\n",
        "      # Split the sequence into multiple sequences.\n",
        "      stride = 5\n",
        "      split = 0\n",
        "      while (split*stride+max_seq_size \u003c len(item_seq)):\n",
        "        start_ix = split*stride\n",
        "        user_item_sequences.append(item_seq[start_ix:start_ix+max_seq_size])\n",
        "        split += 1\n",
        "      user_item_sequences.append(item_seq[(len(item_seq) - max_seq_size):])\n",
        "    else:\n",
        "      user_item_sequences.append(user_list[user_id])\n",
        "\n",
        "  dataset['user_negative_items'] = user_neg_items\n",
        "  dataset['user_item_sequences'] = user_item_sequences\n",
        "  dataset['num_items'] = num_items\n",
        "  dataset['num_users'] = num_users\n",
        "  dataset['items'] = range(1, num_items + 1)\n",
        "  dataset['max_seq_size'] = max_seq_size\n",
        "  logging.info('Data loaded from %s.', dataset_dir)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kC9xuhytxD7"
      },
      "outputs": [],
      "source": [
        "data = load_amazon_category_data(dataset_dir = root_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoWfdnVNuF4i"
      },
      "outputs": [],
      "source": [
        "user_item_sequences = np.array(data['user_item_sequences'])\n",
        "len(user_item_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kbyMdJJu1ee"
      },
      "outputs": [],
      "source": [
        "user_item_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r7XRIkE10yw"
      },
      "outputs": [],
      "source": [
        "test_next_item = [seq[-1] for seq in user_item_sequences]\n",
        "train_next_item = [seq[-2] for seq in user_item_sequences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY5jaPANtyUm"
      },
      "outputs": [],
      "source": [
        "train_counter = collections.Counter(train_next_item)\n",
        "test_counter = collections.Counter(test_next_item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB05L7n5upUl"
      },
      "outputs": [],
      "source": [
        "len(train_counter), len(test_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pfY3DXmurtr"
      },
      "outputs": [],
      "source": [
        "movie_id_list = sorted(list(movie_map.keys()))\n",
        "train_count = np.array([train_counter.get(movie_id, 0) for movie_id in movie_id_list])\n",
        "train_count = train_count / np.sum(train_count)\n",
        "\n",
        "test_count = np.array([test_counter.get(movie_id, 0) for movie_id in movie_id_list])\n",
        "test_count = test_count / np.sum(test_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twK2pxigxRIh"
      },
      "outputs": [],
      "source": [
        "sorted_indices = np.argsort(-1*train_count)\n",
        "plt.bar(movie_id_list, train_count[sorted_indices], color='coral', alpha=1.0, label='Train')\n",
        "plt.bar(movie_id_list, test_count[sorted_indices], color='teal', alpha=0.8, label='Test')\n",
        "plt.ylabel('Normalized frequency', fontsize=12)\n",
        "plt.xlabel('Movies', fontsize=12)\n",
        "plt.legend()\n",
        "plt.title('MovieLens: Train vs. Test Item Distribution.', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRmKdizmHfGo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DU8u_Ccrwk8D",
        "AVnYcAf6bdPG",
        "1rSDBbU2y8Vo",
        "lF4_CywEfZSK"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Preprocessing-MovieLens.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1BYQgM2_IFSVosYcJNBxh4-YRJMDseyGR",
          "timestamp": 1631309254861
        },
        {
          "file_id": "1GnmoesYMDWwnwdxj7Su6NfzYbxGXnlLl",
          "timestamp": 1626191701823
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
