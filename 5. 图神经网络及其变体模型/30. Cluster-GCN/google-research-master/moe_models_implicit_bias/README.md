# Project exploring the implicit bias of Mixture of Expert (MoE) Models

This document describes the functionality and experiments that study the
implicit bias of MoE models.
This includes the following set of broad experiments.
1. [Mixture of Gaussians]
2. [Mixture of Subspaces]
3. [CIFAR-10 images]
4. [Exploring Sparsity Naturally Occuring in Deep Models]

# From google-research/
To run the experiment corresponding to folder <xyz> run
python -m moe_models_implicit_bias/<xyz>/main.py