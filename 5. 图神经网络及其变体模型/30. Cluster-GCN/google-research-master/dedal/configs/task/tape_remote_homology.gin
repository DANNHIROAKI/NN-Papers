from __gin__ import dynamic_registration

import tensorflow as tf
import tensorflow_addons as tfa

from dedal import multi_task
from dedal import vocabulary
from dedal.data import align_transforms
from dedal.data import builder
from dedal.data import loaders
from dedal.data import specs
from dedal.data import transforms
from dedal.train import checkpoint
from dedal.train import learning_rate_schedules
from dedal.train import logger
from dedal.train import losses
from dedal.train import metrics
from dedal.train import training_loop
from dedal.models import dedal
from dedal.models import lenses


# -----------------------------------------------------------------------------
# REQUIRED GIN BINDINGS.
# -----------------------------------------------------------------------------

# Path to directory containing data for all five TAPE tasks. Must contain
# subdirectories named secondary_structure, remote_homology, proteinnet,
# fluorescence and stability.
TAPE_DATA_DIR = %gin.REQUIRED


# -----------------------------------------------------------------------------
# TOP-LEVEL TRAINING LOOP CONFIG.
# -----------------------------------------------------------------------------

NUM_CLASSES = 1195

PERIOD = 96
MAX_TO_KEEP = 25

vocabulary.get_default:
  vocab = %vocabulary.seqio_vocab

training_loop.TrainingLoop:
  dataset_builder = @specs.make_tape_builder()
  logger_cls = @logger.Logger
  model_cls = @dedal.Dedal
  loss_fn = @losses.MultiTaskLoss()
  optimizer_cls = @tfa.optimizers.AdamW
  batch_size = 128
  num_steps = 150_000
  num_eval_steps = 32
  num_steps_per_train_iteration = 16

# -----------------------------------------------------------------------------
# DATA PIPELINE.
# -----------------------------------------------------------------------------

specs.make_tape_builder:
  root_dir = %TAPE_DATA_DIR
  task = 'remote_homology'
  target = 'fold_label'
  max_len = 512

# -----------------------------------------------------------------------------
# OUTPUT HEADS AND FINETUNING.
# -----------------------------------------------------------------------------

dedal.Dedal:
  aligner_cls = None
  heads_cls = @model/heads/multi_task.Backbone()
  
model/heads/multi_task.Backbone:
  embeddings = [@lenses.MLP]

lenses.MLP:
  output_size = %NUM_CLASSES
  post_pooling_layer_norm = True
  post_pooling_hidden = [512]
  post_pooling_dropout = 0.5
  pooling_cls = @lenses.GlobalAttentionPooling1D

lenses.GlobalAttentionPooling1D:
  normalize = True
  dropout = 0.1

# -----------------------------------------------------------------------------
# MULTI-TASK LOSS.
# -----------------------------------------------------------------------------

losses.MultiTaskLoss:
  losses = @loss/multi_task.Backbone()

loss/multi_task.Backbone:
  embeddings = [@tf.keras.losses.CategoricalCrossentropy()]

tf.keras.losses.CategoricalCrossentropy:
  name = 'bce_loss'
  from_logits = True
  label_smoothing = 0.0
  reduction = 'none'

# -----------------------------------------------------------------------------
# OPTIMIZER.
# -----------------------------------------------------------------------------

tfa.optimizers.AdamW:
  learning_rate = 1e-5
  weight_decay = 0.0
  epsilon = 1e-08
  clipnorm = 1.0

# -----------------------------------------------------------------------------
# METRICS.
# -----------------------------------------------------------------------------

logger.Logger:
  scalars = @scalars/multi_task.Backbone()
  every = %PERIOD

scalars/multi_task.Backbone:
  embeddings = [
      [
          @tf.keras.metrics.CategoricalAccuracy,
          @tf.keras.metrics.TopKCategoricalAccuracy,
      ]
  ]

tf.keras.metrics.TopKCategoricalAccuracy:
  name = 'top_5_categorical_accuracy'
  k = 5

# -----------------------------------------------------------------------------
# CHECKPOINTING.
# -----------------------------------------------------------------------------

checkpoint.Checkpointer:
  save_every = %PERIOD
  max_to_keep = %MAX_TO_KEEP
