{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T23:23:33.717359Z",
     "start_time": "2019-12-11T23:23:33.679016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "import fasttext\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load WordNet 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T22:07:20.502751Z",
     "start_time": "2019-12-11T22:07:18.549629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82115/82115 [00:00<00:00, 157245.13it/s]\n",
      "  0%|          | 0/13767 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in noun taxonomy: 82115\n",
      "Number of edges in noun taxonomy: 75850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13767/13767 [00:00<00:00, 161900.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in verb taxonomy: 13767\n",
      "Number of edges in verb taxonomy: 13239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_synset2def = {}\n",
    "n_hypernymy = []\n",
    "n_hypo2hyper = {}\n",
    "n_synset2lemma = {}\n",
    "for synset in tqdm(list(wn.all_synsets('n'))):\n",
    "    n_synset2def[synset.name()] = synset.definition()\n",
    "    n_hypernymy.extend([[synset.name(), hypo.name()] for hypo in synset.hyponyms()])\n",
    "    n_hypo2hyper[synset.name()] = [ele.name() for ele in synset.hypernyms()]\n",
    "    n_synset2lemma[synset.name()] = synset.name().split(\".\")[0]\n",
    "print(f\"Number of nodes in noun taxonomy: {len(n_synset2def)}\")\n",
    "print(f\"Number of edges in noun taxonomy: {len(n_hypernymy)}\")\n",
    "    \n",
    "v_synset2def = {}\n",
    "v_hypernymy = []\n",
    "v_hypo2hyper = {}\n",
    "v_synset2lemma = {}\n",
    "for synset in tqdm(list(wn.all_synsets('v'))):\n",
    "    v_synset2def[synset.name()] = synset.definition()\n",
    "    v_hypernymy.extend([[synset.name(), hypo.name()] for hypo in synset.hyponyms()])\n",
    "    v_hypo2hyper[synset.name()] = [ele.name() for ele in synset.hypernyms()]\n",
    "    v_synset2lemma[synset.name()] = synset.name().split(\".\")[0]\n",
    "print(f\"Number of nodes in verb taxonomy: {len(v_synset2def)}\")\n",
    "print(f\"Number of edges in verb taxonomy: {len(v_hypernymy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SemEval Task 14 Data and merge with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T22:07:33.059537Z",
     "start_time": "2019-12-11T22:07:32.997665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun] The merged place has multiple parents: train.withdef.209 -- ['hand_tool.n.01', 'lever.n.01']\n",
      "[Verb] The merged place has no parent: train.withdef.241\n",
      "[Noun] The merged place has multiple parents: train.withdef.245 -- ['expulsion.n.03', 'reflex.n.01']\n",
      "[Noun] The merged place has no parent: train.withdef.356\n",
      "[Noun] The merged place has multiple parents: val.withdef.4 -- ['name.n.01', 'slang.n.02']\n",
      "[Verb] The merged place has no parent: val.withdef.51\n",
      "[Verb] The merged place has no parent: val.withdef.53\n",
      "[Verb] The merged place has no parent: val.withdef.121\n",
      "[Noun] The merged place has no parent: test.test.536\n",
      "[Noun] The merged place has multiple parents: test.test.545 -- ['physical_condition.n.01', 'waking.n.01']\n"
     ]
    }
   ],
   "source": [
    "train_data = [\"../data/semeval-2016-task-14/data/training/training.data.tsv\",\n",
    "              \"../data/semeval-2016-task-14/keys/gold/training/training.key.tsv\"]\n",
    "trial_data = [\"../data/semeval-2016-task-14/data/trial/trial.data.tsv\",\n",
    "              \"../data/semeval-2016-task-14/keys/gold/trial/trial.key.tsv\"]\n",
    "test_data = [\"../data/semeval-2016-task-14/data/test/test.data.tsv\",\n",
    "             \"../data/semeval-2016-task-14/keys/gold/test/test.key.tsv\"]\n",
    "\n",
    "\n",
    "for i, dataset in enumerate([train_data, trial_data, test_data]):\n",
    "    if i == 0:\n",
    "        prefix = \"train\"\n",
    "    elif i == 1:\n",
    "        prefix = \"val\"\n",
    "    else:\n",
    "        prefix = \"test\"\n",
    "    with open(dataset[0], \"r\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                lemma, pos, idx, def_sent, url = line.split(\"\\t\")\n",
    "                lemma = \"_\".join(lemma.split())\n",
    "                idx = prefix+\".\"+idx\n",
    "                if pos == \"noun\":  # noun taxonomy\n",
    "                    n_synset2def[idx] = def_sent\n",
    "                    n_synset2lemma[idx] = lemma\n",
    "                else:\n",
    "                    v_synset2def[idx] = def_sent\n",
    "                    v_synset2lemma[idx] = lemma\n",
    "    with open(dataset[1], \"r\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                idx, correct_position, op = line.split(\"\\t\")\n",
    "                wnid = wn.synset(correct_position.replace(\"#\", \".\").replace(' ', \"_\")).name()\n",
    "                pos = wnid.split(\".\")[1]\n",
    "                if pos == \"n\": # noun taxonomy\n",
    "                    if op == \"attach\":  \n",
    "                        n_hypernymy.append([wnid, prefix+\".\"+idx])\n",
    "                    else:\n",
    "                        if len(n_hypo2hyper[wnid]) < 1:\n",
    "                            print(f\"[Noun] The merged place has no parent: {prefix}.{idx}\")\n",
    "                            n_hypernymy.append([wnid, prefix+\".\"+idx])\n",
    "                        elif len(n_hypo2hyper[wnid]) > 1:\n",
    "                            print(f\"[Noun] The merged place has multiple parents: {prefix}.{idx} -- {n_hypo2hyper[wnid]}\")\n",
    "                            n_hypernymy.extend([[hyper, prefix+\".\"+idx] for hyper in n_hypo2hyper[wnid]])                    \n",
    "                        else:\n",
    "                            n_hypernymy.append([n_hypo2hyper[wnid][0], prefix+\".\"+idx])\n",
    "                else:\n",
    "                    if op == \"attach\":\n",
    "                        v_hypernymy.append([wnid, prefix+\".\"+idx])\n",
    "                    else:\n",
    "                        if len(v_hypo2hyper[wnid]) < 1:\n",
    "                            print(f\"[Verb] The merged place has no parent: {prefix}.{idx}\")\n",
    "                            v_hypernymy.append([wnid, prefix+\".\"+idx])\n",
    "                        elif len(v_hypo2hyper[wnid]) > 1:\n",
    "                            print(f\"[Verb] The merged place has multiple parents: {prefix}.{idx} -- {v_hypo2hyper[wnid]}\")\n",
    "                            v_hypernymy.extend([[hyper, prefix+\".\"+idx] for hyper in v_hypo2hyper[wnid]])                                                \n",
    "                        else:\n",
    "                            v_hypernymy.append([v_hypo2hyper[wnid][0], prefix+\".\"+idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save term/taxo files and define term lemmas/definition_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T22:07:37.191695Z",
     "start_time": "2019-12-11T22:07:37.161845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A proband is an individual being studied or reported on. A proband is usually the first affected individual in a family who brings a genetic disorder to the attention of the medical community.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_synset2def['train.withdef.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T23:00:19.266943Z",
     "start_time": "2019-12-11T23:00:19.219039Z"
    }
   },
   "outputs": [],
   "source": [
    "for n_synset in n_synset2lemma.keys():\n",
    "    if n_synset[0].isupper():\n",
    "        print(n_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T22:09:26.948863Z",
     "start_time": "2019-12-11T22:09:26.722108Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = \"/datadrive/structure_expan/data/semeval-2016-task-14-new/\"\n",
    "\n",
    "# noun\n",
    "with open(os.path.join(dir_path, \"wordnet_noun.terms\"), \"w\") as fout:\n",
    "    for synset in n_synset2lemma:\n",
    "        fout.write(f\"{synset}\\t{n_synset2lemma[synset]}||{synset}\\n\")\n",
    "        \n",
    "with open(os.path.join(dir_path, \"wordnet_noun.taxo\"), \"w\") as fout:\n",
    "    for hypernymy in n_hypernymy:\n",
    "        fout.write(f\"{hypernymy[0]}\\t{hypernymy[1]}\\n\")\n",
    "\n",
    "with open(os.path.join(dir_path, \"wordnet_noun.definitions\"), \"w\") as fout:\n",
    "    for synset in n_synset2lemma:\n",
    "        fout.write(f\"{synset}\\t{n_synset2def[synset]}\\n\")\n",
    "\n",
    "# verb\n",
    "with open(os.path.join(dir_path, \"wordnet_verb.terms\"), \"w\") as fout:\n",
    "    for synset in v_synset2lemma:\n",
    "        fout.write(f\"{synset}\\t{v_synset2lemma[synset]}||{synset}\\n\")\n",
    "        \n",
    "with open(os.path.join(dir_path, \"wordnet_verb.taxo\"), \"w\") as fout:\n",
    "    for hypernymy in v_hypernymy:\n",
    "        fout.write(f\"{hypernymy[0]}\\t{hypernymy[1]}\\n\")\n",
    "        \n",
    "with open(os.path.join(dir_path, \"wordnet_verb.definitions\"), \"w\") as fout:\n",
    "    for synset in v_synset2lemma:\n",
    "        fout.write(f\"{synset}\\t{v_synset2def[synset]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate and save term initial (embedding) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T22:52:36.500955Z",
     "start_time": "2019-11-29T22:52:35.983067Z"
    }
   },
   "outputs": [],
   "source": [
    "fasttext = fasttext.load_model(\"/datadrive/fastText-pretrained-embedding/cc.en.300.bin\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# bert = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 1 (bert_base_uncased_defonly): BERT embedding of definition sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T03:53:25.874907Z",
     "start_time": "2019-11-28T03:15:33.142767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-jishen/anaconda3/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=100\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "n_synset_list = []\n",
    "n_synset_definition_list = []\n",
    "for k,v in n_synset2def.items():\n",
    "    n_synset_list.append(k)\n",
    "    n_synset_definition_list.append(v)\n",
    "    \n",
    "n_bert_def_only_embed = bc.encode(n_synset_definition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T04:10:48.961825Z",
     "start_time": "2019-11-28T04:04:24.201209Z"
    }
   },
   "outputs": [],
   "source": [
    "v_synset_list = []\n",
    "v_synset_definition_list = []\n",
    "for k,v in v_synset2def.items():\n",
    "    v_synset_list.append(k)\n",
    "    v_synset_definition_list.append(v)\n",
    "    \n",
    "v_bert_def_only_embed = bc.encode(v_synset_definition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T04:58:23.037780Z",
     "start_time": "2019-11-28T04:57:37.459888Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = \"/datadrive/structure_expan/data/semeval-2016-task-14/\"\n",
    "\n",
    "# noun\n",
    "with open(os.path.join(dir_path, \"wordnet_noun.terms.bert_uncased_defonly.embed\"), \"w\") as fout:\n",
    "    fout.write(f\"{n_bert_def_only_embed.shape[0]} {n_bert_def_only_embed.shape[1]}\\n\")\n",
    "    for i, synset in enumerate(n_synset_list):\n",
    "        synset_embed = n_bert_def_only_embed[i, :]\n",
    "        synset_embed_string = \" \".join([str(e) for e in synset_embed])\n",
    "        fout.write(f\"{synset} {synset_embed_string}\\n\")\n",
    "        \n",
    "# verb\n",
    "with open(os.path.join(dir_path, \"wordnet_verb.terms.bert_uncased_defonly.embed\"), \"w\") as fout:\n",
    "    fout.write(f\"{v_bert_def_only_embed.shape[0]} {v_bert_def_only_embed.shape[1]}\\n\")\n",
    "    for i, synset in enumerate(v_synset_list):\n",
    "        synset_embed = v_bert_def_only_embed[i, :]\n",
    "        synset_embed_string = \" \".join([str(e) for e in synset_embed])\n",
    "        fout.write(f\"{synset} {synset_embed_string}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T22:35:46.088873Z",
     "start_time": "2019-11-29T22:35:46.048380Z"
    }
   },
   "source": [
    "#### Embedding 2,3 (fasttext_defonly|fasttext_all_average): fasttext based embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T22:37:40.087857Z",
     "start_time": "2019-11-29T22:37:40.051081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never_event'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_synset2lemma['test.test.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T22:40:37.124642Z",
     "start_time": "2019-11-29T22:40:37.094067Z"
    }
   },
   "outputs": [],
   "source": [
    "def obtain_fasttext_embed(model, token_list, vector_size=300):\n",
    "    embed = np.zeros(vector_size)\n",
    "    cnt = 0\n",
    "    for token in token_list:\n",
    "        if token in model:  # whenever a character ngrams appaer in the token\n",
    "            embed += model.get_word_vector(token)\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        embed /= cnt\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T23:36:04.120092Z",
     "start_time": "2019-11-29T22:52:43.071110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83073 [00:00<?, ?it/s]/home/t-jishen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "100%|██████████| 83073/83073 [43:21<00:00, 31.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "n_synset_list = []\n",
    "n_synset_defonly_embed_list = []\n",
    "n_synset_average_defonly_plus_lemma_embed_list = []\n",
    "n_synset_weighted_average_defonly_plus_lemma_embed_list = []\n",
    "n_synset_defonly_plus_lemma_with_fwfs_embed_list = []\n",
    "\n",
    "for n_id in tqdm(n_synset2lemma):\n",
    "    n_synset_list.append(n_id)\n",
    "    \n",
    "    # lemma embedding\n",
    "    lemma_tok_list = re.split(r\"\\s|, |-\", n_synset2lemma[n_id].lower())\n",
    "    lemma_embed = obtain_fasttext_embed(fasttext, lemma_tok_list)\n",
    "    \n",
    "    # definition embedding\n",
    "    def_pos_list = []\n",
    "    def_tok_list = []\n",
    "    for token in nlp(n_synset2def[n_id]):\n",
    "        def_pos_list.append(token.pos_)\n",
    "        def_tok_list.append(token.text)\n",
    "    def_embed = obtain_fasttext_embed(fasttext, def_tok_list)\n",
    "    \n",
    "    # first same pos tagged word embedding\n",
    "    first_same_pos_tagged_word = \"\"\n",
    "    for tok, pos in zip(def_tok_list, def_pos_list):\n",
    "        if pos == \"NOUN\":\n",
    "            first_same_pos_tagged_word = tok\n",
    "            break\n",
    "    if first_same_pos_tagged_word != \"\":\n",
    "        first_same_pos_tagged_word_tok_list = re.split(r\"\\s|, |-\", first_same_pos_tagged_word.lower())\n",
    "        first_same_pos_tagged_word_tok_embed = obtain_fasttext_embed(fasttext, first_same_pos_tagged_word_tok_list)\n",
    "    else:\n",
    "        first_same_pos_tagged_word_tok_embed = \"\"\n",
    "    \n",
    "    # embedding based only on definition\n",
    "    n_synset_defonly_embed_list.append(def_embed)\n",
    "    \n",
    "    # embedding based on half lemma name and half definition\n",
    "    n_synset_average_defonly_plus_lemma_embed_list.append((lemma_embed+def_embed)/2)\n",
    "    \n",
    "    # embedding based on 0.25 lemma name and 0.75 definition\n",
    "    n_synset_weighted_average_defonly_plus_lemma_embed_list.append(0.25*lemma_embed + 0.75*def_embed)\n",
    "\n",
    "    # embedding based on lemma name, definition, and first same pos tagged word embedding\n",
    "    if first_same_pos_tagged_word_tok_embed == \"\":\n",
    "        n_synset_defonly_plus_lemma_with_fwfs_embed_list.append((lemma_embed + 2*def_embed) / 3)\n",
    "    else:\n",
    "        n_synset_defonly_plus_lemma_with_fwfs_embed_list.append((lemma_embed + def_embed + first_same_pos_tagged_word_tok_embed) / 3)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T23:40:48.841855Z",
     "start_time": "2019-11-29T23:36:59.725388Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13936 [00:00<?, ?it/s]/home/t-jishen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "100%|██████████| 13936/13936 [03:49<00:00, 60.84it/s]\n"
     ]
    }
   ],
   "source": [
    "v_synset_list = []\n",
    "v_synset_defonly_embed_list = []\n",
    "v_synset_average_defonly_plus_lemma_embed_list = []\n",
    "v_synset_weighted_average_defonly_plus_lemma_embed_list = []\n",
    "v_synset_defonly_plus_lemma_with_fwfs_embed_list = []\n",
    "\n",
    "for v_id in tqdm(v_synset2lemma):\n",
    "    v_synset_list.append(v_id)\n",
    "    \n",
    "    # lemma embedding\n",
    "    lemma_tok_list = re.split(r\"\\s|, |-\", v_synset2lemma[v_id].lower())\n",
    "    lemma_embed = obtain_fasttext_embed(fasttext, lemma_tok_list)\n",
    "    \n",
    "    # definition embedding\n",
    "    def_pos_list = []\n",
    "    def_tok_list = []\n",
    "    for token in nlp(v_synset2def[v_id]):\n",
    "        def_pos_list.append(token.pos_)\n",
    "        def_tok_list.append(token.text)\n",
    "    def_embed = obtain_fasttext_embed(fasttext, def_tok_list)\n",
    "    \n",
    "    # first same pos tagged word embedding\n",
    "    first_same_pos_tagged_word = \"\"\n",
    "    for tok, pos in zip(def_tok_list, def_pos_list):\n",
    "        if pos == \"VERB\":\n",
    "            first_same_pos_tagged_word = tok\n",
    "            break\n",
    "    if first_same_pos_tagged_word != \"\":\n",
    "        first_same_pos_tagged_word_tok_list = re.split(r\"\\s|, |-\", first_same_pos_tagged_word.lower())\n",
    "        first_same_pos_tagged_word_tok_embed = obtain_fasttext_embed(fasttext, first_same_pos_tagged_word_tok_list)\n",
    "    else:\n",
    "        first_same_pos_tagged_word_tok_embed = \"\"\n",
    "    \n",
    "    # embedding based only on definition\n",
    "    v_synset_defonly_embed_list.append(def_embed)\n",
    "    \n",
    "    # embedding based on half lemma name and half definition\n",
    "    v_synset_average_defonly_plus_lemma_embed_list.append((lemma_embed+def_embed)/2)\n",
    "    \n",
    "    # embedding based on 0.25 lemma name and 0.75 definition\n",
    "    v_synset_weighted_average_defonly_plus_lemma_embed_list.append(0.25*lemma_embed + 0.75*def_embed)\n",
    "\n",
    "    # embedding based on lemma name, definition, and first same pos tagged word embedding\n",
    "    if first_same_pos_tagged_word_tok_embed == \"\":\n",
    "        v_synset_defonly_plus_lemma_with_fwfs_embed_list.append((lemma_embed + 2*def_embed) / 3)\n",
    "    else:\n",
    "        v_synset_defonly_plus_lemma_with_fwfs_embed_list.append((lemma_embed + def_embed + first_same_pos_tagged_word_tok_embed) / 3)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T23:42:54.154185Z",
     "start_time": "2019-11-29T23:42:52.773555Z"
    }
   },
   "outputs": [],
   "source": [
    "n_synset_defonly_embed_list = np.array(n_synset_defonly_embed_list)\n",
    "n_synset_average_defonly_plus_lemma_embed_list = np.array(n_synset_average_defonly_plus_lemma_embed_list)\n",
    "n_synset_weighted_average_defonly_plus_lemma_embed_list = np.array(n_synset_weighted_average_defonly_plus_lemma_embed_list)\n",
    "n_synset_defonly_plus_lemma_with_fwfs_embed_list = np.array(n_synset_defonly_plus_lemma_with_fwfs_embed_list)\n",
    "\n",
    "v_synset_defonly_embed_list = np.array(v_synset_defonly_embed_list)\n",
    "v_synset_average_defonly_plus_lemma_embed_list = np.array(v_synset_average_defonly_plus_lemma_embed_list)\n",
    "v_synset_weighted_average_defonly_plus_lemma_embed_list = np.array(v_synset_weighted_average_defonly_plus_lemma_embed_list)\n",
    "v_synset_defonly_plus_lemma_with_fwfs_embed_list = np.array(v_synset_defonly_plus_lemma_with_fwfs_embed_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T23:45:01.483201Z",
     "start_time": "2019-11-29T23:42:55.488261Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = \"/datadrive/structure_expan/data/semeval-2016-task-14/\"\n",
    "\n",
    "# noun\n",
    "embed_suffices = [\"fasttext_mode1\", \"fasttext_mode2\", \"fasttext_mode3\", \"fasttext_mode4\"]\n",
    "embed_lists = [\n",
    "    n_synset_defonly_embed_list, \n",
    "    n_synset_average_defonly_plus_lemma_embed_list, \n",
    "    n_synset_weighted_average_defonly_plus_lemma_embed_list,\n",
    "    n_synset_defonly_plus_lemma_with_fwfs_embed_list\n",
    "]\n",
    "for suffix, embed_list in zip(embed_suffices, embed_lists):\n",
    "    with open(os.path.join(dir_path, f\"wordnet_noun.terms.{suffix}.embed\"), \"w\") as fout:\n",
    "        fout.write(f\"{embed_list.shape[0]} {embed_list.shape[1]}\\n\")\n",
    "        for i, synset in enumerate(n_synset_list):\n",
    "            synset_embed = embed_list[i, :]\n",
    "            synset_embed_string = \" \".join([str(e) for e in synset_embed])\n",
    "            fout.write(f\"{synset} {synset_embed_string}\\n\")\n",
    "        \n",
    "# verb\n",
    "embed_suffices = [\"fasttext_mode1\", \"fasttext_mode2\", \"fasttext_mode3\", \"fasttext_mode4\"]\n",
    "embed_lists = [\n",
    "    v_synset_defonly_embed_list, \n",
    "    v_synset_average_defonly_plus_lemma_embed_list, \n",
    "    v_synset_weighted_average_defonly_plus_lemma_embed_list,\n",
    "    v_synset_defonly_plus_lemma_with_fwfs_embed_list\n",
    "]\n",
    "for suffix, embed_list in zip(embed_suffices, embed_lists):\n",
    "    with open(os.path.join(dir_path, f\"wordnet_verb.terms.{suffix}.embed\"), \"w\") as fout:\n",
    "        fout.write(f\"{embed_list.shape[0]} {embed_list.shape[1]}\\n\")\n",
    "        for i, synset in enumerate(v_synset_list):\n",
    "            synset_embed = embed_list[i, :]\n",
    "            synset_embed_string = \" \".join([str(e) for e in synset_embed])\n",
    "            fout.write(f\"{synset} {synset_embed_string}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
