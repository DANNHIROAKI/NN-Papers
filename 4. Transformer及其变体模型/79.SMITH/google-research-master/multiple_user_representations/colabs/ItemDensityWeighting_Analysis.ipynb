{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE0uozr_8yja"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n482lHrfzwpt"
      },
      "source": [
        "\n",
        "Author: Nikhil Mehta  \n",
        "Description: Analysis of Item Density Weighting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMvUecptL5zx"
      },
      "outputs": [],
      "source": [
        "# %reset\n",
        "import collections\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "from typing import Dict, List, Text, Tuple, Union, Optional, Any\n",
        "\n",
        "from absl import logging\n",
        "from colabtools import drive\n",
        "from colabtools import adhoc_import\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.colors import to_rgb, to_rgba\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "from multiple_user_representations.synthetic_data import util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q0vbj7YMOZm"
      },
      "outputs": [],
      "source": [
        "def load_dataset(root_dir, dataset_path, alpha_str, is_npz = False):\n",
        "\n",
        "  data_path = '{}/{}/synthetic_data_{}'.format(\n",
        "      root_dir, dataset_path, alpha_str)\n",
        "  data = util.load_data(data_path)\n",
        "  item_clusters = data['item_clusters']\n",
        "  all_items = data['items']\n",
        "  test_cluster = item_clusters[data['user_item_sequences'][:, -1]]\n",
        "  user_item_sequences = data['user_item_sequences']\n",
        "  user_interests = data.get('user_interests', None)\n",
        "    \n",
        "  return (item_clusters, all_items, user_interests, test_cluster, user_item_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPr68qVoMXIq"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.6_gamma0.3/'\n",
        "results_path = 'results/d2/density_smoothing/reruns_3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imr8ovizNqQk"
      },
      "source": [
        "## Visualize Final Embedding Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVULFknnN-3b"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "\n",
        "seed = 1235\n",
        "model_str = 'MUR_5'\n",
        "\n",
        "item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'item_embeddings.npy')\n",
        "\n",
        "user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'user_embeddings.npy')\n",
        "\n",
        "with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "  item_embeddings = np.load(f)\n",
        "\n",
        "with tf.io.gfile.GFile(user_embeddings_path, 'rb') as f:\n",
        "  user_embeddings = np.load(f, allow_pickle=True)\n",
        "\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              s=rgb_alphas,\n",
        "              label='{}'.format(label))\n",
        "\n",
        "  \n",
        "plt.legend()\n",
        "plt.title('Output embedding space')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X0')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKvST4BeFvpk"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 5, figsize=(20, 14), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {}\"\"\".format(sorted(set(user_interests[user_ix]))))\n",
        "  ax.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts_TOH1nGlRZ"
      },
      "outputs": [],
      "source": [
        "user_ix = 2\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              s=rgb_alphas,\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=6.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('MUR (H = 5) for a user having interests: {}.'.format(sorted(set(user_interests[user_ix]))), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK63_L8jNsFp"
      },
      "source": [
        "## Visualize Weight Updates "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MgL9XZONWW"
      },
      "source": [
        "#### Iteration 0: weights "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmmvV3XcNxiP"
      },
      "outputs": [],
      "source": [
        "train_next_items = user_item_sequences[:, -3] # validation\n",
        "train_next_item_counter = collections.Counter(train_next_items)\n",
        "train_next_item_count = np.array([train_next_item_counter[item]\n",
        "                                  for item in all_items],\n",
        "                                 dtype=np.float32)\n",
        "\n",
        "total_count = np.sum(train_next_item_count)\n",
        "prior_w = train_next_item_count/total_count\n",
        "\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "color=[cmap(0)] * 10 + [cmap(1)] * 10 + [cmap(2)] * 10 + [cmap(3)] * 10 + [cmap(4)] * 10\n",
        "\n",
        "plt.bar(all_items, prior_w, color=color)\n",
        "plt.xlabel('Item ID')\n",
        "plt.ylabel('Normalized Frequency')\n",
        "plt.title('Empirical density of items.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k580fKy3Mz2e"
      },
      "outputs": [],
      "source": [
        "train_next_items = user_item_sequences[:, -3] # validation\n",
        "train_next_item_counter = collections.Counter(train_next_items)\n",
        "train_next_item_count = np.array([train_next_item_counter[item]\n",
        "                                  for item in all_items],\n",
        "                                 dtype=np.float32)\n",
        "\n",
        "total_count = np.sum(train_next_item_count)\n",
        "prior_w = train_next_item_count/total_count\n",
        "\n",
        "plt.bar(all_items, prior_w)\n",
        "plt.xlabel('Item ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Empirical density of items.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkxu7r-LObl4"
      },
      "source": [
        "## Load iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8RXatVvOME7"
      },
      "outputs": [],
      "source": [
        "model_results_path = os.path.join(root_dir, results_path, dataset_path, f'synthetic_data_{alpha_str}', f'seed_{seed}', model_str)\n",
        "\n",
        "iteration = 1\n",
        "iteration_path = os.path.join(model_results_path,\n",
        "                              'iteration_{}'.format(iteration))\n",
        "\n",
        "all_embeddings = []\n",
        "all_queries = []\n",
        "all_weights = []\n",
        "while tf.io.gfile.exists(iteration_path):\n",
        "\n",
        "  embeddings_path = os.path.join(iteration_path, 'embeddings.npy')\n",
        "  with tf.io.gfile.GFile(embeddings_path, 'rb') as f:\n",
        "    all_embeddings.append(np.load(f))\n",
        "\n",
        "  queries_path = os.path.join(iteration_path, 'user_queries.npy')\n",
        "  if tf.io.gfile.exists(queries_path):\n",
        "    with tf.io.gfile.GFile(queries_path, 'rb') as f:\n",
        "      all_queries.append(np.load(f))\n",
        "\n",
        "  weights_path = os.path.join(iteration_path, 'weights.npy')\n",
        "  with tf.io.gfile.GFile(weights_path, 'rb') as f:\n",
        "    count_weight_dict = np.load(f, allow_pickle=True).item()\n",
        "    weights = np.array([count_weight_dict[item][1] for item in all_items],\n",
        "                       dtype=np.float32)\n",
        "    all_weights.append(weights)\n",
        "\n",
        "  iteration += 1\n",
        "  iteration_path = os.path.join(model_results_path,\n",
        "                                'iteration_{}'.format(iteration))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s58AQwSVbxm7"
      },
      "source": [
        "## Track  ||w(t+1) - w(t)||"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q84kL0W2YMel"
      },
      "outputs": [],
      "source": [
        "delta_norm = lambda w1, w2: np.linalg.norm(w1-w2, ord=2)\n",
        "\n",
        "w_delta = []\n",
        "\n",
        "prev_w = all_weights[0]\n",
        "for w in all_weights[1:]:\n",
        "  w_delta.append(delta_norm(w, prev_w))\n",
        "  prev_w = w\n",
        "\n",
        "plt.plot(w_delta)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Qt8SPD7Xcc"
      },
      "outputs": [],
      "source": [
        "def flatten(d, parent_key='', sep='_'):\n",
        "  items = []\n",
        "  for k, v in d.items():\n",
        "      new_key = parent_key + sep + k if parent_key else k\n",
        "      if isinstance(v, collections.MutableMapping):\n",
        "          items.extend(flatten(v, new_key, sep=sep).items())\n",
        "      else:\n",
        "          items.append((new_key, v))\n",
        "  return dict(items)\n",
        "\n",
        "def get_cluster_wise_performance(\n",
        "    user_embeddings, item_embeddings, test_cluster, target_next_items):\n",
        "\n",
        "  cluster_results = dict()\n",
        "  for user_cluster_slice_ix in np.unique(test_cluster).astype(int):\n",
        "    user_embeddings_slice = user_embeddings[test_cluster == user_cluster_slice_ix]\n",
        "    target_next_items_slice = target_next_items[test_cluster == user_cluster_slice_ix]\n",
        "\n",
        "    num_samples = user_embeddings_slice.shape[0]\n",
        "    hr5 = compute_top_k_mean_accuracy(user_embeddings_slice, item_embeddings,\n",
        "                                        target_next_items_slice, k=5)\n",
        "    hr10 = compute_top_k_mean_accuracy(user_embeddings_slice, item_embeddings,\n",
        "                                        target_next_items_slice, k=10)\n",
        "    \n",
        "    cluster_results[f'HR@5_Cluster{user_cluster_slice_ix}'] = hr5\n",
        "    cluster_results[f'HR@10_Cluster{user_cluster_slice_ix}'] = hr10\n",
        "    cluster_results[f'N_Cluster{user_cluster_slice_ix}'] = num_samples\n",
        "\n",
        "  return cluster_results\n",
        "\n",
        "def load_results(results_path: str, only_eval_result: bool):\n",
        "  data = dict()\n",
        "  with tf.io.gfile.GFile(os.path.join(results_path, 'eval_result.yaml')) as f:\n",
        "    result_eval = yaml.safe_load(f)\n",
        "  data['eval_result'] = flatten(result_eval)\n",
        "  if only_eval_result:\n",
        "    return data['eval_result']      \n",
        "\n",
        "  for fname in ['user_embeddings',\n",
        "                'item_embeddings',]:\n",
        "    fpath = os.path.join(results_path, fname+'.npy')\n",
        "    with tf.io.gfile.GFile(fpath, 'rb') as f:\n",
        "        data[fname] = np.load(f, allow_pickle=True)\n",
        "        # result_eval = data['eval_result'][()]   \n",
        "  \n",
        "  return data['user_embeddings'], data['item_embeddings'], data['eval_result']\n",
        "\n",
        "def compute_top_k_mean_accuracy(query_embeddings, item_embeddings,\n",
        "                                 target_indices, k=5):\n",
        "  \n",
        "  if len(query_embeddings.shape) == 2:\n",
        "    query_embeddings = np.expand_dims(query_embeddings, axis=1)\n",
        "\n",
        "  m = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "  num_items = item_embeddings.shape[0]\n",
        "\n",
        "  target_label = np.eye(num_items)[target_indices]\n",
        "  \n",
        "  all_scores = np.max(\n",
        "    np.matmul(query_embeddings, np.transpose(item_embeddings)), axis=1)\n",
        "   \n",
        "  m.update_state(target_label, all_scores)\n",
        "  return m.result().numpy()\n",
        "\n",
        "def compute_top_k_elementwise_accuracy(query_embeddings, item_embeddings,\n",
        "                                 target_indices, k=5):\n",
        "  \n",
        "  if len(query_embeddings.shape) == 2:\n",
        "    query_embeddings = np.expand_dims(query_embeddings, axis=1)\n",
        "\n",
        "  num_items = item_embeddings.shape[0]\n",
        "\n",
        "  target_label = np.eye(num_items)[target_indices]\n",
        "  \n",
        "  all_scores = np.max(\n",
        "    np.matmul(query_embeddings, np.transpose(item_embeddings)), axis=1)\n",
        "\n",
        "  m = tf.keras.metrics.top_k_categorical_accuracy(target_label, all_scores, k=k)\n",
        "  \n",
        "  return m.numpy()\n",
        "\n",
        "def evaluate_results(model_str, results_dir, alpha_str, item_clusters = None,\n",
        "                     user_interests = None, test_cluster = None,\n",
        "                     is_npz=False, only_eval_result=False, k_list=None,\n",
        "                     normalize_embeddings=False):\n",
        "\n",
        "  seed_list = [1234, 1235, 1236]\n",
        "  seed_found = len(seed_list)\n",
        "  print (model_str)\n",
        "  mean_results = dict()\n",
        "  if k_list is None:\n",
        "    k_list = range(50, 250, 50)\n",
        "  for seed in seed_list:\n",
        " \n",
        "    if alpha_str:\n",
        "      file_path = os.path.join(results_dir, \n",
        "                                'synthetic_data_{}'.format(alpha_str))\n",
        "    else:\n",
        "      file_path = results_dir\n",
        "      \n",
        "    file_path = os.path.join(file_path, 'seed_{}'.format(seed), model_str)\n",
        "      \n",
        "    if not tf.io.gfile.exists(file_path):  \n",
        "      seed_found  -= 1\n",
        "      print (\"{} does not exist! Ignoring it.\".format(file_path))\n",
        "      continue\n",
        "\n",
        "    if only_eval_result:\n",
        "      result_eval = load_results(file_path, only_eval_result)\n",
        "    else:\n",
        "      user_embeddings, item_embeddings, result_eval = load_results(\n",
        "        file_path, only_eval_result)\n",
        "      \n",
        "      # Cluster wise performance\n",
        "      target_next_items = user_item_sequences[:, -1]\n",
        "      results_slice_cluster = get_cluster_wise_performance(\n",
        "          user_embeddings, item_embeddings, test_cluster, target_next_items)\n",
        "      result_eval.update(results_slice_cluster)\n",
        "      if normalize_embeddings:\n",
        "        user_embeddings /= np.linalg.norm(\n",
        "            user_embeddings, axis=-1, keepdims=True)\n",
        "        item_embeddings /= np.linalg.norm(\n",
        "            item_embeddings, axis=-1, keepdims=True)\n",
        "        \n",
        "              \n",
        "      # Sillhouette score\n",
        "      silhouette_dict = evaluate_silhouette_score(item_embeddings,\n",
        "                                                  item_clusters)\n",
        "      result_eval.update(silhouette_dict)\n",
        "\n",
        "      # # Interest Eval scores\n",
        "      # for k in k_list:\n",
        "      #   interest_eval = get_interest_eval_result(\n",
        "      #       user_embeddings, item_embeddings, user_interests, test_cluster, \n",
        "      #       item_clusters, K=k)\n",
        "      #   result_eval.update(interest_eval)\n",
        "\n",
        "      # # Anisotorpy\n",
        "      # normalized_item_embeddings = item_embeddings / np.linalg.norm(\n",
        "      #     item_embeddings, axis=1, keepdims=True)\n",
        "      # num_items = normalized_item_embeddings.shape[0]\n",
        "      # result_eval['isotropy'] = compute_mean_cosine_similarity(item_embeddings)\n",
        "\n",
        "    if not mean_results:\n",
        "      mean_results.update(result_eval)\n",
        "    else:\n",
        "      for k in mean_results.keys():\n",
        "        mean_results[k] += result_eval[k] \n",
        "\n",
        "  for k in mean_results.keys():\n",
        "    mean_results[k] /= seed_found\n",
        "    \n",
        "  return mean_results\n",
        "\n",
        "def evaluate_silhouette_score(X_data, Y_data, use_kmeans = False):\n",
        "\n",
        "  if use_kmeans:\n",
        "    candidate_clusters_size = [2, 3, 5, 7, 10]\n",
        "    silhouette_scores = []\n",
        "    for cluster_size in candidate_clusters_size:\n",
        "      kmeans = KMeans(n_clusters = cluster_size, random_state=1234).fit(X_data)\n",
        "      silhouette_scores.append(sklearn.metrics.silhouette_score(X_data, kmeans.labels_, random_state=1234))\n",
        "    silhouette_kmeans = max(silhouette_scores)\n",
        "  else:\n",
        "    silhouette_kmeans = 0\n",
        "\n",
        "  silhouette_actual = sklearn.metrics.silhouette_score(\n",
        "      X_data, Y_data, random_state=1234)\n",
        "  \n",
        "  result_scores = {\n",
        "      'silhouette_kmeans': silhouette_kmeans,\n",
        "      'silhouette_actual': silhouette_actual\n",
        "  }\n",
        "  \n",
        "  return result_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOK2VgCadcVS"
      },
      "source": [
        "## Plot iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6cdUxF9OYb3"
      },
      "outputs": [],
      "source": [
        "num_plots = len(all_embeddings)\n",
        "\n",
        "cols = 4\n",
        "rows = num_plots // cols\n",
        "if (rows * cols) \u003c num_plots:\n",
        "  rows += 1\n",
        "\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(14, (3.0)*rows), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "# fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "# metric_name = 'HR@10'\n",
        "\n",
        "for i, (ax, embeddings, queries) in enumerate(zip(axs, all_embeddings, all_queries)):\n",
        "  \n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(embeddings[label_indices,0], \n",
        "                embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "    silhouette = evaluate_silhouette_score(\n",
        "        embeddings,item_clusters)['silhouette_actual']\n",
        "    \n",
        "  # queries = tf.squeeze(queries)\n",
        "  # if len(queries.shape) \u003e 1:\n",
        "  #   H = queries.shape[0]\n",
        "  #   ax.quiver(np.zeros(H), np.zeros(H), queries[:, 0], queries[:, 1], scale=7.)\n",
        "  # else:\n",
        "  #   ax.quiver(0., 0., queries[0], queries[1], scale=6.) \n",
        "  \n",
        "  ax.set_title(f'Iteration {i} (S = {silhouette:.2f})', fontsize=12)\n",
        "\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PILoiNKeZFCE"
      },
      "outputs": [],
      "source": [
        "len(all_weights), len(w_delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP9qriJIbMLd"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(rows, cols, figsize=(14, 3.0*rows), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "\n",
        "for i, ax in enumerate(axs):\n",
        "  \n",
        "  if i \u003e= len(all_weights):\n",
        "    break\n",
        "    \n",
        "  if i == 0 or i == 4:\n",
        "    ax.bar(all_items, prior_w)\n",
        "    ax.set_ylabel('Weight')\n",
        "    ax.set_title(f'Iteration {i}', fontsize=10)\n",
        "  else:\n",
        "    ax.bar(all_items, all_weights[i-1])\n",
        "\n",
        "  if i != 0:\n",
        "    ax.text(20.0, 0.1, f'$||\\Delta w|| = {w_delta[i-1]:.4f}$', fontsize=9)\n",
        "\n",
        "  ax.set_title(f'Iteration {i}', fontsize=12)\n",
        "  ax.set_xlabel('Item ID')\n",
        "  \n",
        "  \n",
        "plt.suptitle('Weights of items in iterative training.', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XnvW22zP_z"
      },
      "source": [
        "## Plot item density in the output embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrMPqrpvW0nZ"
      },
      "outputs": [],
      "source": [
        "all_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_ObtOTTW13_"
      },
      "outputs": [],
      "source": [
        "all_weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtAb81LL5jw3"
      },
      "outputs": [],
      "source": [
        "def plot_kernel_estimates(ax, iter_w, iter_embeddings):\n",
        "\n",
        "  num_samples = 10000\n",
        "  sampled_items = np.random.choice(all_items, size=num_samples, p=w)\n",
        "  data = iter_embeddings[sampled_items]\n",
        "  data += np.random.normal(loc=0.0, scale=0.05, size=data.shape)\n",
        "  kernel = stats.gaussian_kde(data.T)\n",
        "\n",
        "  xmin = -2\n",
        "  ymin = -2\n",
        "  xmax = 2\n",
        "  ymax = 2\n",
        "\n",
        "  X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
        "  positions = np.vstack([X.ravel(), Y.ravel()])\n",
        "\n",
        "  Z = np.reshape(kernel(positions).T, X.shape)\n",
        "\n",
        "  ax.set_xlim(xmin, xmax)\n",
        "  ax.set_ylim(ymin, ymax)\n",
        "  cfset = ax.contourf(X, Y, Z, cmap='Blues')\n",
        "  cset = ax.contour(X, Y, Z, colors='k')\n",
        "  ax.clabel(cset, inline=1, fontsize=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyDJzRZqyXgD"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(rows, cols, figsize=(14, 3.0*rows), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "\n",
        "for i, (ax, iter_w, iter_embeddings) in enumerate(zip(axs, all_weights, all_embeddings)):\n",
        "\n",
        "  plot_kernel_estimates(ax, iter_w, iter_embeddings)\n",
        "  \n",
        "  ax.set_xlabel('X1', fontsize=12)\n",
        "  ax.set_ylabel('X0', fontsize=12)\n",
        "  ax.set_title(f'Iteration {i}', fontsize=12)\n",
        "  \n",
        "plt.suptitle('Density in the output embedding space over iterations.', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7s_OAZ-adh9"
      },
      "outputs": [],
      "source": [
        "hr5 = []\n",
        "hr10 = []\n",
        "cluster_indices = []\n",
        "models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjnQPk2dg-dE"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_5'\n",
        "results_path = 'results/d2/density_smoothing/baseline/'\n",
        "\n",
        "results_dir = os.path.join(root_dir, results_path, dataset_path)    \n",
        "\n",
        "MUR5_results = evaluate_results(model_str, results_dir, alpha_str,\n",
        "                                item_clusters, user_interests, test_cluster,\n",
        "                                k_list=[1, 5, 10])\n",
        "\n",
        "for cluster_ix in range(5):\n",
        "  hr5.append(MUR5_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(MUR5_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append('MUR_5_NW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x2leHdfNOg5"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_5'\n",
        "results_path = 'results/d2/density_smoothing/baseline-sample_weight/'\n",
        "\n",
        "results_dir = os.path.join(root_dir, results_path, dataset_path)    \n",
        "\n",
        "MUR5_SW_results = evaluate_results(model_str, results_dir, alpha_str,\n",
        "                                item_clusters, user_interests, test_cluster,\n",
        "                                k_list=[1, 5, 10])\n",
        "\n",
        "for cluster_ix in range(5):\n",
        "  hr5.append(MUR5_SW_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(MUR5_SW_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append(model_str + '_FW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmdq_TYTaf0h"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_5'\n",
        "results_path = 'results/d2/density_smoothing/reruns_3/'\n",
        "\n",
        "results_dir = os.path.join(root_dir, results_path, dataset_path)    \n",
        "\n",
        "MUR5_DW_results = evaluate_results(model_str, results_dir, alpha_str,\n",
        "                                item_clusters, user_interests, test_cluster,\n",
        "                                k_list=[1, 5, 10])\n",
        "\n",
        "\n",
        "for cluster_ix in range(5):\n",
        "  hr5.append(MUR5_DW_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(MUR5_DW_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append(model_str + '_DW')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hlojYfChCsK"
      },
      "outputs": [],
      "source": [
        "# Plotting cluster slice performance\n",
        "# Subplot for alpha_str\n",
        "d = {'Model': models, 'HR@10': hr10, 'HR@5': hr5, 'Cluster': cluster_indices}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.hlines(y=MUR5_results['HR@5'], xmin=-0.5, xmax=4.5,\n",
        "          colors='teal', alpha=0.6)\n",
        "ax.hlines(y=MUR5_SW_results['HR@5'], xmin=-0.5, xmax=4.5,\n",
        "          colors='coral', alpha=0.6)\n",
        "ax.hlines(y=MUR5_DW_results['HR@5'], xmin=-0.5, xmax=4.5,\n",
        "          colors='steelblue', alpha=0.6)\n",
        "\n",
        "sns.barplot(x=\"Cluster\", y=\"HR@10\", hue=\"Model\", data=df, ax=ax,\n",
        "            palette = {'MUR_5_NW': 'teal', 'MUR_5_FW': 'coral', 'MUR_5_DW': 'steelblue'})\n",
        "\n",
        "plt.ylabel('HR@10', fontsize=12)\n",
        "plt.xlabel('Cluster', fontsize=12)\n",
        "plt.title('Performance sliced by cluster index.', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppjp16yhN18y"
      },
      "outputs": [],
      "source": [
        "# Plotting cluster slice performance\n",
        "# Subplot for alpha_str\n",
        "d = {'Model': models, 'HR@10': hr10, 'HR@5': hr5, 'Cluster': cluster_indices}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.hlines(y=MUR5_results['HR@10'], xmin=-0.5, xmax=4.5,\n",
        "          colors='teal', alpha=0.6)\n",
        "ax.hlines(y=MUR5_SW_results['HR@10'], xmin=-0.5, xmax=4.5,\n",
        "          colors='coral', alpha=0.6)\n",
        "ax.hlines(y=MUR5_DW_results['HR@10'], xmin=-0.5, xmax=4.5,\n",
        "          colors='steelblue', alpha=0.6)\n",
        "\n",
        "sns.barplot(x=\"Cluster\", y=\"HR@10\", hue=\"Model\", data=df, ax=ax,\n",
        "            palette = {'MUR_5_NW': 'teal', 'MUR_5_FW': 'coral', 'MUR_5_DW': 'steelblue'})\n",
        "plt.ylabel('HR@10', fontsize=12)\n",
        "plt.xlabel('Cluster', fontsize=12)\n",
        "plt.title('Performance sliced by cluster index.', fontsize=14)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZFtIMWFzsm0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "ItemDensityWeighting_Analysis.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1RaesJgCDmXGDpwIpJwAJJ4tXQHYn6boG",
          "timestamp": 1631459873513
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
