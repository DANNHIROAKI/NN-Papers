# Memory-efficient Attention

This repository accompanies our [paper](https://arxiv.org/abs/2112.05682) on a memory-efficient implementation of (self-)attention. It consists of a colab notebook containing the standard attention implementation, our memory-efficient attention implementation, and evaluation code to determine and compare their runtime performance.

Please remember to connect to a colab runtime with a TPU.
