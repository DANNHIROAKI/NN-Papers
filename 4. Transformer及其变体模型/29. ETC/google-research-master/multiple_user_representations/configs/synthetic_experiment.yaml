# Configuration specifying the hyperparameters used for synthetic experiments.
# The parameter values specified here are default values for the experiment. The parameters can be
# overrided by specifying flags in the training script (Overriding is useful for hyperparam search).

# The following parameters can be overridden through the training script when flags are passed:
# `results_dir`, `dataset_path`, `num_representations`, `epochs`, `use_disagreement`.

# dataset_name should be one of the datasets specified in dataloader.load_data
dataset_name: conditional_synthetic

# root_dir is the prefix of all the other paths
root_dir: 'specify_root_dir'
results_dir: 'results/train-test-split/sample_weight'
dataset_path: 'datasets/sparse_C5_I10_U3/synthetic_data_interest-power1.0_item-power0.5_alpha0.6_gamma0.3'

# Training params
train_batch_size: 128
test_batch_size: 512
epochs: 10
learning_rate: 0.1
softmax_temperature: 1

model_config:

  # Type of retrieval_model_type.
  # Possible values: 'density_smoothed_retrieval', 'standard_retrieval'
  retrieval_model_type: 'density_smoothed_retrieval'

  # Required Hyperparameters
  input_embedding_dimension: 2 # int specifyfing the dimension of input embedding for items.
  output_dimension: 2 # int specifyfing the dimension of output embedding for user and items.
  use_disagreement_loss: False # bool flag specifyfing whether to use query disagreement loss.
  use_shared_input_embedding: False

  # For item tower, only MLP supported.
  item_model: mlp
  item_model_config:
    num_layers: 0 # Use input embeddings as output of item_tower

  # The user_model should be one of the models specified in models/__init__.py
  user_model: simple_parametric_attention
  # The user_model_config contains hyperparameters specific to the user_model.
  # While creating the user_model instance, parameters of user_model_config are passed to the
  # __init__ methond: my_user_model = user_model(**user_model_config)
  user_model_config:
    num_representations: 3
    use_positional_encoding: False
    use_projection_layer: False
