# Configuration for unlearning by negating the training objective
# See https://arxiv.org/abs/2210.01504

from __gin__ import dynamic_registration
import __main__ as train_script

import seqio
from t5x import models
from t5x import adafactor
from t5x import utils
from t5x.examples.t5 import network

from learn_to_forget.transformers import prior_unlearn 
from learn_to_forget.transformers import tasks
from learn_to_forget.transformers import wmt_tasks

include 'third_party/py/t5x/examples/t5/t5_1_1/base.gin'
include 'third_party/py/t5x/configs/runs/finetune.gin'

MIXTURE_OR_TASK_NAME = %gin.REQUIRED
INITIAL_CHECKPOINT_PATH = %gin.REQUIRED
BATCH_SIZE = %gin.REQUIRED
SAVE_CKPT_FREQ = %gin.REQUIRED
USE_CACHED_TASKS = False

TASK_FEATURE_LENGTHS = {"inputs": 256, "targets": 256}
DROPOUT_RATE = 0.0
EVAL_STEPS = 1
LABEL_SMOOTHING = None

MODEL = @prior_unlearn.EncoderDecoderNegLikelihood()
prior_unlearn.EncoderDecoderNegLikelihood:
  module = @network.Transformer()
  input_vocabulary = %VOCABULARY
  output_vocabulary = %VOCABULARY
  optimizer_def = %OPTIMIZER

train/utils.DatasetConfig:
  batch_size = %BATCH_SIZE

train_script.train:
  eval_period = 1
  eval_steps = 1
  random_seed = 0
  use_hardware_rng = True

utils.SaveCheckpointConfig:
  period = %SAVE_CKPT_FREQ # checkpoint frequency
