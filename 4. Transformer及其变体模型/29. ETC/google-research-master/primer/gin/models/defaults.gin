# Default parameters for all models.

import mesh_tensorflow.optimize
import mesh_tensorflow.transformer.learning_rate_schedules
import mesh_tensorflow.transformer.transformer
import mesh_tensorflow.transformer.transformer_layers
import mesh_tensorflow.transformer.learning_rate_schedules
import t5_models

transformer.make_layer_stack.layers = [
    @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
    @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
]

transformer.LayerStack.sublayers_initial = [
    @transformer.sublayer_dropout,
]
transformer.LayerStack.sublayers_per_layer = [
    @transformer.sublayer_true_layer_norm,
    @transformer.sublayer_mask_padding,
    @transformer.sublayer_call_layer,
    @transformer.sublayer_dropout,
    @transformer.sublayer_residual,
]
transformer.LayerStack.sublayers_final = [
    @transformer.sublayer_true_layer_norm,
    @transformer.sublayer_dropout,
    @transformer.sublayer_mask_padding,
]

Unitransformer.shared_embedding_and_softmax_weights = True

LayerStack.dropout_rate = None
LayerStack.norm_epsilon = None
PrePostNormLayerStack.dropout_rate = None
PrePostNormLayerStack.norm_epsilon = None

transformer.make_layer_stack.num_layers = %num_layers
Unitransformer.d_model = %d_model
DenseReluDense.hidden_size = %d_ff
SelfAttention.num_heads = %num_heads
SelfAttention.key_value_size = %d_kv
MDHA.num_heads = %num_heads
MDHA.key_value_size = %d_kv

dropout_rate = 0.0
DenseReluDense.dropout_rate = %dropout_rate
SelfAttention.dropout_rate = %dropout_rate
MDHA.dropout_rate = %dropout_rate
transformer.sublayer_dropout.dropout_rate = %dropout_rate

decoder/Unitransformer.label_smoothing = 0.0

utils.run.sequence_length = {"inputs": 1, "targets": 512}
Unitransformer.max_length = 512
utils.run.optimizer = @optimize.AdafactorOptimizer
utils.run.layout_rules = "ensemble:ensemble,batch:batch,d_ff:model,heads:model,vocab:model,experts:batch"
utils.serialize_num_microbatches.tokens_per_microbatch_per_replica = 2048
utils.run.learning_rate_schedule = @learning_rate_schedules.learning_rate_schedule_noam
utils.run.mesh_shape = @mesh_tensorflow.transformer.utils.tpu_mesh_shape()
utils.tpu_mesh_shape.model_parallelism = 1
utils.get_variable_dtype.activation_dtype = "bfloat16"

# All models use relative attention.
transformer_layers.SelfAttention.relative_attention_type = "bias_shared"
t5_models.MDHA.relative_attention_type = "bias_shared"
transformer.Unitransformer.positional_embedding = False

DenseReluDense.use_bias = True
