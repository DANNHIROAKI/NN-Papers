# Switch Transformer.
#
# Configuration mimicks implementation given by Sharan et al.:
# https://github.com/google-research/google-research/tree/master/transformer_modifications

import mesh_tensorflow.transformer.moe

MoE1D.moe_gating = "switch"
MoE1D.switch_policy_train = "input_jitter"
MoE1D.switch_policy_eval = "input_jitter"
MoE1D.switch_jitter = 1e-2

MoE1D.num_experts = 32
MoE1D.activation = ["relu"]
MoE1D.hidden_size = %d_ff
MoE1D.dropout_rate = %dropout_rate

num_layers = 3
d_model = 768
d_ff = 3072
num_heads = 12
d_kv = 64

transformer.make_layer_stack.layers = [
  @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
  @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
  @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
  @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
  @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
  @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
  @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
  @mesh_tensorflow.transformer.moe.MoE1D,
]

VarianceScalingInitializer.scale = 0.1
encoder/MoE1D.capacity_factor_train = 1.2
deccoder/MoE1D.capacity_factor_train = 1.2
encoder/MoE1D.capacity_factor_eval = 1.2
deccoder/MoE1D.capacity_factor_eval = 1.2

utils.get_variable_dtype.activation_dtype = "bfloat16"
