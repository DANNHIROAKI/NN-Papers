# Learning rate schedule for the Adam Optimizer. Includes a linear warmup at
# the beginning instead of a constant schedule.

import mesh_tensorflow.transformer.learning_rate_schedules

utils.run.optimizer = @optimize.AdamWeightDecayOptimizer

utils.run.learning_rate_schedule = \
    [@learning_rate_schedules.truncated_rsqrt,
     @learning_rate_schedules.linear_warmup,
     0.05]

learning_rate_schedules.linear_warmup.steps_or_fraction = 4000
