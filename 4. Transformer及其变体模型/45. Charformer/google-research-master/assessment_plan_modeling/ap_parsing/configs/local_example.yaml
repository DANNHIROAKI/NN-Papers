task:
  model:
    encoder:
      hidden_size: 64
      num_layers: 1
    input_embedding:
      shape_configs:
        token_ids:
          vocab_size: 25000
          embedding_size: 250
        token_type:
          vocab_size: 6
          embedding_size: 16
        is_upper:
          vocab_size: 2
          embedding_size: 8
        is_title:
          vocab_size: 2
          embedding_size: 8
  train_data:
    input_path: "test"
    seq_length: 16
    is_training: true
    global_batch_size: 32
    drop_remainder: true
  validation_data:
    input_path: "test"
    seq_length: 16
    is_training: false
    global_batch_size: 32
    drop_remainder: true
trainer:
  checkpoint_interval: 10
  max_to_keep: 5
  steps_per_loop: 1
  summary_interval: 20
  train_steps: 50
  validation_interval: 1
  validation_steps: 1
  best_checkpoint_export_subdir: "best_val_acc"
  best_checkpoint_eval_metric: "all/accuracy"
  optimizer_config:
    optimizer:
      type: "adamw"
    learning_rate:
      type: polynomial
      polynomial:
        initial_learning_rate: 0.005
        decay_steps: 1000
runtime:
  run_eagerly: true
