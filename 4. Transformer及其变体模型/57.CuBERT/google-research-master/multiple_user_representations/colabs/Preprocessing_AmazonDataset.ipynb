{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9kL0V3J80Qt"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA14k-JN7e68"
      },
      "source": [
        "\n",
        "Author: Nikhil Mehta  \n",
        "Description: Preprocessing Amazon Dataset for Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rNmFklq7cW3"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import html\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from typing import List, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from colabtools import adhoc_import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xWzweXsX4KF"
      },
      "outputs": [],
      "source": [
        "amazon_data_path = 'data_path/AmazonDataset/category_data/'\n",
        "categories = [\n",
        "              'Kindle_Store',\n",
        "              'Grocery_and_Gourmet_Food', \n",
        "              'CDs_and_Vinyl',\n",
        "              'Sports_and_Outdoors',]\n",
        "extension = '.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCn0cU9ENmXq"
      },
      "outputs": [],
      "source": [
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdTpL74OakCv"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "# def parse(path):\n",
        "#   \"\"\"Reads a json file with multiple objects, stores them as a list and returns.\n",
        "  \n",
        "#   Args:\n",
        "#     path: Path to the json file.\n",
        "\n",
        "#   Returns:\n",
        "#     data: List of objects in the json.\n",
        "#   \"\"\"\n",
        "\n",
        "#   data = []\n",
        "#   with tf.io.gfile.GFile(path, 'r') as f:\n",
        "#     for line in f:\n",
        "#       print_function(line)\n",
        "#       break\n",
        "#       data.append(json.loads(line))\n",
        "  \n",
        "#   return data\n",
        "\n",
        "def parse_stream(path, max_lines=100000000):\n",
        "  \"\"\"Reads json with multiple objects and yields object for an iterator.\n",
        "\n",
        "    Args:\n",
        "      max_lines (int): reads a maximum of max_lines (default:100M). Used when files are very large.\n",
        "\n",
        "    Yields:\n",
        "      obj (JsonObject):  The next json object.\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.io.gfile.GFile(path, 'r') as f:\n",
        "    for line_num, line in enumerate(f):\n",
        "      if line_num \u003e max_lines:\n",
        "        print ('max line reached in parse_stream.')\n",
        "        break\n",
        "      \n",
        "      obj = json.loads(line)\n",
        "      yield obj\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S9WLjX0dBuM"
      },
      "outputs": [],
      "source": [
        "# Data older than this timestamp will be ignored.\n",
        "# Corresponds to Tue 01 Jan 2013 12:00:00 AM UTC\n",
        "# TIME_THRESHOLD = 1356998400\n",
        "TIME_THRESHOLD = 1026900400\n",
        "\n",
        "# Users and items with total number of interactions less than this will be ignored.\n",
        "MIN_INTERACTION_THRESHOLD = 5\n",
        "\n",
        "def extract_user_item_time(raw_dataset_path: str, output_file_path: str):\n",
        "  \"\"\"Extracts triplets (user, item, time) and stores in text file.\n",
        "\n",
        "    Args:\n",
        "      raw_dataset_path: Path to Amazon Review data.\n",
        "      output_file_path: Path of the output text file.\n",
        "  \"\"\"\n",
        "\n",
        "  user_interaction_count = defaultdict(lambda: 0)\n",
        "  item_interaction_count = defaultdict(lambda: 0)\n",
        "  line = 0\n",
        "  largest_time = 0\n",
        "  count_ignored = 0\n",
        "\n",
        "  with tf.io.gfile.GFile(output_file_path, 'w') as fout:\n",
        "\n",
        "    # Get the total interaction count for items and users.\n",
        "    for obj in parse_stream(file_path):\n",
        "      time = obj['unixReviewTime']\n",
        "      if time \u003c TIME_THRESHOLD:\n",
        "        continue\n",
        "\n",
        "      user = obj['reviewerID']\n",
        "      item = obj['asin']\n",
        "\n",
        "      user_interaction_count[user] += 1\n",
        "      item_interaction_count[item] += 1\n",
        "\n",
        "    \n",
        "    for obj in parse_stream(file_path):\n",
        "      time = obj['unixReviewTime']\n",
        "      if time \u003c TIME_THRESHOLD:\n",
        "        continue\n",
        "\n",
        "      user = obj['reviewerID']\n",
        "      item = obj['asin']\n",
        "\n",
        "      min_interaction = min(user_interaction_count[user], item_interaction_count[item])\n",
        "      if min_interaction \u003c MIN_INTERACTION_THRESHOLD:\n",
        "        count_ignored += 1\n",
        "        continue\n",
        "\n",
        "      largest_time = max(time, largest_time)\n",
        "      \n",
        "      # Writing \u003cuser item time\u003e triplets per line.\n",
        "      fout.write(\" \".join([user, item, str(time)]) + ' \\n')\n",
        "      line += 1\n",
        "      if line  == 1:\n",
        "        print(\"Started writing...\")\n",
        "      if line % 100000 == 0:\n",
        "        print(\"At line: \", line, \" with largest time so far: \", largest_time)\n",
        "    \n",
        "    fout.close()\n",
        "\n",
        "    print (f'Users : {len(user_interaction_count)}, Items: {len(item_interaction_count)}')\n",
        "    print ('Total triplets ignored: {}/{}'.format(count_ignored, line+count_ignored))\n",
        "    \n",
        "  return user_interaction_count, item_interaction_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qeft0F-4j8YA"
      },
      "outputs": [],
      "source": [
        "for category in categories:\n",
        "  print (f'\\n Category: {category}')\n",
        "  file_path = os.path.join(amazon_data_path, category, category + extension)\n",
        "  output_path = os.path.join(amazon_data_path, category, category + '_user_item_time.txt')\n",
        "  user_interaction_count, item_interaction_count = extract_user_item_time(file_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky-IG2NNAz_S"
      },
      "outputs": [],
      "source": [
        "output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66gu6l6ka4lq"
      },
      "outputs": [],
      "source": [
        "def generate_user_item_sequence(path: str) -\u003e Dict[int, List[int]]:\n",
        "  \"\"\"Generates user item sequence from triplets (user, item, time) saved in a file.\n",
        "\n",
        "    Args:\n",
        "      path(str): Txt File path with the triplets.\n",
        "\n",
        "    Returns:\n",
        "      user_item_seq(Dict): A dictionary that maps from user-id to a list of \n",
        "        items. The item sequence sorted by time.\n",
        "      user_map: A dictionary that maps user to an integer id.\n",
        "      item_map: A dictionary that maps item to Tuple(id, popularity)\n",
        "  \"\"\"\n",
        "\n",
        "  user_map = dict()\n",
        "  user_num = 0\n",
        "  item_map = dict()\n",
        "  item_num = 0\n",
        "  user_item_seq = dict()\n",
        "  \n",
        "  with tf.io.gfile.GFile(path, 'r') as fin:\n",
        "    for line in fin:\n",
        "      user, item, time = line.rstrip().split(' ')\n",
        "      \n",
        "      if user in user_map:\n",
        "        user_id = user_map[user]\n",
        "      else:\n",
        "        user_num += 1\n",
        "        user_id = user_num\n",
        "        user_map[user] = user_id\n",
        "        user_item_seq[user_id] = []\n",
        "      \n",
        "      if item in item_map:\n",
        "        item_id = item_map[item][0]\n",
        "        item_map[item][1] += 1\n",
        "      else:\n",
        "        item_num += 1\n",
        "        item_id = item_num\n",
        "        item_map[item] = [item_id, 1]\n",
        "      \n",
        "      user_item_seq[user_id].append([item_id, time])\n",
        "\n",
        "    fin.close()\n",
        "\n",
        "  print(\"Total number of users with \u003e= {} interactions: {}\".format(\n",
        "      MIN_INTERACTION_THRESHOLD, user_num))\n",
        "  print(\"Total number of items with \u003e= {} interactions: {}\".format(\n",
        "      MIN_INTERACTION_THRESHOLD, item_num))\n",
        "\n",
        "  # Sort reviews in user_item_seq according to time\n",
        "  for user_id in user_item_seq.keys():\n",
        "    user_item_seq[user_id].sort(key=lambda x: x[1])\n",
        "\n",
        "  return user_item_seq, user_map, item_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUceMiKvG2I8"
      },
      "outputs": [],
      "source": [
        "def sample_item_negatives(\n",
        "    start, end, num_negative_samples, item_counts, skip_items=None) -\u003e List[int]:\n",
        "  \"\"\"Samples negatives for each item for the standard evaluation protocol. \n",
        "  \n",
        "  Following the standard evaluation protocol followed in the literature, only\n",
        "  the negative samples are used while computing metrics HR@K and NDCG@K. For\n",
        "  negative sampling, only those items are considered which the user has not\n",
        "  interacted with.\n",
        "\n",
        "  Args:\n",
        "    start: Smallest item id to consider while sampling.\n",
        "    end: Largest item id to consider while sampling.\n",
        "    num_negative_samples: Number of negative items to sample.\n",
        "    skip_items: List of items that are to be skipped while sampling. This is\n",
        "      used to skip items that the user has already interacted with.\n",
        "    item_counts: A dictionary mapping from item to their engagement count.\n",
        "    \n",
        "  Returns:\n",
        "    negative_samples: A list consisting of negative sample items.\n",
        "  \"\"\"\n",
        "\n",
        "  negative_samples = []\n",
        "  if skip_items is None:\n",
        "    skip_items = []\n",
        "  \n",
        "  adjusted_item_counts = np.array(\n",
        "      [item_counts[item_ix] if item_ix not in skip_items else 0.0 \n",
        "       for item_ix in range(start, end)])\n",
        "  weights = adjusted_item_counts * 1.0 / sum(adjusted_item_counts)\n",
        "  return list(np.random.choice(\n",
        "      np.arange(start, end), size=num_negative_samples, p=weights, replace=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e85IUm5cESpL"
      },
      "outputs": [],
      "source": [
        "for category in categories[1:]:\n",
        "\n",
        "  print(f'Processing category: {category}')\n",
        "  path = os.path.join(amazon_data_path, category, category + '_user_item_time.txt')\n",
        "  user_item_seq, user_map, item_map = generate_user_item_sequence(path)\n",
        "\n",
        "  # Generate an output file with \u003cuser_id item_id time\u003e per line. \n",
        "  path = os.path.join(amazon_data_path, category, 'user_item_mapped.txt')\n",
        "  f = tf.io.gfile.GFile(path, 'w')\n",
        "  for user in user_item_seq.keys():\n",
        "    for item_time in user_item_seq[user]:\n",
        "      # item_time: [item_id, time]\n",
        "      f.write('%d %d %s\\n' % (user, item_time[0], str(item_time[1])))\n",
        "  f.close()\n",
        "\n",
        "  path = os.path.join(amazon_data_path, category, 'item_map.json')\n",
        "  with tf.io.gfile.GFile(path, 'w') as f:\n",
        "    json.dump(item_map, f)\n",
        "\n",
        "  path = os.path.join(amazon_data_path, category, 'user_map.json')\n",
        "  with tf.io.gfile.GFile(path, 'w') as f:\n",
        "    json.dump(user_map, f)\n",
        "\n",
        "  # Get item counts\n",
        "  item_counts = dict()\n",
        "  for item, item_val in item_map.items():\n",
        "    item_counts[item_val[0]] = item_val[1]\n",
        "\n",
        "  # Store neg samples for each user.\n",
        "  neg_path = os.path.join(amazon_data_path, category, 'user_neg_items.txt')\n",
        "  neg_sample_f = tf.io.gfile.GFile(neg_path, 'w')\n",
        "  for user in user_item_seq.keys():\n",
        "    user_items = [item_time[0] for item_time in user_item_seq[user]]\n",
        "    negative_samples = sample_item_negatives(\n",
        "        1, len(item_map)+1, 99, item_counts, skip_items=user_items)\n",
        "    neg_sample_f.write(\n",
        "        '%d: %s\\n' % (user, ' '.join(map(str, negative_samples))))\n",
        "  neg_sample_f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU_PRQ0Hh4nC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9iPP_J2nTdT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFcwyNwvV0m8"
      },
      "outputs": [],
      "source": [
        "def get_item_categories (amazon_dataset_category: str) -\u003e Dict[Text, Any], Dict[int, int]\n",
        "  \"\"\"Returns the category_id_map and item_category_map for analysis.\"\"\"\n",
        "\n",
        "  meta_path = os.path.join(amazon_data_path, category, f'meta_{category}.json'.)\n",
        "  \n",
        "  # Removes numeric strings \n",
        "  remove_words = ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th']\n",
        "  \n",
        "  category_num = 0\n",
        "  no_category_query = 'NOC'\n",
        "\n",
        "  category_id_map = dict(no_category_query = (category_num, 0))\n",
        "  item_category_map = defaultdict(lambda: 0)\n",
        "  items_without_category = 0\n",
        "  for ix, obj in enumerate(parse_stream(meta_path)):\n",
        "\n",
        "      if 'category' not in obj:\n",
        "        category = no_category_query\n",
        "        items_without_category += 1\n",
        "      else:\n",
        "        \n",
        "        if len(obj['category']) \u003c 3:\n",
        "          items_without_category += 1\n",
        "          continue\n",
        "      \n",
        "        category = obj['category'][2]\n",
        "        category = bs4.BeautifulSoup(category).get_text()\n",
        "        # category = removes_text_in_parenthesis(category)\n",
        "        category = preprocess_category_text(category, remove_words)\n",
        "\n",
        "      if category not in category_id_map:\n",
        "        category_num += 1\n",
        "        category_id_map[category] = (category_num, 1)\n",
        "      else:\n",
        "        category_id_map[category] = (\n",
        "            category_id_map[category][0], category_id_map[category][1]+1)\n",
        "      \n",
        "      item = obj['asin'] \n",
        "      if item in item_map:\n",
        "        item_category_map[item_map[item]] = category_id_map[category][0]\n",
        "\n",
        "  print(f'Items without category: {items_without_category}')\n",
        "  return category_id_map, item_category_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDiREorDOD-X"
      },
      "outputs": [],
      "source": [
        "items_without_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZWSCzeJQgAp"
      },
      "outputs": [],
      "source": [
        "items_with_category = 0\n",
        "for k in sorted(list(category_id_map.keys())):\n",
        "  print (f\"{category_id_map[k][0]:\u003c{10}} {k:\u003c{40}}: {category_id_map[k][1]:\u003e{20}}\")\n",
        "  items_with_category += category_id_map[k][1]\n",
        "\n",
        "print (f\"\\nTotal items with category information {items_with_category}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Preprocessing-AmazonDataset.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1GnmoesYMDWwnwdxj7Su6NfzYbxGXnlLl",
          "timestamp": 1631309512748
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
