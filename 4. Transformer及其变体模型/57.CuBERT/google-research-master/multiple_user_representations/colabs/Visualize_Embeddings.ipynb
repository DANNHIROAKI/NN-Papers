{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n01htfxI89ew"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emzkYqPaz8r5"
      },
      "source": [
        "Author: Nikhil Mehta  \n",
        "Description: Embedding visualization for synthetic data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0yFMGicBRGo"
      },
      "outputs": [],
      "source": [
        "# %reset\n",
        "import collections\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "from typing import Dict, List, Text, Tuple, Union, Optional, Any\n",
        "\n",
        "from absl import logging\n",
        "from colabtools import drive\n",
        "from colabtools import adhoc_import\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.colors import to_rgb, to_rgba\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "from multiple_user_representations.synthetic_data import util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdx33BQBBUIb"
      },
      "outputs": [],
      "source": [
        "def load_dataset(root_dir, dataset_path, alpha_str, is_npz = False):\n",
        "\n",
        "  if is_npz:\n",
        "    data_path = '{}/{}/synthetic_data_{}.npz'.format(\n",
        "        root_dir, dataset_path, alpha_str)\n",
        "\n",
        "    with tf.io.gfile.GFile(data_path, 'rb') as f:\n",
        "        data = np.load(f, allow_pickle=True)\n",
        "        if 'item_clusters' in data:\n",
        "          item_clusters = data['item_clusters']\n",
        "        else:\n",
        "          item_clusters = data['item_labels']\n",
        "        all_items = data['items']\n",
        "        user_interests = data['user_interests']\n",
        "        test_cluster = item_clusters[data['user_item_sequences'][:, -1]]\n",
        "        user_item_sequences = data['user_item_sequences']\n",
        "  else:\n",
        "    data_path = '{}/{}/synthetic_data_{}'.format(\n",
        "        root_dir, dataset_path, alpha_str)\n",
        "    data = util.load_data(data_path)\n",
        "    item_clusters = data['item_clusters']\n",
        "    all_items = data['items']\n",
        "    test_cluster = item_clusters[data['user_item_sequences'][:, -1]]\n",
        "    user_item_sequences = data['user_item_sequences']\n",
        "    user_interests = data.get('user_interests', None)\n",
        "    \n",
        "  return (item_clusters, all_items, user_interests, test_cluster, user_item_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ms2vjmqsNTi"
      },
      "outputs": [],
      "source": [
        "def evaluate_silhouette_score(X_data, Y_data, use_kmeans = False):\n",
        "\n",
        "  if use_kmeans:\n",
        "    candidate_clusters_size = [2, 3, 5, 7, 10]\n",
        "    silhouette_scores = []\n",
        "    for cluster_size in candidate_clusters_size:\n",
        "      kmeans = KMeans(n_clusters = cluster_size, random_state=1234).fit(X_data)\n",
        "      silhouette_scores.append(sklearn.metrics.silhouette_score(X_data, kmeans.labels_, random_state=1234))\n",
        "    silhouette_kmeans = max(silhouette_scores)\n",
        "  else:\n",
        "    silhouette_kmeans = 0\n",
        "\n",
        "  silhouette_actual = sklearn.metrics.silhouette_score(\n",
        "      X_data, Y_data, random_state=1234)\n",
        "  \n",
        "  result_scores = {\n",
        "      'silhouette_kmeans': silhouette_kmeans,\n",
        "      'silhouette_actual': silhouette_actual\n",
        "  }\n",
        "  \n",
        "  return result_scores\n",
        "  \n",
        "def evaluate_interest_retrieval(user_embeddings, item_embeddings,\n",
        "                                ground_truth_interests, item_clusters, K=10):\n",
        "\n",
        "  def compute_mutilabel_recall(predictions, labels):\n",
        "    # print (predictions, labels)\n",
        "    intersection = np.intersect1d(predictions, labels)\n",
        "    return len(intersection) / len(labels)\n",
        "\n",
        "  num_user_interests = len(ground_truth_interests[0]) # 3\n",
        "  num_items = np.unique(item_clusters, return_counts=True)[1][0] # items p cluster\n",
        "\n",
        "  if len(user_embeddings.shape) == 2:\n",
        "    user_embeddings = np.expand_dims(user_embeddings, axis=1)\n",
        "\n",
        "  result_score = np.matmul(user_embeddings, np.transpose(item_embeddings))\n",
        "  result_score = np.max(result_score, axis=1)\n",
        "  sorted_items = np.argsort(-result_score, axis=1)\n",
        "  top_k_items = sorted_items[:, :K]\n",
        "  top_k_item_clusters = item_clusters[top_k_items]\n",
        "  \n",
        "  interest_acc = []\n",
        "  for user_k_predictions, user_interest in zip(top_k_item_clusters,\n",
        "                                               ground_truth_interests):\n",
        "    predicted_clusters, count = np.unique(user_k_predictions,\n",
        "                                          return_counts=True)\n",
        "    sorted_indices = np.argsort(-count)\n",
        "    predicted_clusters = predicted_clusters[sorted_indices][:num_user_interests]\n",
        "    interest_acc.append(compute_mutilabel_recall(predicted_clusters, user_interest))\n",
        "\n",
        "  return interest_acc\n",
        "\n",
        "def get_interest_eval_result(user_embeddings, item_embeddings, \n",
        "                             user_interests, test_cluster, item_clusters, K=20):\n",
        "\n",
        "  return_dict = dict()\n",
        "  if len(test_cluster.shape) \u003c 2:\n",
        "    test_cluster = np.expand_dims(test_cluster, axis=-1)\n",
        "\n",
        "  interest_acc = evaluate_interest_retrieval(user_embeddings, item_embeddings,\n",
        "                                             test_cluster, item_clusters, K=K)\n",
        "  return_dict[f'next_interest_hr@{K}'] = np.mean(interest_acc)\n",
        "\n",
        "  interest_acc = evaluate_interest_retrieval(user_embeddings, item_embeddings,\n",
        "                                             user_interests, item_clusters, K=K)\n",
        "  return_dict[f'interest_recall@{K}'] = np.mean(interest_acc)\n",
        "\n",
        "  return return_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3-YJ0wHDM6L"
      },
      "outputs": [],
      "source": [
        "def flatten(d, parent_key='', sep='_'):\n",
        "  items = []\n",
        "  for k, v in d.items():\n",
        "      new_key = parent_key + sep + k if parent_key else k\n",
        "      if isinstance(v, collections.MutableMapping):\n",
        "          items.extend(flatten(v, new_key, sep=sep).items())\n",
        "      else:\n",
        "          items.append((new_key, v))\n",
        "  return dict(items)\n",
        "\n",
        "def load_results(results_path: str, only_eval_result: bool):\n",
        "\n",
        "  if results_path.endswith('.npz'):\n",
        "    with tf.io.gfile.GFile(results_path, 'rb') as f:\n",
        "          data = np.load(f, allow_pickle=True)\n",
        "\n",
        "          result_eval = data['eval_result'][()]\n",
        "          result_eval = flatten(result_eval)\n",
        "          if only_eval_result:\n",
        "            return result_eval\n",
        "            \n",
        "          item_embeddings = data['item_embeddings']\n",
        "          user_embeddings = data['user_embeddings']\n",
        "\n",
        "    return user_embeddings, item_embeddings, result_eval\n",
        "  else:\n",
        "    \n",
        "    data = dict()\n",
        "    with tf.io.gfile.GFile(os.path.join(results_path, 'eval_result.yaml')) as f:\n",
        "      result_eval = yaml.safe_load(f)\n",
        "    data['eval_result'] = flatten(result_eval)\n",
        "    if only_eval_result:\n",
        "      return data['eval_result']      \n",
        "\n",
        "    for fname in ['user_embeddings',\n",
        "                  'item_embeddings',]:\n",
        "      fpath = os.path.join(results_path, fname+'.npy')\n",
        "      with tf.io.gfile.GFile(fpath, 'rb') as f:\n",
        "          data[fname] = np.load(f, allow_pickle=True)\n",
        "          # result_eval = data['eval_result'][()]   \n",
        "    \n",
        "    return data['user_embeddings'], data['item_embeddings'], data['eval_result']\n",
        "  \n",
        "def compute_mean_cosine_similarity(item_embeddings):\n",
        "\n",
        "  normalized_item_embeddings = item_embeddings / np.linalg.norm(\n",
        "          item_embeddings, axis=1, keepdims=True)\n",
        "  num_items = normalized_item_embeddings.shape[0]\n",
        "  pairwise_similarity = np.sum(\n",
        "      np.matmul(normalized_item_embeddings,\n",
        "                np.transpose(normalized_item_embeddings))) - num_items\n",
        "\n",
        "  return np.abs(pairwise_similarity / (num_items*(num_items-1)))\n",
        "\n",
        "def compute_top_k_mean_accuracy(query_embeddings, item_embeddings,\n",
        "                                 target_indices, k=5):\n",
        "  \n",
        "  if len(query_embeddings.shape) == 2:\n",
        "    query_embeddings = np.expand_dims(query_embeddings, axis=1)\n",
        "\n",
        "  m = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "  num_items = item_embeddings.shape[0]\n",
        "\n",
        "  target_label = np.eye(num_items)[target_indices]\n",
        "  \n",
        "  all_scores = np.max(\n",
        "    np.matmul(query_embeddings, np.transpose(item_embeddings)), axis=1)\n",
        "   \n",
        "  m.update_state(target_label, all_scores)\n",
        "  return m.result().numpy()\n",
        "\n",
        "def compute_top_k_elementwise_accuracy(query_embeddings, item_embeddings,\n",
        "                                 target_indices, k=5):\n",
        "  \n",
        "  if len(query_embeddings.shape) == 2:\n",
        "    query_embeddings = np.expand_dims(query_embeddings, axis=1)\n",
        "\n",
        "  num_items = item_embeddings.shape[0]\n",
        "\n",
        "  target_label = np.eye(num_items)[target_indices]\n",
        "  \n",
        "  all_scores = np.max(\n",
        "    np.matmul(query_embeddings, np.transpose(item_embeddings)), axis=1)\n",
        "\n",
        "  m = tf.keras.metrics.top_k_categorical_accuracy(target_label, all_scores, k=k)\n",
        "  \n",
        "  return m.numpy()\n",
        "\n",
        "def get_cluster_wise_performance(\n",
        "    user_embeddings, item_embeddings, test_cluster, target_next_items):\n",
        "\n",
        "  cluster_results = dict()\n",
        "  for user_cluster_slice_ix in np.unique(test_cluster).astype(int):\n",
        "    user_embeddings_slice = user_embeddings[test_cluster == user_cluster_slice_ix]\n",
        "    target_next_items_slice = target_next_items[test_cluster == user_cluster_slice_ix]\n",
        "\n",
        "    num_samples = user_embeddings_slice.shape[0]\n",
        "    hr5 = compute_top_k_mean_accuracy(user_embeddings_slice, item_embeddings,\n",
        "                                        target_next_items_slice, k=5)\n",
        "    hr10 = compute_top_k_mean_accuracy(user_embeddings_slice, item_embeddings,\n",
        "                                        target_next_items_slice, k=10)\n",
        "    \n",
        "    cluster_results[f'HR@5_Cluster{user_cluster_slice_ix}'] = hr5\n",
        "    cluster_results[f'HR@10_Cluster{user_cluster_slice_ix}'] = hr10\n",
        "    cluster_results[f'N_Cluster{user_cluster_slice_ix}'] = num_samples\n",
        "\n",
        "  return cluster_results\n",
        "\n",
        "def evaluate_results(model_str, results_dir, alpha_str, item_clusters = None,\n",
        "                     user_interests = None, test_cluster = None,\n",
        "                     is_npz=False, only_eval_result=False, k_list=None,\n",
        "                     normalize_embeddings=False):\n",
        "\n",
        "  seed_list = [1234, 1235, 1236]\n",
        "  seed_found = len(seed_list)\n",
        "  print (model_str)\n",
        "  mean_results = dict()\n",
        "  if k_list is None:\n",
        "    k_list = range(50, 250, 50)\n",
        "  for seed in seed_list:\n",
        "\n",
        "    if is_npz:\n",
        "      fname = 'embeddings_data.npz'    \n",
        "      file_path = os.path.join(results_dir,\n",
        "                              'synthetic_data_{}'.format(alpha_str),\n",
        "                              'seed_{}'.format(seed),\n",
        "                              model_str, fname)\n",
        "    else:\n",
        "      if alpha_str:\n",
        "        file_path = os.path.join(results_dir, \n",
        "                                 'synthetic_data_{}'.format(alpha_str))\n",
        "      else:\n",
        "        file_path = results_dir\n",
        "        \n",
        "      file_path = os.path.join(file_path, 'seed_{}'.format(seed), model_str)\n",
        "      \n",
        "    if not tf.io.gfile.exists(file_path):\n",
        "      seed_found  -= 1\n",
        "      print (\"{} does not exist! Ignoring it.\".format(file_path))\n",
        "      continue\n",
        "\n",
        "    if only_eval_result:\n",
        "      result_eval = load_results(file_path, only_eval_result)\n",
        "    else:\n",
        "      user_embeddings, item_embeddings, result_eval = load_results(\n",
        "        file_path, only_eval_result)\n",
        "      \n",
        "      # Cluster wise performance\n",
        "      target_next_items = user_item_sequences[:, -1]\n",
        "      results_slice_cluster = get_cluster_wise_performance(\n",
        "          user_embeddings, item_embeddings, test_cluster, target_next_items)\n",
        "      result_eval.update(results_slice_cluster)\n",
        "      if normalize_embeddings:\n",
        "        user_embeddings /= np.linalg.norm(\n",
        "            user_embeddings, axis=-1, keepdims=True)\n",
        "        item_embeddings /= np.linalg.norm(\n",
        "            item_embeddings, axis=-1, keepdims=True)\n",
        "        \n",
        "              \n",
        "      # Sillhouette score\n",
        "      silhouette_dict = evaluate_silhouette_score(item_embeddings,\n",
        "                                                  item_clusters)\n",
        "      result_eval.update(silhouette_dict)\n",
        "\n",
        "      # Interest Eval scores\n",
        "      for k in k_list:\n",
        "        interest_eval = get_interest_eval_result(\n",
        "            user_embeddings, item_embeddings, user_interests, test_cluster, \n",
        "            item_clusters, K=k)\n",
        "        result_eval.update(interest_eval)\n",
        "\n",
        "      # Anisotorpy\n",
        "      normalized_item_embeddings = item_embeddings / np.linalg.norm(\n",
        "          item_embeddings, axis=1, keepdims=True)\n",
        "      num_items = normalized_item_embeddings.shape[0]\n",
        "      result_eval['isotropy'] = compute_mean_cosine_similarity(item_embeddings)\n",
        "\n",
        "    if not mean_results:\n",
        "      mean_results.update(result_eval)\n",
        "    else:\n",
        "      for k in mean_results.keys():\n",
        "        mean_results[k] += result_eval[k] \n",
        "\n",
        "  for k in mean_results.keys():\n",
        "    mean_results[k] /= seed_found\n",
        "  return mean_results\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo-wWLMgNLQF"
      },
      "outputs": [],
      "source": [
        "def get_volatility_str(gamma: float):\n",
        "  if gamma == 0.0:\n",
        "    return 'No Volatility'\n",
        "  elif gamma \u003e= 0.4:\n",
        "    return 'High Volatility'\n",
        "  elif gamma \u003c= 0.1:\n",
        "    return 'Low Volatility'\n",
        "  else:\n",
        "    return 'Medium Volatility'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55unP2KQlN2N"
      },
      "source": [
        "Plot for varying the number of heads, and analyzing performance for user slices, where each slice has different number of interests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwapNf3uwfnl"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/C100_I50_U7-5-3-2_T40/'\n",
        "d = 16\n",
        "\n",
        "model_heads = range(1, 21)\n",
        "# model_heads.extend(list(range(10, 130, 10)))\n",
        "results_dir = 'results/reruns/d{}'.format(d)\n",
        "results_path = os.path.join(root_dir, results_dir, dataset_path)      \n",
        "\n",
        "plot_alphas = [0.4, 0.6, 0.8]\n",
        "model_arr = []\n",
        "results_arr_hr10 = []\n",
        "results_arr_hr50 = []\n",
        "results_arr_hr100 = []\n",
        "results_arr_hr200 = []\n",
        "split_ix_arr = []\n",
        "gamma_arr = []\n",
        "\n",
        "for ix, alpha in enumerate(plot_alphas):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  # dataset = load_dataset(root_dir, dataset_path, alpha_str, is_npz = False)\n",
        "  print(alpha_str)\n",
        "  for h in model_heads:\n",
        "    if h == 1:\n",
        "      model_str = 'SUR'\n",
        "    else:\n",
        "      model_str = f'MUR_{h}'\n",
        "    result = evaluate_results(\n",
        "        model_str, results_path, alpha_str, is_npz=False, only_eval_result=True)\n",
        "    for split_ix in range(4):\n",
        "      hr_10 = result[f'split_{split_ix}_top_10_categorical_accuracy']\n",
        "      hr_50 = result[f'split_{split_ix}_top_50_categorical_accuracy']\n",
        "      hr_100 = result[f'split_{split_ix}_top_100_categorical_accuracy']\n",
        "      hr_200 = result[f'split_{split_ix}_top_200_categorical_accuracy']\n",
        "      \n",
        "      split_ix_arr.append(split_ix)\n",
        "      results_arr_hr10.append(hr_10)\n",
        "      results_arr_hr50.append(hr_50)\n",
        "      results_arr_hr100.append(hr_100)\n",
        "      results_arr_hr200.append(hr_200)\n",
        "      model_arr.append(h)\n",
        "      gamma_arr.append(get_volatility_str(gamma))\n",
        "      # print(hr_100, end = ' ')\n",
        "    # print('')\n",
        "  # print('', end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuWvzTFniY9f"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "# initialize fig plot\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 8), constrained_layout=True, \n",
        "                        sharex=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=100, |Yu|=7_5_3_2, |U|=50000\", fontsize=16)\n",
        "\n",
        "# Subplot for alpha_str\n",
        "d = {'Num Heads': model_arr, 'HR@100': results_arr_hr100,\n",
        "     'HR@200': results_arr_hr200, 'HR@50': results_arr_hr50,\n",
        "     'HR@10': results_arr_hr10, 'split_ix_arr': split_ix_arr,\n",
        "     'gamma_arr': gamma_arr}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "for metric_ix, metric_k in enumerate([10, 50]):\n",
        "  for split_ix, interests in enumerate([7, 5, 3, 2]):\n",
        "\n",
        "    title = f'Num_Interests: {interests}'\n",
        "    df_split = df.loc[df['split_ix_arr'] == split_ix]\n",
        "\n",
        "    ax = sns.lineplot(\n",
        "        x='Num Heads', y=f'HR@{metric_k}', hue=\"gamma_arr\", data=df_split,\n",
        "        markers=True, style=\"gamma_arr\", ax=axs[metric_ix*4+split_ix])\n",
        "\n",
        "    ax.legend(title='Interest Volatility')\n",
        "    ax.set_title(title)\n",
        "\n",
        "  # ax.set_ylabel(f'HR@{K}')\n",
        "  # ax.set_xticks(model_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd8HFlIqui0p"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/C10_I50_U7-5-3-2_T40/'\n",
        "d = 16\n",
        "\n",
        "model_heads = range(1, 21)\n",
        "# model_heads.extend(list(range(10, 130, 10)))\n",
        "results_dir = 'results/reruns/d{}'.format(d)\n",
        "results_path = os.path.join(root_dir, results_dir, dataset_path)      \n",
        "\n",
        "plot_alphas = [0.4, 0.6, 0.8]\n",
        "model_arr = []\n",
        "results_arr_hr10 = []\n",
        "results_arr_hr50 = []\n",
        "results_arr_hr100 = []\n",
        "results_arr_hr200 = []\n",
        "split_ix_arr = []\n",
        "gamma_arr = []\n",
        "\n",
        "for ix, alpha in enumerate(plot_alphas):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  # dataset = load_dataset(root_dir, dataset_path, alpha_str, is_npz = False)\n",
        "  print(alpha_str)\n",
        "  for h in model_heads:\n",
        "    if h == 1:\n",
        "      model_str = 'SUR'\n",
        "    else:\n",
        "      model_str = f'MUR_{h}'\n",
        "    result = evaluate_results(\n",
        "        model_str, results_path, alpha_str, is_npz=False, only_eval_result=True)\n",
        "    for split_ix in range(4):\n",
        "      hr_10 = result[f'split_{split_ix}_top_10_categorical_accuracy']\n",
        "      hr_50 = result[f'split_{split_ix}_top_50_categorical_accuracy']\n",
        "      hr_100 = result[f'split_{split_ix}_top_100_categorical_accuracy']\n",
        "      hr_200 = result[f'split_{split_ix}_top_200_categorical_accuracy']\n",
        "      \n",
        "      split_ix_arr.append(split_ix)\n",
        "      results_arr_hr10.append(hr_10)\n",
        "      results_arr_hr50.append(hr_50)\n",
        "      results_arr_hr100.append(hr_100)\n",
        "      results_arr_hr200.append(hr_200)\n",
        "      model_arr.append(h)\n",
        "      gamma_arr.append(get_volatility_str(gamma))\n",
        "      # print(hr_100, end = ' ')\n",
        "    # print('')\n",
        "  # print('', end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1xgYSkKXcFm"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "# initialize fig plot\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 8), constrained_layout=True, \n",
        "                        sharex=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=100, |Yu|=7_5_3_2, |U|=50000\", fontsize=16)\n",
        "\n",
        "# Subplot for alpha_str\n",
        "d = {'Num Heads': model_arr, 'HR@100': results_arr_hr100,\n",
        "     'HR@200': results_arr_hr200, 'HR@50': results_arr_hr50,\n",
        "     'HR@10': results_arr_hr10, 'split_ix_arr': split_ix_arr,\n",
        "     'gamma_arr': gamma_arr}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "for metric_ix, metric_k in enumerate([50, 100]):\n",
        "  for split_ix, interests in enumerate([7, 5, 3, 2]):\n",
        "\n",
        "    title = f'Num_Interests: {interests}'\n",
        "    df_split = df.loc[df['split_ix_arr'] == split_ix]\n",
        "\n",
        "    ax = sns.lineplot(\n",
        "        x='Num Heads', y=f'HR@{metric_k}', hue=\"gamma_arr\", data=df_split,\n",
        "        markers=True, style=\"gamma_arr\", ax=axs[metric_ix*4+split_ix])\n",
        "\n",
        "    ax.legend(title='Interest Volatility')\n",
        "    ax.set_title(title)\n",
        "\n",
        "  # ax.set_ylabel(f'HR@{K}')\n",
        "  # ax.set_xticks(model_heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyLY2M15lXbU"
      },
      "source": [
        "# Plotting results performance vs. num_heads for different intereset volatility slices of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "audGs5cklLWO"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/C20_I50_U5_T40/synthetic_data_mixture_alpha0.9_0.8_0.7_0.6/'\n",
        "d = 16\n",
        "\n",
        "\n",
        "model_heads = list(range(1, 21))\n",
        "# model_heads.extend(list(range(10, 130, 10)))\n",
        "results_dir = 'results/reruns/d{}'.format(d)\n",
        "results_path = os.path.join(root_dir, results_dir, dataset_path)      \n",
        "\n",
        "plot_alphas = [0.4, 0.6, 0.8]\n",
        "model_arr = []\n",
        "split_ix_arr = []\n",
        "gamma_arr = []\n",
        "\n",
        "results_arr_hr10 = []\n",
        "results_arr_hr50 = []\n",
        "results_arr_hr100 = []\n",
        "results_arr_hr200 = []\n",
        "\n",
        "alpha_str = ''\n",
        "for h in model_heads:\n",
        "  if h == 1:\n",
        "    model_str = 'SUR'\n",
        "  else:\n",
        "    model_str = f'MUR_{h}'\n",
        "  result = evaluate_results(\n",
        "      model_str, results_path, alpha_str, is_npz=False, only_eval_result=True)\n",
        "  for split_ix in range(4):\n",
        "    hr_10 = result[f'split_{split_ix}_top_10_categorical_accuracy']\n",
        "    hr_50 = result[f'split_{split_ix}_top_50_categorical_accuracy']\n",
        "    hr_100 = result[f'split_{split_ix}_top_100_categorical_accuracy']\n",
        "    hr_200 = result[f'split_{split_ix}_top_200_categorical_accuracy']\n",
        "     \n",
        "    results_arr_hr10.append(hr_10)\n",
        "    results_arr_hr50.append(hr_50)\n",
        "    results_arr_hr100.append(hr_100)\n",
        "    results_arr_hr200.append(hr_200)\n",
        "    \n",
        "    split_ix_arr.append(split_ix)\n",
        "    model_arr.append(h)\n",
        "    gamma_arr.append(get_volatility_str(gamma))\n",
        "#     print(hr_100, end = ' ')\n",
        "#   print('')\n",
        "# print('', end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGeivOEEqZwV"
      },
      "outputs": [],
      "source": [
        "# initialize fig plot\n",
        "# fig, axs = plt.subplots(1, 1, figsize=(4, 4), constrained_layout=True, \n",
        "#                         sharex=True, sharey=True)\n",
        "# axs = axs.flat\n",
        "# fig.suptitle(f\"I=100, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "\n",
        "fig = plt.figure(figsize=(4, 4), constrained_layout=True)\n",
        "\n",
        "d = {'Num Heads': model_arr, 'HR@100': results_arr_hr100,\n",
        "     'HR@200': results_arr_hr200, 'HR@50': results_arr_hr50,\n",
        "     'HR@10': results_arr_hr10, 'split_ix_arr': split_ix_arr,\n",
        "     'gamma_arr': gamma_arr}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "ax = sns.lineplot(\n",
        "      x='Num Heads', y='HR@50', hue='split_ix_arr', data=df, style='split_ix_arr',\n",
        "      markers=True)\n",
        "\n",
        "# for split_ix in range(4):\n",
        "\n",
        "#   title = f'Split: {split_ix}'\n",
        "#   df_split = df.loc[df['split_ix_arr'] == split_ix]\n",
        "\n",
        "#   ax = sns.lineplot(\n",
        "#       x='Num Heads', y='HR@100', data=df_split,\n",
        "#       markers=True, ax=axs[split_ix])\n",
        "\n",
        "#   ax.legend(loc='upper left')\n",
        "#   ax.set_title(title)\n",
        "  # ax.set_ylabel(f'HR@{K}')\n",
        "  # ax.set_xticks(model_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIsrP6x9qldY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_dataset_results(dataset_path: str, alpha_str: str, vocab_size: int = 1000):\n",
        "\n",
        "  print (f'Getting results for {dataset_path} with {alpha_str} dataset.')\n",
        "  models = ['SUR', 'MUR_3', 'MUR_4', 'MUR_7']\n",
        "  H= [1, 3, 4, 7]\n",
        "  d_arr = [4, 8, 16, 32, 64, 128]\n",
        "  df_model = []\n",
        "  df_results = dict()\n",
        "  df_embedding_dim = []\n",
        "  df_parameters = []\n",
        "  for d in d_arr:\n",
        "    results_dir = 'results/d{}'.format(d)\n",
        "    results_path = os.path.join(root_dir, results_dir, dataset_path)\n",
        "    for model_str, h in zip(models, H):\n",
        "      result = evaluate_results(model_str, results_path, alpha_str,\n",
        "                      is_npz=False, only_eval_result=True)\n",
        "      \n",
        "      for key in result:\n",
        "        if key in df_results:\n",
        "          df_results[key].append(result[key])\n",
        "        else:\n",
        "          df_results[key] = [result[key]]\n",
        "      \n",
        "      params = d * (vocab_size + h)\n",
        "      df_embedding_dim.append(d)\n",
        "      df_model.append(model_str)\n",
        "      df_parameters.append(params)\n",
        "      \n",
        "  d = {\n",
        "      'Model': df_model, \n",
        "      'Embedding size': df_embedding_dim,\n",
        "       'Parameters': df_parameters\n",
        "    }\n",
        "  for key, value in df_results.items():\n",
        "    d[key] = value\n",
        "\n",
        "  return d\n",
        "\n",
        "def get_cluster_results(clusters: List[int],\n",
        "                        alpha_str: str,\n",
        "                        only_eval_result: bool = True):\n",
        "\n",
        "  models = ['SUR', 'MUR_3', 'MUR_4', 'MUR_7']\n",
        "  d = 16 \n",
        "  df_model = []\n",
        "  df_results = dict()\n",
        "  df_embedding_dim = []\n",
        "  df_clusters = []\n",
        "  for cluster in clusters:\n",
        "    dataset_path = f'datasets/C{cluster}_I50_U5'\n",
        "    \n",
        "    if only_eval_result:\n",
        "      item_clusters = None\n",
        "      user_interests = None\n",
        "      test_cluster = None\n",
        "    else:\n",
        "      item_clusters, all_items, user_interests, test_cluster, _ = load_dataset(\n",
        "          root_dir, dataset_path, alpha_str, is_npz=False)\n",
        "\n",
        "    print (f'Getting results for {dataset_path} with {alpha_str} dataset.')\n",
        "    results_dir = f'results/d{d}'\n",
        "    results_path = os.path.join(root_dir, results_dir, dataset_path)\n",
        "    for model_str in models:\n",
        "      \n",
        "      result = evaluate_results(model_str, results_path, alpha_str, \n",
        "                                item_clusters = item_clusters, \n",
        "                                user_interests=user_interests, \n",
        "                                test_cluster=test_cluster,\n",
        "                                is_npz=False, only_eval_result=only_eval_result)\n",
        "      \n",
        "      for key in result:\n",
        "        if key in df_results:\n",
        "          df_results[key].append(result[key])\n",
        "        else:\n",
        "          df_results[key] = [result[key]]\n",
        "      \n",
        "      df_embedding_dim.append(d)\n",
        "      df_clusters.append(cluster)\n",
        "      df_model.append(model_str)\n",
        "      \n",
        "  d = {\n",
        "      'Model': df_model, \n",
        "      'Embedding size': df_embedding_dim,\n",
        "      'Clusters': df_clusters\n",
        "    }\n",
        "\n",
        "  for key, value in df_results.items():\n",
        "    d[key] = value\n",
        "\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4VL6bwUzK7I"
      },
      "outputs": [],
      "source": [
        "d_arr = [4, 8, 16, 32, 64, 128]\n",
        "K = 100\n",
        "\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"C=20, I=50, |Yu|=5, |U|={user_interests.shape[0]}\", fontsize=16)\n",
        "\n",
        "plot_metric = f'top_{K}_categorical_accuracy'\n",
        "\n",
        "for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "  gamma = 0.9 - alpha\n",
        "\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  title = alpha_str\n",
        "  d = get_dataset_results(dataset_path, alpha_str)\n",
        "  df = pd.DataFrame(data=d)\n",
        "  ax = sns.lineplot(\n",
        "      x='Embedding size', y=plot_metric, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  ax.legend(loc='upper right')\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel(f'HR@{K}')\n",
        "  ax.set_xticks(d_arr)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ3ns8JT8xje"
      },
      "outputs": [],
      "source": [
        "d_arr = [4, 8, 16, 32, 64, 128]\n",
        "K = 100\n",
        "plot_metric = f'top_{K}_categorical_accuracy'\n",
        "\n",
        "alphas = [0.4, 0.6, 0.8]\n",
        "\n",
        "alpha = alphas[0]\n",
        "gamma = 0.9 - alpha\n",
        "\n",
        "alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "dataset_path = 'datasets/C20_I50_U5/'\n",
        "d = get_dataset_results(dataset_path, alpha_str, vocab_size=1000)\n",
        "df = pd.DataFrame(data=d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvPJlpQlIgeI"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOS73QYlK8ru"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "ax = sns.lineplot(x='Parameters', y = plot_metric, hue='Model', data=df, markers=True, style=\"Model\")\n",
        "plt.legend(loc='upper right')\n",
        "# plt.xticks(d_arr)\n",
        "plt.ylabel(f'HR@{K}')\n",
        "# plt.title('Low interest volatility')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDDvJnoJ8qSn"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 3.5), constrained_layout=True)\n",
        "\n",
        "sns.lineplot(x='Embedding size', y=plot_metric, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xticks(d_arr)\n",
        "plt.ylabel(f'HR@{K}')\n",
        "plt.title('Low interest volatility')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLIWulMkHmGj"
      },
      "outputs": [],
      "source": [
        "# d_arr = np.array([4, 8, 16, 32, 64, 128])\n",
        "# d = np.tile(d_arr, (6))\n",
        "\n",
        "# num_items = 1000\n",
        "\n",
        "# MUR_3 = d_arr*(3+num_items)\n",
        "# MUR_4 = d_arr*(4+num_items)\n",
        "# MUR_7 = d_arr*(7+num_items)\n",
        "# SUR = d_arr*(1+num_items)\n",
        "\n",
        "# # params = SUR + MUR_3 + MUR_4 + MUR_7\n",
        "# params = np.concatenate([SUR, MUR_3, MUR_4, MUR_7])\n",
        "# model_str = ['SUR'] * 6 + ['MUR_3'] * 6 + ['MUR_4'] * 6 + ['MUR_7'] * 6\n",
        "\n",
        "# d_arr = np.tile(d_arr, (4))\n",
        "\n",
        "# d = {\n",
        "#       'Model': model_str, \n",
        "#       'Embedding Size': d_arr,\n",
        "#       'Parameters': params}\n",
        "# df = pd.DataFrame(data = d)\n",
        "\n",
        "# plt.figure()\n",
        "# ax = sns.lineplot(x='Parameters', y = '', hue='Model', data=df)\n",
        "\n",
        "# plt.figure()\n",
        "# ax = sns.barplot(\n",
        "#       x='Embedding Size', y='Parameters', hue=\"Model\", data=df)\n",
        "# # ax.legend(loc='upper right')\n",
        "# # ax.set_title('Model Parameters')\n",
        "# # ax.set_xticks(d_arr)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwqIWbMHy_b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0VqqXpa1AIC"
      },
      "outputs": [],
      "source": [
        "clusters = [20,30,40,50,60]\n",
        "K = 100\n",
        "alpha = 0.8\n",
        "gamma = 0.9 - alpha\n",
        "\n",
        "plot_metric = f'top_{K}_categorical_accuracy'\n",
        "alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "\n",
        "d = get_cluster_results(clusters, alpha_str)\n",
        "df = pd.DataFrame(data=d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "offphceJ2nC7"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 3.5), constrained_layout=True)\n",
        "\n",
        "sns.lineplot(x='Clusters', y=plot_metric, hue=\"Model\", data=df, markers=True,\n",
        "             style=\"Model\")\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "plt.ylabel(f'HR@{K}')\n",
        "plt.title('Low interest volatility')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVo2EMocQt4p"
      },
      "outputs": [],
      "source": [
        "clusters = [10,20,30,40,50,60]\n",
        "K = 100\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(9, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|={user_interests.shape[0]}\", fontsize=16)\n",
        "\n",
        "plot_metric = f'top_{K}_categorical_accuracy'\n",
        "alphas = [0.4, 0.6, 0.8]\n",
        "# for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "for ix, alpha in enumerate(alphas):\n",
        "  gamma = 0.9 - alpha\n",
        "\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  title = get_volatility_str(0.9 - alpha)\n",
        "  d = get_cluster_results(clusters, alpha_str)\n",
        "  df = pd.DataFrame(data=d)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y=plot_metric, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  ax.legend(loc='upper right', bbox_to_anchor=(1.0, 1.0))\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel(f'HR@{K}')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk2cQWVrzuaT"
      },
      "outputs": [],
      "source": [
        "clusters = [10,20,30,40,50,60]\n",
        "K = 100\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(9, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|={user_interests.shape[0]}\", fontsize=16)\n",
        "\n",
        "plot_metric = f'top_{K}_categorical_accuracy'\n",
        "alphas = [0.4, 0.6, 0.8]\n",
        "# for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "for ix, alpha in enumerate(alphas):\n",
        "  gamma = 0.9 - alpha\n",
        "\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  title = get_volatility_str(0.9 - alpha)\n",
        "  d = get_cluster_results(clusters, alpha_str)\n",
        "  df = pd.DataFrame(data=d)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y=plot_metric, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  ax.legend(loc='upper right', )\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel(f'HR@{K}')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyYtUti1uSQN"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "\n",
        "# dataset_path = 'datasets/C5_I10_U3/'\n",
        "# (item_clusters, all_items, user_interests, test_cluster) = load_dataset(\n",
        "#     root_dir, dataset_path, alpha_str, is_npz=True)\n",
        "# dataset_path = 'datasets/C20_I50_U5/'\n",
        "# (item_clusters, all_items, title = 'Dataset: {}, C=20, I=50, |Yu|=5'.format(alpha_str), test_cluster, user_item_sequences) = load_dataset(\n",
        "#     root_dir, dataset_path, alpha_str, is_npz=False)\n",
        "\n",
        "clusters = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "K = 100\n",
        "\n",
        "# fig, axs = plt.subplots(2, 4, figsize=(18, 6), constrained_layout=True, \n",
        "#                         sharex=True, sharey=True)\n",
        "# axs = axs.flat\n",
        "# fig.suptitle(f\"I=50, |Yu|=5, |U|={user_interests.shape[0]}\", fontsize=16)\n",
        "# plot_metric = f'top_{K}_categorical_accuracy'\n",
        "\n",
        "all_d = []\n",
        "for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "  gamma = 0.9 - alpha\n",
        "\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  dataset_path = 'datasets/C20_I50_U5/'\n",
        "  \n",
        "  # title = alpha_str\n",
        "  d = get_cluster_results(clusters, alpha_str, only_eval_result=False)\n",
        "  print (d)\n",
        "  all_d.append(d)\n",
        "  # df = pd.DataFrame(data=d)\n",
        "  # ax = sns.lineplot(\n",
        "  #     x='Clusters', y=plot_metric, hue=\"Model\", data=df,\n",
        "  #     markers=True, style=\"Model\", ax=axs[ix])\n",
        "  # ax.legend(loc='upper right')\n",
        "  # ax.set_title(title)\n",
        "  # ax.set_ylabel(f'HR@{K}')\n",
        "  # ax.set_xticks(clusters)\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enTKjUo5g9cF"
      },
      "outputs": [],
      "source": [
        "def get_cluster_anisotropy(clusters: List[int], \n",
        "                           alpha_str: str, \n",
        "                           only_eval_result: bool = True):\n",
        "\n",
        "  models = ['SUR', 'MUR_3', 'MUR_4', 'MUR_7']\n",
        "  d = 16 \n",
        "  df_model = []\n",
        "  df_results = dict()\n",
        "  df_embedding_dim = []\n",
        "  df_clusters = []\n",
        "  for cluster in clusters:\n",
        "    dataset_path = f'datasets/C{cluster}_I50_U5'\n",
        "    print (f'Getting results for {dataset_path} with {alpha_str} dataset.')\n",
        "    item_clusters, _, _, _, _ = load_dataset(\n",
        "        root_dir, dataset_path, alpha_str, is_npz=False)\n",
        "    results_dir = f'results/reruns/d{d}'\n",
        "    results_dir = os.path.join(root_dir, results_dir, dataset_path)\n",
        "    for model_str in models:\n",
        "      seed_list = [1234, 1235, 1236]\n",
        "      seed_found = len(seed_list)\n",
        "      mean_results = dict()\n",
        "      result = dict()\n",
        "      result['silhoutte'] = 0\n",
        "      result['anisotropy'] = 0\n",
        "\n",
        "      for seed in seed_list:\n",
        "        fpath = os.path.join(results_dir,\n",
        "                            'synthetic_data_{}'.format(alpha_str),\n",
        "                            'seed_{}'.format(seed),\n",
        "                            model_str, 'item_embeddings.npy')\n",
        "        with tf.io.gfile.GFile(fpath, 'rb') as f:\n",
        "            item_embeddings = np.load(f, allow_pickle=True)\n",
        "\n",
        "        result['anisotropy'] += compute_mean_cosine_similarity(item_embeddings)\n",
        "        result['silhoutte'] += evaluate_silhouette_score(item_embeddings,\n",
        "                                                         item_clusters)['silhouette_actual']\n",
        "      \n",
        "      \n",
        "      result['silhoutte'] /= seed_found\n",
        "      result['anisotropy'] /= seed_found\n",
        "      for key in result:\n",
        "        if key in df_results:\n",
        "          df_results[key].append(result[key])\n",
        "        else:\n",
        "          df_results[key] = [result[key]]\n",
        "      \n",
        "      df_embedding_dim.append(d)\n",
        "      df_clusters.append(cluster)\n",
        "      df_model.append(model_str)\n",
        "      \n",
        "  d = {\n",
        "      'Model': df_model, \n",
        "      'Embedding size': df_embedding_dim,\n",
        "      'Clusters': df_clusters\n",
        "    }\n",
        "\n",
        "  for key, value in df_results.items():\n",
        "    d[key] = value\n",
        "\n",
        "  return d\n",
        "\n",
        "clusters = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "d_embeddings = []\n",
        "for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  title = alpha_str\n",
        "  d_anisotropy = get_cluster_anisotropy(clusters, alpha_str)\n",
        "  d_embeddings.append(d_anisotropy)\n",
        "\n",
        "  df = pd.DataFrame(data=d_anisotropy)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y='anisotropy', hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  \n",
        "  ax.legend(loc='upper right')\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel('Anisotropy')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ6ZSRdeJ0cJ"
      },
      "outputs": [],
      "source": [
        "clusters = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "# d_embeddings = []\n",
        "for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  # d_anisotropy = get_cluster_anisotropy(clusters, alpha_str)\n",
        "  # d_embeddings.append(d_anisotropy)\n",
        "  \n",
        "  d_anisotropy = d_embeddings[ix]\n",
        "  df = pd.DataFrame(data=d_anisotropy)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y='anisotropy', hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  \n",
        "  ax.legend(loc='upper left')\n",
        "  title = alpha_str\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel('Anisottropy')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW33jsxlAfNw"
      },
      "outputs": [],
      "source": [
        "clusters = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "fig, axs = plt.subplots(2, 4, figsize=(18, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "# d_embeddings = []\n",
        "for ix, alpha in enumerate(np.linspace(0.2, 0.9, 8)):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  # d_anisotropy = get_cluster_anisotropy(clusters, alpha_str)\n",
        "  # d_embeddings.append(d_anisotropy)\n",
        "  \n",
        "  d_anisotropy = d_embeddings[ix]\n",
        "  df = pd.DataFrame(data=d_anisotropy)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y='silhoutte', hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  \n",
        "  ax.legend(loc='upper center')\n",
        "  title = alpha_str\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel('Sillhoutte')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gRPu1jFlw_E"
      },
      "outputs": [],
      "source": [
        "# results_dir = 'results/train-test-split'.format(root_dir)\n",
        "results_dir = 'results/'.format(root_dir)\n",
        "results_path = os.path.join(root_dir, results_dir, dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTw0NXkl5FH1"
      },
      "outputs": [],
      "source": [
        "model_str = 'SUR'\n",
        "\n",
        "print(results_path)\n",
        "\n",
        "mean_results = evaluate_results(model_str, results_path, is_npz=False)\n",
        "\n",
        "print()\n",
        "\n",
        "mean_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APczFHZHj2Mv"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_5'\n",
        "mean_results = evaluate_results(model_str, results_path, is_npz=False)\n",
        "print()\n",
        "mean_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHWhM_br4deJ"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_3'\n",
        "\n",
        "mean_results = evaluate_results(model_str, results_path, is_npz=False)\n",
        "print()\n",
        "mean_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2pzCqELHqsY"
      },
      "outputs": [],
      "source": [
        "seed = 1234\n",
        "model_str = 'SUR'\n",
        "alpha_str = 'alpha0.6_gamma0.3'\n",
        "results_path = '{}/results/train-test-split/{}/seed_{}/{}/{}/embeddings_data.npz'.format(\n",
        "        root_dir, dataset_path, seed, model_str, alpha_str)\n",
        "\n",
        "with tf.io.gfile.GFile(results_path, 'rb') as f:\n",
        "    data = np.load(f, allow_pickle=True)\n",
        "    result_eval = data['eval_result'][()]\n",
        "    item_embeddings = data['item_embeddings']\n",
        "    user_embeddings = data['user_embeddings']\n",
        "\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "\n",
        "plt.title('Medium interest volatility.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2QDhr7lHlJo"
      },
      "outputs": [],
      "source": [
        "dataset_path = 'datasets/C5_I10_U3/'\n",
        "item_clusters, _, user_interests, test_cluster, _ = load_dataset(root_dir, dataset_path, alpha_str, is_npz=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6tYAcckJ3Dv"
      },
      "outputs": [],
      "source": [
        "alpha_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY4cnli-L7Rr"
      },
      "outputs": [],
      "source": [
        "seed = 1234\n",
        "model_str = 'MUR_3'\n",
        "results_path = '{}/results/train-test-split/{}/seed_{}/{}/{}/embeddings_data.npz'.format(\n",
        "        root_dir, dataset_path, seed, model_str, alpha_str)\n",
        "\n",
        "with tf.io.gfile.GFile(results_path, 'rb') as f:\n",
        "    data = np.load(f, allow_pickle=True)\n",
        "    result_eval = data['eval_result'][()]\n",
        "    item_embeddings = data['item_embeddings']\n",
        "    user_embeddings = data['user_embeddings']\n",
        "\n",
        "interest_acc = evaluate_interest_retrieval(\n",
        "    user_embeddings[:20], item_embeddings, user_interests[:20], item_clusters,\n",
        "    K=20)\n",
        "\n",
        "np.mean(interest_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-MDG8kGBavK"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 5, figsize=(20, 14), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {} \u0026 InterestRecall@20: {:.2f}\"\"\".format(\n",
        "      sorted(user_interests[user_ix]), interest_acc[user_ix]))\n",
        "  ax.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgL4O4tRg5ez"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 5, figsize=(20, 14), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {} \u0026 InterestRecall@20: {:.2f}\"\"\".format(\n",
        "      sorted(user_interests[user_ix]), interest_acc[user_ix]))\n",
        "  ax.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQXaVJ3Iiw0K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SII54UCQ-AXJ"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 5, figsize=(18, 6), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {} \u0026 InterestRecall@20: {:.2f}\"\"\".format(\n",
        "      user_interests[user_ix], interest_acc[user_ix]))\n",
        "  ax.legend(loc = 'upper left')\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lSrt82SMc_D"
      },
      "outputs": [],
      "source": [
        "user_ix = 15\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=7.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('SUR for a user having interests: {}.'.format(sorted(user_interests[user_ix])), fontsize=11)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XCJcZsigenb"
      },
      "outputs": [],
      "source": [
        "user_ix = 15\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=4.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('SUR for a user having interests: {}.'.format(sorted(user_interests[user_ix])), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6iVLfdLh0Fa"
      },
      "outputs": [],
      "source": [
        "user_ix = 9\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=4.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('MUR (H=3) for a user having interests: {}.'.format(sorted(user_interests[user_ix])), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1jOy8W1h3g-"
      },
      "outputs": [],
      "source": [
        "user_ix = 9\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=4.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('SUR for a user having interests: {}.'.format(sorted(user_interests[user_ix])), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3BA91gDpKvh"
      },
      "outputs": [],
      "source": [
        "candidate_clusters_size = [2, 3, 5, 7, 10]\n",
        "silhouette_scores = []\n",
        "for cluster_size in candidate_clusters_size:\n",
        "  kmeans = KMeans(n_clusters = cluster_size, random_state=1234).fit(item_embeddings)\n",
        "  silhouette_scores.append(sklearn.metrics.silhouette_score(item_embeddings, kmeans.labels_, random_state=1234))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyk8XJZLYXsO"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/Test/data'\n",
        "data_path = '{}/synthetic_data.npz'.format(root_dir, dataset_path)\n",
        "\n",
        "with tf.io.gfile.GFile(data_path, 'rb') as f:\n",
        "    data = np.load(f, allow_pickle=True)\n",
        "    if 'item_clusters' in data:\n",
        "      item_clusters = data['item_clusters']\n",
        "    else:\n",
        "      item_clusters = data['item_labels']\n",
        "    all_items = data['items']\n",
        "    # user_interests = data['user_interests']\n",
        "    # test_cluster = item_clusters[data['user_item_sequences'][:, -1]]\n",
        "    seq = data['user_item_sequences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5krC0VwEgGgx"
      },
      "source": [
        "\n",
        "## Visualize learned embeddings for sparse datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8PK7dMzvQGC"
      },
      "outputs": [],
      "source": [
        "root_dir = root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "results_path = 'results/d2/validation/logQ-separate_embedding/lr0.1/'\n",
        "seed = 1236\n",
        "model_str = 'MUR_5'\n",
        "alphas = np.linspace(0.6, 0.9, 3)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for ax, alpha in zip(axs.flat, alphas):\n",
        "\n",
        "  alpha_str = f'interest-power1.0_item-power1.0_alpha{alpha:.1f}_gamma{0.9-alpha:.1f}/'\n",
        "  dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "  (item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "  # target_next_items = user_item_sequences[:, -1]\n",
        "\n",
        "  print (alpha_str)\n",
        "  item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'item_embeddings.npy')\n",
        "\n",
        "  user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'user_embeddings.npy')\n",
        "\n",
        "  # eval_results_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "  #                                     f'synthetic_data_{alpha_str}',\n",
        "  #                                     f'seed_{seed}', model_str,\n",
        "  #                                     'eval_result.yaml')\n",
        "\n",
        "  with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "    item_embeddings = np.load(f)\n",
        "\n",
        "  with tf.io.gfile.GFile(user_embeddings_path, 'rb') as f:\n",
        "    user_embeddings = np.load(f, allow_pickle=True)\n",
        "  #   print (user_embeddings[user_ix])\n",
        "  #   # user_embeddings /= np.linalg.norm(user_embeddings, axis=-1, keepdims=True)\n",
        "\n",
        "  # with tf.io.gfile.GFile(eval_results_path) as f:\n",
        "  #   result_eval = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "  # Print Cluster wise performance\n",
        "  # print_cluster_wise_performance(user_embeddings, item_embeddings,\n",
        "  #                              test_cluster, target_next_items)\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "  \n",
        "  ax.set_title(f'{get_volatility_str(0.9-alpha)}')\n",
        "\n",
        "plt.legend()\n",
        "fig.suptitle(f'Item embedding space using {model_str} for sparse data.',\n",
        "             fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmq2pZ1jku5W"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "results_path = 'results/d2/validation/logQ-separate_embedding/lr0.1/'\n",
        "seed = 1234\n",
        "model_str = 'SUR'\n",
        "alphas = reversed(np.linspace(0.5, 0.8, 3))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for ax, alpha in zip(axs.flat, alphas):\n",
        "\n",
        "  alpha_str = f'interest-power1.0_item-power1.0_alpha{alpha:.1f}_gamma{0.9-alpha:.1f}/'\n",
        "  dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "  (item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "  # target_next_items = user_item_sequences[:, -1]\n",
        "\n",
        "  print (alpha_str)\n",
        "  item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'item_embeddings.npy')\n",
        "\n",
        "  user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'user_embeddings.npy')\n",
        "\n",
        "  # eval_results_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "  #                                     f'synthetic_data_{alpha_str}',\n",
        "  #                                     f'seed_{seed}', model_str,\n",
        "  #                                     'eval_result.yaml')\n",
        "\n",
        "  with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "    item_embeddings = np.load(f)\n",
        "\n",
        "  with tf.io.gfile.GFile(user_embeddings_path, 'rb') as f:\n",
        "    user_embeddings = np.load(f, allow_pickle=True)\n",
        "  #   print (user_embeddings[user_ix])\n",
        "  #   # user_embeddings /= np.linalg.norm(user_embeddings, axis=-1, keepdims=True)\n",
        "\n",
        "  # with tf.io.gfile.GFile(eval_results_path) as f:\n",
        "  #   result_eval = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "  # Print Cluster wise performance\n",
        "  # print_cluster_wise_performance(user_embeddings, item_embeddings,\n",
        "  #                              test_cluster, target_next_items)\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "  \n",
        "  ax.set_title(f'{get_volatility_str(0.9-alpha)}')\n",
        "\n",
        "plt.legend()\n",
        "fig.suptitle(f'Item embedding space using {model_str} for sparse data.',\n",
        "             fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBbKszz38_dh"
      },
      "outputs": [],
      "source": [
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.7_gamma0.2/'\n",
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "\n",
        "item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'item_embeddings.npy')\n",
        "\n",
        "user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'user_embeddings.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9ZNg9F29Kw-"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 5, figsize=(20, 14), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {}\"\"\".format(sorted(set(user_interests[user_ix]))))\n",
        "  ax.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMox-KmsEQJ_"
      },
      "outputs": [],
      "source": [
        "user_ix = 0\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              s=rgb_alphas,\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "user_embeddings = tf.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=4.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "plt.title('SUR for a user having interests: {}.'.format(sorted(set(user_interests[user_ix]))), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz6z2Z46Gost"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "results_path = 'results/d2/validation/logQ-separate_embedding/lr0.1/'\n",
        "seed = 1235\n",
        "model_str = 'MUR_5'\n",
        "alphas = reversed(np.linspace(0.5, 0.8, 3))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for ax, alpha in zip(axs.flat, alphas):\n",
        "\n",
        "  alpha_str = f'interest-power1.0_item-power1.0_alpha{alpha:.1f}_gamma{0.9-alpha:.1f}/'\n",
        "  dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "  (item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "  # target_next_items = user_item_sequences[:, -1]\n",
        "\n",
        "  print (alpha_str)\n",
        "  item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'item_embeddings.npy')\n",
        "\n",
        "  user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                      f'synthetic_data_{alpha_str}',\n",
        "                                      f'seed_{seed}', model_str,\n",
        "                                      'user_embeddings.npy')\n",
        "\n",
        "  # eval_results_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "  #                                     f'synthetic_data_{alpha_str}',\n",
        "  #                                     f'seed_{seed}', model_str,\n",
        "  #                                     'eval_result.yaml')\n",
        "\n",
        "  with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "    item_embeddings = np.load(f)\n",
        "\n",
        "  with tf.io.gfile.GFile(user_embeddings_path, 'rb') as f:\n",
        "    user_embeddings = np.load(f, allow_pickle=True)\n",
        "  #   print (user_embeddings[user_ix])\n",
        "  #   # user_embeddings /= np.linalg.norm(user_embeddings, axis=-1, keepdims=True)\n",
        "\n",
        "  # with tf.io.gfile.GFile(eval_results_path) as f:\n",
        "  #   result_eval = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "  # Print Cluster wise performance\n",
        "  # print_cluster_wise_performance(user_embeddings, item_embeddings,\n",
        "  #                              test_cluster, target_next_items)\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "  \n",
        "  ax.set_title(f'{get_volatility_str(0.9-alpha)}')\n",
        "\n",
        "plt.legend()\n",
        "fig.suptitle(f'Item embedding space using {model_str} for sparse data.',\n",
        "             fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZcY9hgA6yo7"
      },
      "outputs": [],
      "source": [
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.8_gamma0.1/'\n",
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "\n",
        "item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'item_embeddings.npy')\n",
        "\n",
        "user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'user_embeddings.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pesDC-Q85qON"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 5, figsize=(20, 14), constrained_layout=True,\n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "for user_ix, ax in enumerate(axs.flat):\n",
        "\n",
        "  label_set = np.unique(item_clusters).astype(int)\n",
        "  cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "  for label in label_set:\n",
        "    label_indices = np.where(item_clusters == label)\n",
        "    rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "    ax.scatter(item_embeddings[label_indices,0], \n",
        "                item_embeddings[label_indices,1], \n",
        "                c=cmap(label),\n",
        "                s=rgb_alphas,\n",
        "                label='{}'.format(label))\n",
        "    \n",
        "  user_embeddings = tf.squeeze(user_embeddings)\n",
        "  if len(user_embeddings.shape) \u003e 2:\n",
        "    H = user_embeddings.shape[1]\n",
        "    ax.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0], user_embeddings[user_ix,:, 1], scale=7.)\n",
        "  else:\n",
        "    ax.quiver(0., 0., user_embeddings[user_ix, 0], user_embeddings[user_ix, 1], scale=6.)\n",
        "\n",
        "  ax.set_title(\"\"\"$Y_u$ = {}\"\"\".format(sorted(set(user_interests[user_ix]))))\n",
        "  ax.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "fig.suptitle('User representations for 10 different users.', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7AV1IBAuhvE"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.8_gamma0.1/'\n",
        "results_path = 'results/d2/validation/logQ-separate_embedding/lr0.1/'\n",
        "\n",
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, _, user_item_sequences) = dataset\n",
        "\n",
        "seed = 1235\n",
        "model_str = 'MUR_5'\n",
        "\n",
        "item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'item_embeddings.npy')\n",
        "\n",
        "with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "  item_embeddings = np.load(f)\n",
        "\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              s=rgb_alphas,\n",
        "              label='{}'.format(label))\n",
        "\n",
        "  \n",
        "plt.legend()\n",
        "plt.title('Output embedding space')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X0')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29ebCv3hw0Ln"
      },
      "outputs": [],
      "source": [
        "len(predictions[test_items == 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPLM-2LkuWKH"
      },
      "outputs": [],
      "source": [
        "test_next_items = user_item_sequences[:, -1]\n",
        "train_next_items = user_item_sequences[:, -3] # validation\n",
        "\n",
        "predictions = compute_top_k_elementwise_accuracy(\n",
        "    user_embeddings, item_embeddings, test_next_items, k=20)\n",
        "\n",
        "item_predictions = []\n",
        "for item_id in all_items:\n",
        "  item_predictions.append(np.mean(predictions[test_next_items == item_id]))\n",
        "\n",
        "plt.bar(all_items, item_predictions)\n",
        "plt.xlabel('Item ID')\n",
        "plt.ylabel('HR@20')\n",
        "plt.title('Average performance for each item.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az8wvtEI0EZo"
      },
      "outputs": [],
      "source": [
        "train_next_item_counter = collections.Counter(train_next_items)\n",
        "train_next_item_count = np.array([train_next_item_counter[item]\n",
        "                                  for item in all_items],\n",
        "                                 dtype=np.float32)\n",
        "\n",
        "total_count = np.sum(train_next_item_count)\n",
        "train_next_item_count /= total_count\n",
        "\n",
        "plt.bar(all_items, train_next_item_count)\n",
        "plt.xlabel('Item ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Empirical density of items.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdW-066uaRp4"
      },
      "outputs": [],
      "source": [
        "train_next_item_counter = collections.Counter(train_next_items)\n",
        "train_next_item_count = np.array([train_next_item_counter[item]\n",
        "                                  for item in all_items],\n",
        "                                 dtype=np.float32)\n",
        "\n",
        "total_count = np.sum(train_next_item_count)\n",
        "train_next_item_count /= total_count\n",
        "\n",
        "plt.bar(all_items, train_next_item_count)\n",
        "plt.xlabel('Item ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Empirical density of items.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM08AeBzy7SB"
      },
      "outputs": [],
      "source": [
        "stats.pearsonr(item_predictions, train_next_item_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfuxbu4-7B9D"
      },
      "outputs": [],
      "source": [
        "num_samples = 100000\n",
        "\n",
        "# uniformly sample indices\n",
        "sample_indices = np.random.choice(len(train_next_items), size=num_samples)\n",
        "# sample_indices = np.random.choice(train_next_items, size=num_samples)\n",
        "\n",
        "# get embeddings from train_next_items using sampled indices.\n",
        "data = item_embeddings[train_next_items[sample_indices]]\n",
        "\n",
        "data += np.random.normal(loc=0.0, scale=0.1, size=data.shape)\n",
        "kernel = stats.gaussian_kde(data.T)\n",
        "\n",
        "xmin = -2\n",
        "ymin = -2\n",
        "xmax = 2\n",
        "ymax = 2\n",
        "\n",
        "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
        "positions = np.vstack([X.ravel(), Y.ravel()])\n",
        "\n",
        "Z = np.reshape(kernel(positions).T, X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOvCJOP_1NP0"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.gca()\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "\n",
        "# Contourf plot\n",
        "cfset = ax.contourf(X, Y, Z, cmap='Blues')\n",
        "## Or kernel density estimate plot instead of the contourf plot\n",
        "#ax.imshow(np.rot90(f), cmap='Blues', extent=[xmin, xmax, ymin, ymax])\n",
        "\n",
        "# Contour plot\n",
        "cset = ax.contour(X, Y, Z, colors='k')\n",
        "\n",
        "# Label plot\n",
        "ax.clabel(cset, inline=1, fontsize=10)\n",
        "ax.set_xlabel('X1')\n",
        "ax.set_ylabel('X0')\n",
        "ax.set_title('Density in the Output Embedding Space using KDE.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ0e-L770xLq"
      },
      "outputs": [],
      "source": [
        "density = kernel(item_embeddings[all_items].T)\n",
        "density.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6nbK0wh87rR"
      },
      "outputs": [],
      "source": [
        "stats.pearsonr(item_predictions, density)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDz2l8Rw408R"
      },
      "outputs": [],
      "source": [
        "## Plot user "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9IPt3wJnHUv"
      },
      "outputs": [],
      "source": [
        "user_ix = 1\n",
        "fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
        "\n",
        "# for user_ix, ax in enumerate(axs.flat):\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  rgb_alphas = list(reversed(np.linspace(5.0, 50.0, len(label_indices[0]))))\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              s=rgb_alphas,\n",
        "              label='{}'.format(label))\n",
        "  \n",
        "# user_embeddings = np.squeeze(user_embeddings)\n",
        "# if len(user_embeddings.shape) \u003e 2:\n",
        "#   H = user_embeddings.shape[1]\n",
        "#   plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "#             user_embeddings[user_ix,:, 1], scale=4.)\n",
        "# else:\n",
        "#   plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "#             user_embeddings[user_ix, 1], scale=1.)\n",
        "\n",
        "# ax.set_title(\"\"\"$Y_u$ = {} \u0026 Interest Recall: {:.2f}\"\"\".format(\n",
        "#     user_interests[user_ix], interest_acc[user_ix]))\n",
        "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 1.0))\n",
        "\n",
        "# plt.title('MUR (H=3) for a user having interests: {}.'.format(sorted(user_interests[user_ix])), fontsize=11)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MgkWBhGDMF6"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.6_gamma0.3/'\n",
        "# results_path = 'results/d2/validation/logQ-separate-embedding/lr0.1/'\n",
        "results_path = 'results/d2/validation/logQ-separate_embedding-sample_weight/lr0.1/'\n",
        "\n",
        "# dataset_path = 'datasets/New_C5_I10_U3/'\n",
        "# alpha_str = 'alpha0.6_gamma0.3/'\n",
        "\n",
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, _, user_item_sequences) = dataset\n",
        "\n",
        "seed = 1236\n",
        "model_str = 'SUR'\n",
        "\n",
        "item_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'item_embeddings.npy')\n",
        "\n",
        "user_embeddings_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "                                    f'synthetic_data_{alpha_str}',\n",
        "                                    f'seed_{seed}', model_str,\n",
        "                                    'user_embeddings.npy')\n",
        "\n",
        "# eval_results_path = os.path.join(root_dir, results_path, dataset_path,\n",
        "#                                     f'synthetic_data_{alpha_str}',\n",
        "#                                     f'seed_{seed}', model_str,\n",
        "#                                     'eval_result.yaml')\n",
        "\n",
        "with tf.io.gfile.GFile(item_embeddings_path, 'rb') as f:\n",
        "  item_embeddings = np.load(f)\n",
        "\n",
        "with tf.io.gfile.GFile(user_embeddings_path, 'rb') as f:\n",
        "  user_embeddings = np.load(f, allow_pickle=True)\n",
        "\n",
        "# with tf.io.gfile.GFile(eval_results_path) as f:\n",
        "#   result_eval = yaml.safe_load(f)\n",
        "\n",
        "label_set = np.unique(item_clusters).astype(int)\n",
        "cmap = plt.cm.get_cmap('RdYlBu', len(label_set)+1)\n",
        "for label in label_set:\n",
        "  label_indices = np.where(item_clusters == label)\n",
        "  plt.scatter(item_embeddings[label_indices,0], \n",
        "              item_embeddings[label_indices,1], \n",
        "              c=cmap(label),\n",
        "              label='{}'.format(label))\n",
        "\n",
        "user_embeddings = np.squeeze(user_embeddings)\n",
        "if len(user_embeddings.shape) \u003e 2:\n",
        "  H = user_embeddings.shape[1]\n",
        "  plt.quiver(np.zeros(H), np.zeros(H), user_embeddings[user_ix,:, 0],\n",
        "            user_embeddings[user_ix,:, 1], scale=4.)\n",
        "else:\n",
        "  plt.quiver(0., 0., user_embeddings[user_ix, 0],\n",
        "            user_embeddings[user_ix, 1], scale=5.)\n",
        "  \n",
        "plt.legend()\n",
        "plt.title('Low interest volatility.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6ZRDSZGzmxZ"
      },
      "outputs": [],
      "source": [
        "# init plot data df data\n",
        "cluster_indices = []\n",
        "hr5 = []\n",
        "hr10 = []\n",
        "models = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H-ptC_6SFq0"
      },
      "outputs": [],
      "source": [
        "root_dir = 'root_dir/'\n",
        "dataset_path = 'datasets/sparse_C5_I10_U3_N10000/'\n",
        "alpha_str = 'interest-power1.0_item-power1.0_alpha0.8_gamma0.1/'\n",
        "# Wihtout sample_weighting\n",
        "# results_dir = 'results/d2/reruns/logQ-separate-embedding/lr0.1/'\n",
        "\n",
        "# With sample_weight\n",
        "results_dir = 'results/d2/reruns/logQ-separate_embedding-sample_weight/lr0.1/'\n",
        "\n",
        "results_path = os.path.join(root_dir, results_dir, dataset_path)    \n",
        "\n",
        "dataset = load_dataset(root_dir, dataset_path, alpha_str)\n",
        "(item_clusters, all_items, user_interests, test_cluster, user_item_sequences) = dataset\n",
        "\n",
        "model_str = 'SUR'\n",
        "sur_results = evaluate_results(model_str, results_path, alpha_str, item_clusters, \n",
        "                               user_interests, test_cluster, k_list=[1, 5, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSsPR-NnwEzA"
      },
      "outputs": [],
      "source": [
        "for cluster_ix in range(5):\n",
        "  hr5.append(sur_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(sur_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append('SUR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pZZ7XYkrPbR"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_3'\n",
        "mur3_results = evaluate_results(model_str, results_path, alpha_str, item_clusters, \n",
        "                 user_interests, test_cluster, k_list=[1, 5, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyGEC9Z0z6gh"
      },
      "outputs": [],
      "source": [
        "for cluster_ix in range(5):\n",
        "  hr5.append(mur3_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(mur3_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append('MUR_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQw7EfgXrvSv"
      },
      "outputs": [],
      "source": [
        "model_str = 'MUR_5'\n",
        "mur5_results = evaluate_results(model_str, results_path, alpha_str,\n",
        "                                item_clusters, user_interests, test_cluster,\n",
        "                                k_list=[1, 5, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z_S_XJGb2SJ"
      },
      "outputs": [],
      "source": [
        "for cluster_ix in range(5):\n",
        "  hr5.append(mur5_results[f'HR@5_Cluster{cluster_ix}'])\n",
        "  hr10.append(mur5_results[f'HR@10_Cluster{cluster_ix}'])\n",
        "  cluster_indices.append(cluster_ix)\n",
        "  models.append('MUR_5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtSdvP1Z075F"
      },
      "outputs": [],
      "source": [
        "models_FW = [m + '_FW' for m in models]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfBekCpk0Tz8"
      },
      "outputs": [],
      "source": [
        "# Plotting cluster slice performance\n",
        "\n",
        "# Subplot for alpha_str\n",
        "d = {'Model': models_FW, 'HR@10': hr10, 'HR@5': hr5, 'Cluster': cluster_indices}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.hlines(y=sur_results['top_5_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='teal', alpha=0.6)\n",
        "ax.hlines(y=mur3_results['top_5_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='coral', alpha=0.6)\n",
        "ax.hlines(y=mur5_results['top_5_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='steelblue', alpha=0.6)\n",
        "\n",
        "sns.barplot(x=\"Cluster\", y=\"HR@5\", hue=\"Model\", data=df, ax=ax,\n",
        "            palette = {'SUR_FW': 'teal', 'MUR_3_FW': 'coral', 'MUR_5_FW': 'steelblue'})\n",
        "plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9qKTh6ploS-"
      },
      "outputs": [],
      "source": [
        "# Plotting cluster slice performance\n",
        "\n",
        "# Subplot for alpha_str\n",
        "d = {'Model': models, 'HR@10': hr10, 'HR@5': hr5, 'Cluster': cluster_indices}\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.hlines(y=sur_results['top_10_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='teal', alpha=0.6)\n",
        "ax.hlines(y=mur3_results['top_10_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='coral', alpha=0.6)\n",
        "ax.hlines(y=mur5_results['top_10_categorical_accuracy'], xmin=-0.5, xmax=4.5,\n",
        "          colors='steelblue', alpha=0.6)\n",
        "\n",
        "sns.barplot(x=\"Cluster\", y=\"HR@10\", hue=\"Model\", data=df, ax=ax,\n",
        "            palette = {'SUR': 'teal', 'MUR_3': 'coral', 'MUR_5': 'steelblue'})\n",
        "plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyLTvVqORe2O"
      },
      "outputs": [],
      "source": [
        "def get_cluster_results(clusters: List[int],\n",
        "                        alpha_str: str,\n",
        "                        only_eval_result: bool = True):\n",
        "\n",
        "  # models = ['SUR', 'MUR_3', 'MUR_5', 'MUR_7']\n",
        "  models = ['SUR', 'MUR_5', 'MUR_7']\n",
        "  d = 16 \n",
        "  df_model = []\n",
        "  df_results = dict()\n",
        "  df_embedding_dim = []\n",
        "  df_clusters = []\n",
        "  for cluster in clusters:\n",
        "    dataset_path = f'datasets/sparse_C{cluster}_I50_U3_N10000'\n",
        "    \n",
        "    if only_eval_result:\n",
        "      item_clusters = None\n",
        "      user_interests = None\n",
        "      test_cluster = None\n",
        "    else:\n",
        "      item_clusters, all_items, user_interests, test_cluster, _ = load_dataset(\n",
        "          root_dir, dataset_path, alpha_str, is_npz=False)\n",
        "\n",
        "    print (f'Getting results for {dataset_path} with {alpha_str} dataset.')\n",
        "    results_dir = f'results/d{d}'\n",
        "    results_path = os.path.join(root_dir, results_dir, dataset_path)\n",
        "    for model_str in models:\n",
        "      \n",
        "      result = evaluate_results(model_str, results_path, alpha_str, \n",
        "                                item_clusters = item_clusters, \n",
        "                                user_interests=user_interests, \n",
        "                                test_cluster=test_cluster,\n",
        "                                is_npz=False, only_eval_result=only_eval_result)\n",
        "      \n",
        "      for key in result:\n",
        "        if key in df_results:\n",
        "          df_results[key].append(result[key])\n",
        "        else:\n",
        "          df_results[key] = [result[key]]\n",
        "      \n",
        "      df_embedding_dim.append(d)\n",
        "      df_clusters.append(cluster)\n",
        "      df_model.append(model_str)\n",
        "      \n",
        "  d = {\n",
        "      'Model': df_model, \n",
        "      'Embedding size': df_embedding_dim,\n",
        "      'Clusters': df_clusters\n",
        "    }\n",
        "\n",
        "  for key, value in df_results.items():\n",
        "    d[key] = value\n",
        "\n",
        "  return d\n",
        "\n",
        "clusters = [20, 30, 40, 50, 60]\n",
        "fig, axs = plt.subplots(1, 4, figsize=(18, 6), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "all_results = []\n",
        "metric_name = 'top_100_categorical_accuracy'\n",
        "\n",
        "for ix, alpha in enumerate(np.linspace(0.5, 0.8, 4)):\n",
        "  gamma = 0.9 - alpha\n",
        "  alpha_str = 'interest-power2.0_item-power2.0_alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  results = get_cluster_results(clusters, alpha_str)\n",
        "  # d_embeddings.append(d_anisotropy)\n",
        "  all_results.append(results)\n",
        "  \n",
        "  df = pd.DataFrame(data=results)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y=metric_name, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  \n",
        "  ax.legend(loc='upper center')\n",
        "  title = alpha_str\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel(metric_name)\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H23Ba6B5XUo8"
      },
      "outputs": [],
      "source": [
        "clusters = [20, 30, 40, 50, 60]\n",
        "fig, axs = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True, \n",
        "                        sharex=True, sharey=True)\n",
        "axs = axs.flat\n",
        "fig.suptitle(f\"I=50, |Yu|=5, |U|=50000\", fontsize=16)\n",
        "metric_name = 'top_100_categorical_accuracy'\n",
        "alphas = [0.5, 0.8]\n",
        "for ix, alpha in enumerate(alphas):\n",
        "  print (alpha)\n",
        "  gamma = 0.9 - alpha\n",
        "  # alpha_str = 'interest-power1.0_item-power1.0_alpha{:0.1f}_gamma{:0.1f}'.format(alpha, gamma)\n",
        "  # results = get_cluster_results(clusters, alpha_str)\n",
        "  # d_embeddings.append(d_anisotropy)\n",
        "  results = all_results[ix]\n",
        "  \n",
        "  df = pd.DataFrame(data=results)\n",
        "  ax = sns.lineplot(\n",
        "      x='Clusters', y=metric_name, hue=\"Model\", data=df,\n",
        "      markers=True, style=\"Model\", ax=axs[ix])\n",
        "  \n",
        "  if ix == 1:\n",
        "    ax.legend(loc='upper right')\n",
        "  else:\n",
        "    ax.get_legend().remove()\n",
        "\n",
        "  \n",
        "  title = alpha_str\n",
        "  ax.set_title(get_volatility_str(1-alpha))\n",
        "  ax.set_ylabel('HR@100')\n",
        "  ax.set_xticks(clusters)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Visualize_Embeddings.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1i2173WLG8NLvRoVlSunoYfN3xhPHWnvK",
          "timestamp": 1631309272438
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
