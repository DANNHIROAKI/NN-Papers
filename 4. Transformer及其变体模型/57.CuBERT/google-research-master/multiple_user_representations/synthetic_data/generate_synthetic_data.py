# coding=utf-8
# Copyright 2023 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""Generates and saves synthetic data for analysis.
"""

import os
from typing import Any, Dict, List, Sequence, Text

from absl import app
from absl import flags
from absl import logging
import numpy as np
from tensorflow.io import gfile
from multiple_user_representations.synthetic_data import util

FLAGS = flags.FLAGS

flags.DEFINE_string('output_file_path', None, 'Path to store the dataset.')
flags.DEFINE_enum('type', 'conditional', [
    'global', 'conditional', 'mixture_num_interests',
    'mixture_interest_volatility', 'sparse_conditional'
], 'The type of synthetic dataset to generate. For details, see the proposal doc: http://shortn/_PO6OdvUuAs#heading=h.1jb3wn2ie0bq')
flags.DEFINE_integer('num_clusters', 3, 'Number of clusters for items.')
flags.DEFINE_integer('num_data_points', 10000,
                     'Number of user sequences to generate.')
flags.DEFINE_integer('items_per_cluster', 30, 'Items in each cluster.')
flags.DEFINE_integer('interests_per_user', 2,
                     'Number of interests assigned to each user.')
flags.DEFINE_integer('num_items_in_seq', 20, 'Number of items in the sequence.')
flags.DEFINE_float(
    'alpha',
    0.8,
    """Probability of staying in a cluster. See the data generating mechanism at http://shortn/_PO6OdvUuAs#heading=h.1jb3wn2ie0bq for the role of alpha.""",
    lower_bound=0.0,
    upper_bound=1.0)
flags.DEFINE_float(
    'gamma',
    0.1,
    """Probability of transition to a different interesting cluster. See the data generating mechanism at http://shortn/_PO6OdvUuAs#heading=h.1jb3wn2ie0bq for the role of gamma.""",
    lower_bound=0.0,
    upper_bound=1.0)
flags.DEFINE_float(
    'epsilon',
    0.2,
    """Probability of staying in a "not-interesting" cluster. See the data generating mechanism at http://shortn/_PO6OdvUuAs#heading=h.1jb3wn2ie0bq. """,
    lower_bound=0.0,
    upper_bound=1.0)
flags.DEFINE_float('interest_power', 1.0,
                   """Exponent for user interests power law.""")
flags.DEFINE_float('item_power', 0.5,
                   """Exponent for item power law within a cluster.""")


def save_file(path, **data):
  """Saves numpy data arrays."""

  gfile.makedirs(path)
  for arr_name, arr in data.items():
    with gfile.GFile(os.path.join(path, arr_name + '.npy'), 'wb') as f:
      np.save(f, arr)


def generate_global_markovian_data(num_clusters,
                                   items_per_cluster,
                                   num_data_points = 10000,
                                   time_steps = 20,
                                   alpha = 0.9):
  """Generates user-item histories using a hierarchical markovian process.

  Assumes that each cluster of items depict a user interest and each cluster has
  a fixed number of items. The user-item interactions are generated by
  considering a discrete stochastic process for cluster transitions. The
  function assumes that the transition matrix has `alpha` as the diagonal and
  `(1-alpha)/(num_interests - 1)` as the off-diagonal elements.

  Args:
    num_clusters: The total number of clusters. The clusters will correspond to
      a state in the discrete markov process.
    items_per_cluster: The number of items belonging to each cluster.
    num_data_points: The number of data points (user sequences) to genertate.
    time_steps: The number of steps for which the markov chain is run.
    alpha: The diagonal entries of the interest transition matrix.

  Returns:
    A dict containg keys 'user_item_sequences' corresponding to user histories,
    'items' corresponding to item ids, `item_labels` corresponding to the
    mapping of items to their respecive clusters.
  """

  items = np.arange(items_per_cluster * num_clusters)
  item_clusters = np.ones((items_per_cluster * num_clusters,)) * -1

  # Transition matrix for the discrete markov process.
  transition_matrix = np.ones((num_clusters, num_clusters)) * (1.0 - alpha) / (
      num_clusters - 1)
  for ix in range(num_clusters):
    transition_matrix[ix, ix] = alpha
    item_clusters[ix * items_per_cluster:(ix + 1) * items_per_cluster] = ix

  initial_state_dist = np.ones((num_clusters,)) * 1.0 / num_clusters
  user_item_sequences = np.ones((num_data_points, time_steps), dtype=int) * -1
  for u in range(num_data_points):
    user_interest_sequence = util.run_markov_chain(transition_matrix,
                                                   initial_state_dist,
                                                   time_steps)
    user_item_sequences[u] = util.generate_item_sequence_from_interest_sequence(
        user_interest_sequence, items_per_cluster)

  dataset = dict(
      user_item_sequences=user_item_sequences,
      items=items,
      item_clusters=item_clusters)

  return dataset


def generate_user_specific_markovian_data(
    num_clusters,
    items_per_cluster,
    clusters_per_user,
    num_data_points = 10000,
    time_steps = 20,
    alpha = 0.8,
    gamma = 0.1,
    epsilon = 0.2,
    interest_power = 0.0,
    item_power = 0.0):
  """Generates user-item histories using a conditional hierarchical markovian process.

  Assumes that each cluster of items depict a user interest and each cluster has
  a fixed number of items. Each user is assumed to have `cluster_per_user`
  interests. The user-item interactions are generated by considering a discrete
  stochastic process for cluster transitions. The transition matrix is user
  specific and depends on the interests of the user. While running the markov
  chain, the transition matrix is assumed to be:

  transition[i,j] = alpha, if `i=j` corresponds to a user interest cluster.
  transition[i,j] = gamma / (clusters_per_user-1), if `i!=j` and both `i` and
    `j` correspond to user's interest clusters.
  transition[i,j] = (1-alpha-gamma)/(H-clusters_per_user), if only `i`
    corresponds to one of user's interest clusters.
  transition[i,j] = epsilon, if `i=j` corresponds to a cluster not in user's
    interests.
  transition[i,j] = 0.0, if `i` and `j` corresponds to clusters not in user's
    interests.
  transition[i,j] = (1-epsilon) / clusters_per_user, if `i` corresponds to a
    cluster not in user's interests and 'j' corresponds to a user interest
    cluster.

  Args:
    num_clusters: The total number of clusters. The clusters will correspond to
      a state in the discrete markov process.
    items_per_cluster: The number of items belonging to each cluster.
    clusters_per_user: The number of interests a user may have. Should be less
      than `num_clusters`.
    num_data_points: The number of data points (user sequences) to genertate.
    time_steps: The number of steps for which the markov chain is run.
    alpha: The transition probability to stay in a user interest cluster.
    gamma: The total probability of transition to a different user interest
      cluster. Note alpha + gamma <= 1
    epsilon: The transition probability to stay in a cluster which is not a user
      interest.
    interest_power: The exponent for power-law distirbution of interests. If
      zero, the distribution will be uniform.
    item_power: The exponent for power-law distribution of items in an interest.
      If zero, the distribution will be uniform.

  Returns:
    A dict containg keys 'user_item_sequences' corresponding to user histories,
    `user_interests` corresponding to a subset of clusters that reflect the
    interest of users, 'items' corresponding to item ids, and `item_clusters`
    corresponding to the mapping of items to their respecive clusters.

  Raises:
    ValueError: If alpha + gamma > 1.
  """

  if alpha + gamma > 1:
    raise ValueError('Probability of transitions cannot be greater than 1.')

  items = np.arange(items_per_cluster * num_clusters)
  interests = np.arange(num_clusters, dtype=int)

  # Allocate interests to users by random cluster assignment.
  def random_choice():
    return np.random.choice(interests, size=clusters_per_user, replace=False)

  if interest_power > 0.0:
    # Sample with replacement to get power-law distribution.
    prob = (interests + 1)**(-1.0 * interest_power)
    prob /= np.sum(prob)
    assigned_interests = np.random.choice(
        interests,
        size=(num_data_points, clusters_per_user),
        replace=True,
        p=prob)
  else:
    # Sample without replacement for each user using a uniform distribution.
    assigned_interests = np.array(
        [random_choice() for ix in range(num_data_points)])

  def get_transition_probability(user_interest_set,
                                 src_interest_state):
    """Returns the `src_state` row of the user-specific transition matrix."""

    src_state_interesting = src_interest_state in user_interest_set
    num_interests = len(user_interest_set)

    if src_state_interesting:

      exploration_prob = (1 - alpha - gamma) / (num_clusters - num_interests)
      transition_probs = np.ones((num_clusters,)) * exploration_prob
      for j in range(num_clusters):
        if j == src_interest_state:
          if num_interests == 1:
            transition_probs[j] = alpha + gamma
          else:
            transition_probs[j] = alpha
        elif j in user_interest_set:
          transition_probs[j] = gamma / (num_interests - 1)
    else:
      transition_probs = np.zeros((num_clusters,))
      transition_probs[src_interest_state] = epsilon
      transition_probs[user_interest_set] = (1 - epsilon) / num_interests

    return transition_probs

  def get_transition_matrix(user_interest_set):
    """Returns the transition matrix of a user given user_interest_set."""

    transition_matrix = np.zeros((num_clusters, num_clusters))
    for src_state in range(num_clusters):
      transition_matrix[src_state] = get_transition_probability(
          user_interest_set, src_state)
    return transition_matrix

  # Assume state distribution at time=0 is uniform.
  initial_state_dist = np.ones((num_clusters,)) * 1.0 / num_clusters
  user_item_sequences = np.ones((num_data_points, time_steps), dtype=int) * -1
  for u in range(num_data_points):
    interest_set = list(set(assigned_interests[u]))
    transition_matrix = get_transition_matrix(interest_set)
    user_interest_sequence = util.run_markov_chain(transition_matrix,
                                                   initial_state_dist,
                                                   time_steps)
    user_item_sequences[u] = util.generate_item_sequence_from_interest_sequence(
        user_interest_sequence, items_per_cluster, item_power)

  item_clusters = np.ones((items_per_cluster * num_clusters,)) * -1
  for ix in range(num_clusters):
    item_clusters[ix * items_per_cluster:(ix + 1) * items_per_cluster] = ix

  dataset = dict(
      user_item_sequences=user_item_sequences,
      user_interests=assigned_interests,
      items=items,
      item_clusters=item_clusters)

  return dataset


def generate_heterogeneuos_user_slices(num_clusters,
                                       items_per_cluster,
                                       clusters_per_user_list,
                                       num_data_points = 10000,
                                       time_steps = 20,
                                       alpha = 0.8,
                                       gamma = 0.1,
                                       epsilon = 0.2):
  """Generate dataset with multiple slices each with distinct clusters_per_user.

  Each dataset slice has equal number of users. Example: If num_data_points=100
  and number of slices is 4, then each slice will have 25 users.

  Args:
    num_clusters: The total number of clusters. The clusters will correspond to
      a state in the discrete markov process.
    items_per_cluster: The number of items belonging to each cluster.
    clusters_per_user_list: A list depicting number of interests to assign for
      each user. For each element of the list, we generate one dataset slice.
      For example, if clusters_per_user_list = [2,3,5], then there will be 3
      dataset slices and the users in the dataset slices will have 2,3, and 5
      interests per user.
    num_data_points: The number of data points (user sequences) to generate.
    time_steps: The number of steps for which the markov chain is run.
    alpha: The transition probability to stay in a user interest cluster.
    gamma: The total probability of transition to a different user interest
      cluster. Note alpha + gamma <= 1
    epsilon: The transition probability to stay in a cluster which is not a user
      interest.

  Returns:
    dataset: A dictionary containing the following keys
      'user_item_sequences': User item sequence corresponding to user histories.
      `user_interests_slice_k`: User interests for the kth slice of the dataset.
      'items' corresponding to item ids.
      `item_clusters` corresponding to the mapping of items to their respective
        clusters.
  """

  dataset = dict()
  user_item_sequences = []
  for k, cluster_per_user in enumerate(clusters_per_user_list):
    dataset_slice = generate_user_specific_markovian_data(
        num_clusters=num_clusters,
        items_per_cluster=items_per_cluster,
        clusters_per_user=cluster_per_user,
        num_data_points=num_data_points // len(clusters_per_user_list),
        time_steps=time_steps,
        alpha=alpha,
        gamma=gamma,
        epsilon=epsilon)

    dataset[f'user_interests_slice_{k+1}'] = dataset_slice['user_interests']
    user_item_sequences.append(dataset_slice['user_item_sequences'])

  dataset['user_item_sequences'] = np.concatenate(user_item_sequences, axis=0)
  # items and item_clusters remain same across dataset slices.
  dataset['items'] = dataset_slice['items']
  dataset['item_clusters'] = dataset_slice['item_clusters']

  return dataset


def generate_mixture_interest_volatility_users(
    num_clusters,
    items_per_cluster,
    clusters_per_user,
    alpha_list,
    num_data_points = 10000,
    time_steps = 20,
    epsilon = 0.2):
  """Generate dataset with multiple slices each with distinct user volatility.

  Each dataset slice has equal number of users. Example: If num_data_points=100
  and number of slices is 4, then each slice will have 25 users.

  Args:
    num_clusters: The total number of clusters. The clusters will correspond to
      a state in the discrete markov process.
    items_per_cluster: The number of items belonging to each cluster.
    clusters_per_user: The number of interests a user has.
    alpha_list: The transition probability to stay in a user interest cluster.
      For each alpha in list, one user slice is created.
    num_data_points: The number of data points (user sequences) to generate.
    time_steps: The number of steps for which the markov chain is run.
    epsilon: The transition probability to stay in a cluster which is not a user
      interest.

  Returns:
    dataset: A dictionary containg the following keys
      'user_item_sequences': User item sequence corresponding to user histories.
      `user_interests`: User interests.
      'items' corresponding to item ids.
      `item_clusters`: corresponding to the mapping of items to their respective
        clusters.
  """

  dataset = dict()
  user_item_sequences = []
  user_interests = []
  for alpha in alpha_list:
    gamma = 0.9 - alpha
    dataset_slice = generate_user_specific_markovian_data(
        num_clusters=num_clusters,
        items_per_cluster=items_per_cluster,
        clusters_per_user=clusters_per_user,
        num_data_points=num_data_points // len(alpha_list),
        time_steps=time_steps,
        alpha=alpha,
        gamma=gamma,
        epsilon=epsilon)

    user_interests.append(dataset_slice['user_interests'])
    user_item_sequences.append(dataset_slice['user_item_sequences'])

  dataset['user_interests'] = np.concatenate(user_interests, axis=0)
  dataset['user_item_sequences'] = np.concatenate(user_item_sequences, axis=0)
  # items and item_clusters remain same across dataset slices.
  dataset['items'] = dataset_slice['items']
  dataset['item_clusters'] = dataset_slice['item_clusters']

  return dataset


def main(argv):

  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')

  dataset_type = FLAGS.type
  if dataset_type == 'conditional':
    dataset = generate_user_specific_markovian_data(
        num_clusters=FLAGS.num_clusters,
        clusters_per_user=FLAGS.interests_per_user,
        items_per_cluster=FLAGS.items_per_cluster,
        num_data_points=FLAGS.num_data_points,
        time_steps=FLAGS.num_items_in_seq,
        alpha=FLAGS.alpha,
        gamma=FLAGS.gamma,
        epsilon=FLAGS.epsilon)
    alpha_str = 'alpha{}_gamma{}'.format(FLAGS.alpha, FLAGS.gamma)
  elif dataset_type == 'mixture_num_interests':
    dataset = generate_heterogeneuos_user_slices(
        num_clusters=FLAGS.num_clusters,
        clusters_per_user_list=[7, 5, 3, 2],
        items_per_cluster=FLAGS.items_per_cluster,
        num_data_points=FLAGS.num_data_points,
        time_steps=FLAGS.num_items_in_seq,
        alpha=FLAGS.alpha,
        gamma=FLAGS.gamma,
        epsilon=FLAGS.epsilon)
    alpha_str = 'alpha{}_gamma{}'.format(FLAGS.alpha, FLAGS.gamma)
  elif dataset_type == 'mixture_interest_volatility':
    alpha_mixture = [0.9, 0.8, 0.7, 0.6]
    dataset = generate_mixture_interest_volatility_users(
        num_clusters=FLAGS.num_clusters,
        clusters_per_user=FLAGS.interests_per_user,
        items_per_cluster=FLAGS.items_per_cluster,
        num_data_points=FLAGS.num_data_points,
        time_steps=FLAGS.num_items_in_seq,
        alpha_list=alpha_mixture,
        epsilon=FLAGS.epsilon)
    alpha_str = 'mixture_alpha{}'.format('_'.join(map(str, alpha_mixture)))
  elif dataset_type == 'sparse_conditional':
    dataset = generate_user_specific_markovian_data(
        num_clusters=FLAGS.num_clusters,
        clusters_per_user=FLAGS.interests_per_user,
        items_per_cluster=FLAGS.items_per_cluster,
        num_data_points=FLAGS.num_data_points,
        alpha=FLAGS.alpha,
        gamma=FLAGS.gamma,
        epsilon=FLAGS.epsilon,
        time_steps=FLAGS.num_items_in_seq,
        interest_power=FLAGS.interest_power,
        item_power=FLAGS.item_power)
    alpha_str = 'interest-power{}_item-power{}_alpha{}_gamma{}'.format(
        FLAGS.interest_power, FLAGS.item_power, FLAGS.alpha, FLAGS.gamma)
  else:
    dataset = generate_global_markovian_data(
        num_clusters=FLAGS.num_clusters,
        items_per_cluster=FLAGS.items_per_cluster,
        num_data_points=FLAGS.num_data_points,
        time_steps=FLAGS.num_items_in_seq,
        alpha=FLAGS.alpha)
    alpha_str = 'alpha{}'.format(FLAGS.alpha)

  logging.info('Dataset created. Saving the data...')
  data_path = os.path.join(FLAGS.output_file_path,
                           'synthetic_data_{}'.format(alpha_str))
  save_file(data_path, **dataset)
  logging.info('Dataset saved at %s.', data_path)


if __name__ == '__main__':
  flags.mark_flag_as_required('output_file_path')
  app.run(main)
