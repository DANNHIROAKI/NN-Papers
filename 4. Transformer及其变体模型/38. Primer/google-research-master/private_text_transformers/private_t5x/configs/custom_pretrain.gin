from __gin__ import dynamic_registration
import __main__ as train_script
from private_text_transformers.private_t5x import trainer
import t5.data.mixtures
from t5x import losses
from t5x import partitioning
from t5x import utils

include 't5x/configs/runs/pretrain.gin'

# To pretrain a T5 Small model, you need to set the following flags:
# IMPORTANT: First list the model gin file, and then the custom_pretrain.gin.
# If the order is incorrect, the wrong model is used.
# --gin_search_paths="third_party/py/t5/experimental/p5x/configs,research/privacy/language_models/t5/configs"
# --gin_file=models/t5_small.gin
# --gin_file=custom_pretrain.gin
# --gin.MIXTURE_OR_TASK_NAME="c4_v220_span_corruption"
# --gin.TASK_FEATURE_LENGTHS="{\"inputs\":512,\"targets\":114}"
# --gin.TRAIN_STEPS=100_000

# DP related
USE_DP = %gin.REQUIRED
DP_L2_CLIP_NORM = %gin.REQUIRED
DP_NOISE_MULTIPLIER = %gin.REQUIRED
# If not set, we will use per example grads.
DP_NUM_MICROBATCHES = None

RANDOM_SEED = 10

# Can use:
# - None (no loss normalization)
# - losses.SpecialLossNormalizingFactor.NUM_REAL_TARGET_TOKENS (the real number
#   of non-padding tokens in the batch.
LOSS_NORMALIZING_FACTOR = None

# Use the default partitioning rules but remove the `batch` rule since xmap
# already uses the `batch` dimension in PrivateTrainer.

partitioning.PjitPartitioner:
  logical_axis_rules = @trainer.standard_logical_axis_rules_without_batch()

train_script.train:
  trainer_cls = @trainer.PrivateTrainer
  random_seed = %RANDOM_SEED  # If not set, will use hardware rng.
  partitioner = @partitioning.PjitPartitioner()

# Note that the trainer.Trainer config from `pretrain.gin` is not used!
trainer.PrivateTrainer:
  num_microbatches = %DP_NUM_MICROBATCHES
  learning_rate_fn = @utils.create_learning_rate_scheduler()
  use_dp = %USE_DP
  dp_l2_clip_norm = %DP_L2_CLIP_NORM
  dp_noise_multiplier = %DP_NOISE_MULTIPLIER

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 1000
