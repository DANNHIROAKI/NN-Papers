# Vanilla Transformer from "Attention is all you need". The only difference
# is that we pre-LN transformer as opposed to the post-LN transformer that
# was proposed in the original paper.

include 'models/t5.1.0.base.gin'

import mesh_tensorflow.transformer.transformer
import mesh_tensorflow.transformer.transformer_layers

# Layer Norm instead of RMS norm.
transformer.LayerStack.sublayers_per_layer = [
    @transformer.sublayer_true_layer_norm,
    @transformer.sublayer_mask_padding,
    @transformer.sublayer_call_layer,
    @transformer.sublayer_dropout,
    @transformer.sublayer_residual]

transformer.LayerStack.sublayers_final = [
    @transformer.sublayer_true_layer_norm,
    @transformer.sublayer_dropout,
    @transformer.sublayer_mask_padding]

# Learned positional embeddings instead of relative embeddings
transformer_layers.SelfAttention.relative_attention_type = None
transformer.Unitransformer.positional_embedding = True

# Include biase in the feed forward layers
DenseReluDense.use_bias = True
