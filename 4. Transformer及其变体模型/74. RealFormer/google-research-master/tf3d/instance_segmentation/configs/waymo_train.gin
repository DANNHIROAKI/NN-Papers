num_classes = 5


# Preprocessor
waymo_prepare_lidar_images_and_correspondences.resized_image_height = 481
waymo_prepare_lidar_images_and_correspondences.resized_image_width = 721

prepare_waymo_open_dataset.valid_object_classes = [1, 2, 3, 4]
object_detection_preprocess.input_field_mapping_fn = @prepare_waymo_open_dataset
object_detection_preprocess.images_points_correspondence_fn = @waymo_prepare_lidar_images_and_correspondences
object_detection_preprocess.points_pad_or_clip_size = 250000
object_detection_preprocess.voxel_grid_cell_size = (0.2, 0.2, 0.2)
object_detection_preprocess.voxels_pad_or_clip_size = 80000
object_detection_preprocess.point_feature_keys = ('point_offset_bins',)
object_detection_preprocess.x_min_degree_rotation = None
object_detection_preprocess.x_max_degree_rotation = None
object_detection_preprocess.y_min_degree_rotation = None
object_detection_preprocess.y_max_degree_rotation = None
object_detection_preprocess.z_min_degree_rotation = -10.0
object_detection_preprocess.z_max_degree_rotation = 10.0
object_detection_preprocess.min_scale_ratio = 0.9
object_detection_preprocess.max_scale_ratio = 1.1
object_detection_preprocess.translation_range = 0.1
object_detection_preprocess.points_within_box_margin = 0.1
object_detection_preprocess.min_num_points_in_objects = 5


# Dataset
get_tf_data_dataset.dataset_name = 'waymo_object_per_frame'
get_tf_data_dataset.split_name = 'train'
get_tf_data_dataset.preprocess_fn = @object_detection_preprocess
get_tf_data_dataset.feature_keys = ['voxel_positions',
                                    'voxel_features',
                                    'voxel_xyz_indices',
                                    'points_to_voxel_mapping',
                                    'num_valid_voxels']
get_tf_data_dataset.label_keys = ['object_class_voxels',
                                  'object_instance_id_voxels']
get_tf_data_dataset.shuffle_buffer_size = 32
get_tf_data_dataset.filenames_shuffle_buffer_size = 30
get_tf_data_dataset.read_block_length = 2
get_tf_data_dataset.num_readers = 32


# 3D Network
SparseConvUNet.conv_filter_size = 3
SparseConvUNet.encoder_dimensions = ((64, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (256, 256))
SparseConvUNet.bottleneck_dimensions = (256, 256)
SparseConvUNet.decoder_dimensions = ((256, 256), (224, 224), (192, 192), (160, 160), (128, 128), (96, 96), (64, 64))
SparseConvUNet.use_batch_norm = True


# Losses
npair_loss.num_samples = 1000
npair_loss.max_instance_id = 300
npair_loss.similarity_strategy = 'distance'
npair_loss.loss_strategy = 'sigmoid'
npair_loss.is_intermediate = False

embedding_regularization_loss.lambda_coef = 0.00001
embedding_regularization_loss.regularization_type = 'unit_length'

classification_loss_using_mask_iou.num_samples = 1000
classification_loss_using_mask_iou.max_instance_id = 300
classification_loss_using_mask_iou.similarity_strategy = 'distance'
classification_loss_using_mask_iou.is_balanced = True
classification_loss_using_mask_iou.is_intermediate = False


# Model
InstanceSegmentationModel.num_classes = %num_classes
InstanceSegmentationModel.embedding_dims = 64
InstanceSegmentationModel.embedding_similarity_strategy = 'distance'
InstanceSegmentationModel.embedding_similarity_threshold = 0.5
InstanceSegmentationModel.loss_names_to_functions = {
    'npair_loss': @npair_loss,
    'embedding_regularization_loss': @embedding_regularization_loss,
    'classification_loss_using_mask_iou': @classification_loss_using_mask_iou,
}
InstanceSegmentationModel.loss_names_to_weights = {
    'npair_loss': 10.0,
    'embedding_regularization_loss': 10.0,
    'classification_loss_using_mask_iou': 1.0,
}


# Training
step_decay.initial_learning_rate = 0.1
step_decay.boundary_list = [15000, 18000, 19500, 21000, 22500]
step_decay.ratio_list = [1.0, 0.3, 0.1, 0.03, 0.01, 0.001]

train.learning_rate_fn = @step_decay
train.input_fn = @get_tf_data_dataset
train.optimizer_fn = @tf.keras.optimizers.SGD
train.model_class = @InstanceSegmentationModel

