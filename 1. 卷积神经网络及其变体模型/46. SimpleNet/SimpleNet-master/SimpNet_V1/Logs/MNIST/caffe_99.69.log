
N:\Caffe\examples\mnist>REM going to the caffe root 

N:\Caffe\examples\mnist>CD ../../ 

N:\Caffe>SET TOOLS=Build/x64/Release 

N:\Caffe>"Build/x64/Release/caffe.exe" train --solver=examples/mnist/solver.prototxt 
I1101 06:09:01.686408 11248 caffe.cpp:186] Using GPUs 0
I1101 06:09:01.911120 11248 caffe.cpp:191] GPU 0: GeForce GTX 980
I1101 06:09:02.219719 11248 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.219719 11248 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 600
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 600
snapshot_prefix: "examples/mnist/simpnet_nodrp"
solver_mode: GPU
device_id: 0
net: "examples/mnist/train_test.prototxt"
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 22000
stepvalue: 29600
stepvalue: 32000
stepvalue: 37000
type: "AdaDelta"
I1101 06:09:02.220721 11248 solver.cpp:91] Creating training net from net file: examples/mnist/simpnet_nodrp_train_test.prototxt
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1101 06:09:02.221721 11248 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1101 06:09:02.222723 11248 net.cpp:49] Initializing net from parameters: 
name: "SimpNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2048
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1101 06:09:02.223222 11248 layer_factory.hpp:77] Creating layer mnist
I1101 06:09:02.223222 11248 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.224723 11248 net.cpp:91] Creating Layer mnist
I1101 06:09:02.225224 11248 net.cpp:399] mnist -> data
I1101 06:09:02.225224 11248 net.cpp:399] mnist -> label
I1101 06:09:02.225724  9768 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.234652  9768 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_train_lmdb_norm2
I1101 06:09:02.265672 11248 data_layer.cpp:41] output data size: 100,1,28,28
I1101 06:09:02.270676 11248 net.cpp:141] Setting up mnist
I1101 06:09:02.270676 11248 net.cpp:148] Top shape: 100 1 28 28 (78400)
I1101 06:09:02.271176 11248 net.cpp:148] Top shape: 100 (100)
I1101 06:09:02.271176 11248 net.cpp:156] Memory required for data: 314000
I1101 06:09:02.271176 11248 layer_factory.hpp:77] Creating layer conv1
I1101 06:09:02.271176 11248 net.cpp:91] Creating Layer conv1
I1101 06:09:02.271176 11248 net.cpp:425] conv1 <- data
I1101 06:09:02.271176 11248 net.cpp:399] conv1 -> conv1
I1101 06:09:02.272677  7792 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.550837 11248 net.cpp:141] Setting up conv1
I1101 06:09:02.550837 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.550837 11248 net.cpp:156] Memory required for data: 20384400
I1101 06:09:02.550837 11248 layer_factory.hpp:77] Creating layer bn1
I1101 06:09:02.550837 11248 net.cpp:91] Creating Layer bn1
I1101 06:09:02.550837 11248 net.cpp:425] bn1 <- conv1
I1101 06:09:02.550837 11248 net.cpp:399] bn1 -> bn1
I1101 06:09:02.551337 11248 net.cpp:141] Setting up bn1
I1101 06:09:02.551337 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.551337 11248 net.cpp:156] Memory required for data: 40454800
I1101 06:09:02.551337 11248 layer_factory.hpp:77] Creating layer scale1
I1101 06:09:02.551337 11248 net.cpp:91] Creating Layer scale1
I1101 06:09:02.551337 11248 net.cpp:425] scale1 <- bn1
I1101 06:09:02.551337 11248 net.cpp:399] scale1 -> scale1
I1101 06:09:02.551337 11248 layer_factory.hpp:77] Creating layer scale1
I1101 06:09:02.551337 11248 net.cpp:141] Setting up scale1
I1101 06:09:02.551337 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.551337 11248 net.cpp:156] Memory required for data: 60525200
I1101 06:09:02.551337 11248 layer_factory.hpp:77] Creating layer relu1
I1101 06:09:02.551337 11248 net.cpp:91] Creating Layer relu1
I1101 06:09:02.551337 11248 net.cpp:425] relu1 <- scale1
I1101 06:09:02.551337 11248 net.cpp:399] relu1 -> relu1
I1101 06:09:02.551836 11248 net.cpp:141] Setting up relu1
I1101 06:09:02.551836 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.551836 11248 net.cpp:156] Memory required for data: 80595600
I1101 06:09:02.551836 11248 layer_factory.hpp:77] Creating layer conv1_0
I1101 06:09:02.551836 11248 net.cpp:91] Creating Layer conv1_0
I1101 06:09:02.552337 11248 net.cpp:425] conv1_0 <- relu1
I1101 06:09:02.552337 11248 net.cpp:399] conv1_0 -> conv1_0
I1101 06:09:02.554839 11248 net.cpp:141] Setting up conv1_0
I1101 06:09:02.554839 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.554839 11248 net.cpp:156] Memory required for data: 120736400
I1101 06:09:02.554839 11248 layer_factory.hpp:77] Creating layer bn1_0
I1101 06:09:02.554839 11248 net.cpp:91] Creating Layer bn1_0
I1101 06:09:02.554839 11248 net.cpp:425] bn1_0 <- conv1_0
I1101 06:09:02.554839 11248 net.cpp:399] bn1_0 -> bn1_0
I1101 06:09:02.554839 11248 net.cpp:141] Setting up bn1_0
I1101 06:09:02.554839 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.555340 11248 net.cpp:156] Memory required for data: 160877200
I1101 06:09:02.555340 11248 layer_factory.hpp:77] Creating layer scale1_0
I1101 06:09:02.555340 11248 net.cpp:91] Creating Layer scale1_0
I1101 06:09:02.555340 11248 net.cpp:425] scale1_0 <- bn1_0
I1101 06:09:02.555340 11248 net.cpp:399] scale1_0 -> scale1_0
I1101 06:09:02.555340 11248 layer_factory.hpp:77] Creating layer scale1_0
I1101 06:09:02.555340 11248 net.cpp:141] Setting up scale1_0
I1101 06:09:02.555340 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.555340 11248 net.cpp:156] Memory required for data: 201018000
I1101 06:09:02.555340 11248 layer_factory.hpp:77] Creating layer relu1_0
I1101 06:09:02.555340 11248 net.cpp:91] Creating Layer relu1_0
I1101 06:09:02.555340 11248 net.cpp:425] relu1_0 <- scale1_0
I1101 06:09:02.555340 11248 net.cpp:399] relu1_0 -> relu1_0
I1101 06:09:02.556340 11248 net.cpp:141] Setting up relu1_0
I1101 06:09:02.556340 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.556340 11248 net.cpp:156] Memory required for data: 241158800
I1101 06:09:02.556340 11248 layer_factory.hpp:77] Creating layer conv2
I1101 06:09:02.556340 11248 net.cpp:91] Creating Layer conv2
I1101 06:09:02.556340 11248 net.cpp:425] conv2 <- relu1_0
I1101 06:09:02.556340 11248 net.cpp:399] conv2 -> conv2
I1101 06:09:02.559842 11248 net.cpp:141] Setting up conv2
I1101 06:09:02.559842 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.559842 11248 net.cpp:156] Memory required for data: 281299600
I1101 06:09:02.559842 11248 layer_factory.hpp:77] Creating layer bn2
I1101 06:09:02.559842 11248 net.cpp:91] Creating Layer bn2
I1101 06:09:02.559842 11248 net.cpp:425] bn2 <- conv2
I1101 06:09:02.559842 11248 net.cpp:399] bn2 -> bn2
I1101 06:09:02.559842 11248 net.cpp:141] Setting up bn2
I1101 06:09:02.559842 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.559842 11248 net.cpp:156] Memory required for data: 321440400
I1101 06:09:02.559842 11248 layer_factory.hpp:77] Creating layer scale2
I1101 06:09:02.559842 11248 net.cpp:91] Creating Layer scale2
I1101 06:09:02.559842 11248 net.cpp:425] scale2 <- bn2
I1101 06:09:02.559842 11248 net.cpp:399] scale2 -> scale2
I1101 06:09:02.559842 11248 layer_factory.hpp:77] Creating layer scale2
I1101 06:09:02.560343 11248 net.cpp:141] Setting up scale2
I1101 06:09:02.560343 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.560343 11248 net.cpp:156] Memory required for data: 361581200
I1101 06:09:02.560343 11248 layer_factory.hpp:77] Creating layer relu2
I1101 06:09:02.560343 11248 net.cpp:91] Creating Layer relu2
I1101 06:09:02.560343 11248 net.cpp:425] relu2 <- scale2
I1101 06:09:02.560343 11248 net.cpp:399] relu2 -> relu2
I1101 06:09:02.560343 11248 net.cpp:141] Setting up relu2
I1101 06:09:02.560343 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.560343 11248 net.cpp:156] Memory required for data: 401722000
I1101 06:09:02.560343 11248 layer_factory.hpp:77] Creating layer conv2_1
I1101 06:09:02.560343 11248 net.cpp:91] Creating Layer conv2_1
I1101 06:09:02.560343 11248 net.cpp:425] conv2_1 <- relu2
I1101 06:09:02.560343 11248 net.cpp:399] conv2_1 -> conv2_1
I1101 06:09:02.564347 11248 net.cpp:141] Setting up conv2_1
I1101 06:09:02.564347 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.564347 11248 net.cpp:156] Memory required for data: 441862800
I1101 06:09:02.564347 11248 layer_factory.hpp:77] Creating layer bn2_1
I1101 06:09:02.564347 11248 net.cpp:91] Creating Layer bn2_1
I1101 06:09:02.564347 11248 net.cpp:425] bn2_1 <- conv2_1
I1101 06:09:02.564347 11248 net.cpp:399] bn2_1 -> bn2_1
I1101 06:09:02.564847 11248 net.cpp:141] Setting up bn2_1
I1101 06:09:02.564847 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.564847 11248 net.cpp:156] Memory required for data: 482003600
I1101 06:09:02.564847 11248 layer_factory.hpp:77] Creating layer scale2_1
I1101 06:09:02.564847 11248 net.cpp:91] Creating Layer scale2_1
I1101 06:09:02.564847 11248 net.cpp:425] scale2_1 <- bn2_1
I1101 06:09:02.564847 11248 net.cpp:399] scale2_1 -> scale2_1
I1101 06:09:02.564847 11248 layer_factory.hpp:77] Creating layer scale2_1
I1101 06:09:02.564847 11248 net.cpp:141] Setting up scale2_1
I1101 06:09:02.564847 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.564847 11248 net.cpp:156] Memory required for data: 522144400
I1101 06:09:02.564847 11248 layer_factory.hpp:77] Creating layer relu2_1
I1101 06:09:02.564847 11248 net.cpp:91] Creating Layer relu2_1
I1101 06:09:02.564847 11248 net.cpp:425] relu2_1 <- scale2_1
I1101 06:09:02.564847 11248 net.cpp:399] relu2_1 -> relu2_1
I1101 06:09:02.567348 11248 net.cpp:141] Setting up relu2_1
I1101 06:09:02.567348 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.567348 11248 net.cpp:156] Memory required for data: 562285200
I1101 06:09:02.567348 11248 layer_factory.hpp:77] Creating layer pool2_1
I1101 06:09:02.567348 11248 net.cpp:91] Creating Layer pool2_1
I1101 06:09:02.567348 11248 net.cpp:425] pool2_1 <- relu2_1
I1101 06:09:02.567348 11248 net.cpp:399] pool2_1 -> pool2_1
I1101 06:09:02.567348 11248 net.cpp:141] Setting up pool2_1
I1101 06:09:02.567348 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.567348 11248 net.cpp:156] Memory required for data: 572320400
I1101 06:09:02.567348 11248 layer_factory.hpp:77] Creating layer conv2_2
I1101 06:09:02.567348 11248 net.cpp:91] Creating Layer conv2_2
I1101 06:09:02.567348 11248 net.cpp:425] conv2_2 <- pool2_1
I1101 06:09:02.567348 11248 net.cpp:399] conv2_2 -> conv2_2
I1101 06:09:02.570850 11248 net.cpp:141] Setting up conv2_2
I1101 06:09:02.571352 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.571352 11248 net.cpp:156] Memory required for data: 582355600
I1101 06:09:02.571352 11248 layer_factory.hpp:77] Creating layer bn2_2
I1101 06:09:02.571352 11248 net.cpp:91] Creating Layer bn2_2
I1101 06:09:02.571352 11248 net.cpp:425] bn2_2 <- conv2_2
I1101 06:09:02.571352 11248 net.cpp:399] bn2_2 -> bn2_2
I1101 06:09:02.571352 11248 net.cpp:141] Setting up bn2_2
I1101 06:09:02.571352 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.571352 11248 net.cpp:156] Memory required for data: 592390800
I1101 06:09:02.571352 11248 layer_factory.hpp:77] Creating layer scale2_2
I1101 06:09:02.571352 11248 net.cpp:91] Creating Layer scale2_2
I1101 06:09:02.571352 11248 net.cpp:425] scale2_2 <- bn2_2
I1101 06:09:02.571352 11248 net.cpp:399] scale2_2 -> scale2_2
I1101 06:09:02.571352 11248 layer_factory.hpp:77] Creating layer scale2_2
I1101 06:09:02.571352 11248 net.cpp:141] Setting up scale2_2
I1101 06:09:02.571851 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.571851 11248 net.cpp:156] Memory required for data: 602426000
I1101 06:09:02.571851 11248 layer_factory.hpp:77] Creating layer relu2_2
I1101 06:09:02.571851 11248 net.cpp:91] Creating Layer relu2_2
I1101 06:09:02.571851 11248 net.cpp:425] relu2_2 <- scale2_2
I1101 06:09:02.571851 11248 net.cpp:399] relu2_2 -> relu2_2
I1101 06:09:02.572351 11248 net.cpp:141] Setting up relu2_2
I1101 06:09:02.572351 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.572351 11248 net.cpp:156] Memory required for data: 612461200
I1101 06:09:02.572351 11248 layer_factory.hpp:77] Creating layer conv3
I1101 06:09:02.572351 11248 net.cpp:91] Creating Layer conv3
I1101 06:09:02.572351 11248 net.cpp:425] conv3 <- relu2_2
I1101 06:09:02.572351 11248 net.cpp:399] conv3 -> conv3
I1101 06:09:02.575855 11248 net.cpp:141] Setting up conv3
I1101 06:09:02.575855 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.575855 11248 net.cpp:156] Memory required for data: 622496400
I1101 06:09:02.575855 11248 layer_factory.hpp:77] Creating layer bn3
I1101 06:09:02.575855 11248 net.cpp:91] Creating Layer bn3
I1101 06:09:02.575855 11248 net.cpp:425] bn3 <- conv3
I1101 06:09:02.575855 11248 net.cpp:399] bn3 -> bn3
I1101 06:09:02.576354 11248 net.cpp:141] Setting up bn3
I1101 06:09:02.576354 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.576354 11248 net.cpp:156] Memory required for data: 632531600
I1101 06:09:02.576354 11248 layer_factory.hpp:77] Creating layer scale3
I1101 06:09:02.576354 11248 net.cpp:91] Creating Layer scale3
I1101 06:09:02.576354 11248 net.cpp:425] scale3 <- bn3
I1101 06:09:02.576354 11248 net.cpp:399] scale3 -> scale3
I1101 06:09:02.576354 11248 layer_factory.hpp:77] Creating layer scale3
I1101 06:09:02.576354 11248 net.cpp:141] Setting up scale3
I1101 06:09:02.576354 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.576354 11248 net.cpp:156] Memory required for data: 642566800
I1101 06:09:02.576354 11248 layer_factory.hpp:77] Creating layer relu3
I1101 06:09:02.576354 11248 net.cpp:91] Creating Layer relu3
I1101 06:09:02.576354 11248 net.cpp:425] relu3 <- scale3
I1101 06:09:02.576354 11248 net.cpp:399] relu3 -> relu3
I1101 06:09:02.576855 11248 net.cpp:141] Setting up relu3
I1101 06:09:02.576855 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.576855 11248 net.cpp:156] Memory required for data: 652602000
I1101 06:09:02.576855 11248 layer_factory.hpp:77] Creating layer conv4
I1101 06:09:02.576855 11248 net.cpp:91] Creating Layer conv4
I1101 06:09:02.576855 11248 net.cpp:425] conv4 <- relu3
I1101 06:09:02.576855 11248 net.cpp:399] conv4 -> conv4
I1101 06:09:02.581858 11248 net.cpp:141] Setting up conv4
I1101 06:09:02.581858 11248 net.cpp:148] Top shape: 100 256 14 14 (5017600)
I1101 06:09:02.581858 11248 net.cpp:156] Memory required for data: 672672400
I1101 06:09:02.581858 11248 layer_factory.hpp:77] Creating layer pool4
I1101 06:09:02.581858 11248 net.cpp:91] Creating Layer pool4
I1101 06:09:02.581858 11248 net.cpp:425] pool4 <- conv4
I1101 06:09:02.581858 11248 net.cpp:399] pool4 -> pool4
I1101 06:09:02.582360 11248 net.cpp:141] Setting up pool4
I1101 06:09:02.582360 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.582360 11248 net.cpp:156] Memory required for data: 677690000
I1101 06:09:02.582360 11248 layer_factory.hpp:77] Creating layer bn4
I1101 06:09:02.582360 11248 net.cpp:91] Creating Layer bn4
I1101 06:09:02.582360 11248 net.cpp:425] bn4 <- pool4
I1101 06:09:02.582360 11248 net.cpp:399] bn4 -> bn4
I1101 06:09:02.582360 11248 net.cpp:141] Setting up bn4
I1101 06:09:02.582360 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.582360 11248 net.cpp:156] Memory required for data: 682707600
I1101 06:09:02.582360 11248 layer_factory.hpp:77] Creating layer scale4
I1101 06:09:02.582360 11248 net.cpp:91] Creating Layer scale4
I1101 06:09:02.582360 11248 net.cpp:425] scale4 <- bn4
I1101 06:09:02.582360 11248 net.cpp:399] scale4 -> scale4
I1101 06:09:02.582360 11248 layer_factory.hpp:77] Creating layer scale4
I1101 06:09:02.582859 11248 net.cpp:141] Setting up scale4
I1101 06:09:02.582859 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.582859 11248 net.cpp:156] Memory required for data: 687725200
I1101 06:09:02.582859 11248 layer_factory.hpp:77] Creating layer relu4
I1101 06:09:02.582859 11248 net.cpp:91] Creating Layer relu4
I1101 06:09:02.582859 11248 net.cpp:425] relu4 <- scale4
I1101 06:09:02.582859 11248 net.cpp:399] relu4 -> relu4
I1101 06:09:02.583359 11248 net.cpp:141] Setting up relu4
I1101 06:09:02.583359 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.583359 11248 net.cpp:156] Memory required for data: 692742800
I1101 06:09:02.583359 11248 layer_factory.hpp:77] Creating layer conv4_1
I1101 06:09:02.583860 11248 net.cpp:91] Creating Layer conv4_1
I1101 06:09:02.583860 11248 net.cpp:425] conv4_1 <- relu4
I1101 06:09:02.583860 11248 net.cpp:399] conv4_1 -> conv4_1
I1101 06:09:02.591365 11248 net.cpp:141] Setting up conv4_1
I1101 06:09:02.591365 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.591365 11248 net.cpp:156] Memory required for data: 697760400
I1101 06:09:02.591365 11248 layer_factory.hpp:77] Creating layer bn4_1
I1101 06:09:02.591365 11248 net.cpp:91] Creating Layer bn4_1
I1101 06:09:02.591365 11248 net.cpp:425] bn4_1 <- conv4_1
I1101 06:09:02.591365 11248 net.cpp:399] bn4_1 -> bn4_1
I1101 06:09:02.591866 11248 net.cpp:141] Setting up bn4_1
I1101 06:09:02.591866 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.591866 11248 net.cpp:156] Memory required for data: 702778000
I1101 06:09:02.591866 11248 layer_factory.hpp:77] Creating layer scale4_1
I1101 06:09:02.591866 11248 net.cpp:91] Creating Layer scale4_1
I1101 06:09:02.591866 11248 net.cpp:425] scale4_1 <- bn4_1
I1101 06:09:02.591866 11248 net.cpp:399] scale4_1 -> scale4_1
I1101 06:09:02.591866 11248 layer_factory.hpp:77] Creating layer scale4_1
I1101 06:09:02.591866 11248 net.cpp:141] Setting up scale4_1
I1101 06:09:02.591866 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.591866 11248 net.cpp:156] Memory required for data: 707795600
I1101 06:09:02.591866 11248 layer_factory.hpp:77] Creating layer relu4_1
I1101 06:09:02.591866 11248 net.cpp:91] Creating Layer relu4_1
I1101 06:09:02.591866 11248 net.cpp:425] relu4_1 <- scale4_1
I1101 06:09:02.591866 11248 net.cpp:399] relu4_1 -> relu4_1
I1101 06:09:02.592365 11248 net.cpp:141] Setting up relu4_1
I1101 06:09:02.592365 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.592365 11248 net.cpp:156] Memory required for data: 712813200
I1101 06:09:02.592365 11248 layer_factory.hpp:77] Creating layer conv4_2
I1101 06:09:02.592365 11248 net.cpp:91] Creating Layer conv4_2
I1101 06:09:02.592365 11248 net.cpp:425] conv4_2 <- relu4_1
I1101 06:09:02.592365 11248 net.cpp:399] conv4_2 -> conv4_2
I1101 06:09:02.599871 11248 net.cpp:141] Setting up conv4_2
I1101 06:09:02.599871 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.599871 11248 net.cpp:156] Memory required for data: 717830800
I1101 06:09:02.599871 11248 layer_factory.hpp:77] Creating layer bn4_2
I1101 06:09:02.599871 11248 net.cpp:91] Creating Layer bn4_2
I1101 06:09:02.599871 11248 net.cpp:425] bn4_2 <- conv4_2
I1101 06:09:02.599871 11248 net.cpp:399] bn4_2 -> bn4_2
I1101 06:09:02.600373 11248 net.cpp:141] Setting up bn4_2
I1101 06:09:02.600373 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.600373 11248 net.cpp:156] Memory required for data: 722848400
I1101 06:09:02.600373 11248 layer_factory.hpp:77] Creating layer scale4_2
I1101 06:09:02.600373 11248 net.cpp:91] Creating Layer scale4_2
I1101 06:09:02.600373 11248 net.cpp:425] scale4_2 <- bn4_2
I1101 06:09:02.600373 11248 net.cpp:399] scale4_2 -> scale4_2
I1101 06:09:02.600373 11248 layer_factory.hpp:77] Creating layer scale4_2
I1101 06:09:02.600373 11248 net.cpp:141] Setting up scale4_2
I1101 06:09:02.600373 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.600373 11248 net.cpp:156] Memory required for data: 727866000
I1101 06:09:02.600373 11248 layer_factory.hpp:77] Creating layer relu4_2
I1101 06:09:02.600373 11248 net.cpp:91] Creating Layer relu4_2
I1101 06:09:02.600373 11248 net.cpp:425] relu4_2 <- scale4_2
I1101 06:09:02.600373 11248 net.cpp:399] relu4_2 -> relu4_2
I1101 06:09:02.600872 11248 net.cpp:141] Setting up relu4_2
I1101 06:09:02.600872 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.600872 11248 net.cpp:156] Memory required for data: 732883600
I1101 06:09:02.600872 11248 layer_factory.hpp:77] Creating layer pool4_2
I1101 06:09:02.600872 11248 net.cpp:91] Creating Layer pool4_2
I1101 06:09:02.600872 11248 net.cpp:425] pool4_2 <- relu4_2
I1101 06:09:02.600872 11248 net.cpp:399] pool4_2 -> pool4_2
I1101 06:09:02.600872 11248 net.cpp:141] Setting up pool4_2
I1101 06:09:02.600872 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.600872 11248 net.cpp:156] Memory required for data: 734522000
I1101 06:09:02.600872 11248 layer_factory.hpp:77] Creating layer conv4_0
I1101 06:09:02.600872 11248 net.cpp:91] Creating Layer conv4_0
I1101 06:09:02.600872 11248 net.cpp:425] conv4_0 <- pool4_2
I1101 06:09:02.600872 11248 net.cpp:399] conv4_0 -> conv4_0
I1101 06:09:02.616384 11248 net.cpp:141] Setting up conv4_0
I1101 06:09:02.616384 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.616384 11248 net.cpp:156] Memory required for data: 737798800
I1101 06:09:02.616384 11248 layer_factory.hpp:77] Creating layer bn4_0
I1101 06:09:02.616384 11248 net.cpp:91] Creating Layer bn4_0
I1101 06:09:02.616384 11248 net.cpp:425] bn4_0 <- conv4_0
I1101 06:09:02.616384 11248 net.cpp:399] bn4_0 -> bn4_0
I1101 06:09:02.616883 11248 net.cpp:141] Setting up bn4_0
I1101 06:09:02.616883 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.616883 11248 net.cpp:156] Memory required for data: 741075600
I1101 06:09:02.616883 11248 layer_factory.hpp:77] Creating layer scale4_0
I1101 06:09:02.616883 11248 net.cpp:91] Creating Layer scale4_0
I1101 06:09:02.616883 11248 net.cpp:425] scale4_0 <- bn4_0
I1101 06:09:02.616883 11248 net.cpp:399] scale4_0 -> scale4_0
I1101 06:09:02.616883 11248 layer_factory.hpp:77] Creating layer scale4_0
I1101 06:09:02.616883 11248 net.cpp:141] Setting up scale4_0
I1101 06:09:02.616883 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.616883 11248 net.cpp:156] Memory required for data: 744352400
I1101 06:09:02.616883 11248 layer_factory.hpp:77] Creating layer relu4_0
I1101 06:09:02.616883 11248 net.cpp:91] Creating Layer relu4_0
I1101 06:09:02.616883 11248 net.cpp:425] relu4_0 <- scale4_0
I1101 06:09:02.616883 11248 net.cpp:399] relu4_0 -> relu4_0
I1101 06:09:02.617883 11248 net.cpp:141] Setting up relu4_0
I1101 06:09:02.617883 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.617883 11248 net.cpp:156] Memory required for data: 747629200
I1101 06:09:02.617883 11248 layer_factory.hpp:77] Creating layer cccp4
I1101 06:09:02.617883 11248 net.cpp:91] Creating Layer cccp4
I1101 06:09:02.617883 11248 net.cpp:425] cccp4 <- relu4_0
I1101 06:09:02.617883 11248 net.cpp:399] cccp4 -> cccp4
I1101 06:09:02.628891 11248 net.cpp:141] Setting up cccp4
I1101 06:09:02.628891 11248 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 06:09:02.628891 11248 net.cpp:156] Memory required for data: 760736400
I1101 06:09:02.628891 11248 layer_factory.hpp:77] Creating layer relu_cccp4
I1101 06:09:02.628891 11248 net.cpp:91] Creating Layer relu_cccp4
I1101 06:09:02.628891 11248 net.cpp:425] relu_cccp4 <- cccp4
I1101 06:09:02.628891 11248 net.cpp:386] relu_cccp4 -> cccp4 (in-place)
I1101 06:09:02.629392 11248 net.cpp:141] Setting up relu_cccp4
I1101 06:09:02.629392 11248 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 06:09:02.629392 11248 net.cpp:156] Memory required for data: 773843600
I1101 06:09:02.629392 11248 layer_factory.hpp:77] Creating layer cccp5
I1101 06:09:02.629392 11248 net.cpp:91] Creating Layer cccp5
I1101 06:09:02.629392 11248 net.cpp:425] cccp5 <- cccp4
I1101 06:09:02.629392 11248 net.cpp:399] cccp5 -> cccp5
I1101 06:09:02.636665 11248 net.cpp:141] Setting up cccp5
I1101 06:09:02.636665 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.636665 11248 net.cpp:156] Memory required for data: 775482000
I1101 06:09:02.636665 11248 layer_factory.hpp:77] Creating layer relu_cccp5
I1101 06:09:02.636665 11248 net.cpp:91] Creating Layer relu_cccp5
I1101 06:09:02.636665 11248 net.cpp:425] relu_cccp5 <- cccp5
I1101 06:09:02.636665 11248 net.cpp:386] relu_cccp5 -> cccp5 (in-place)
I1101 06:09:02.637166 11248 net.cpp:141] Setting up relu_cccp5
I1101 06:09:02.637166 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.637166 11248 net.cpp:156] Memory required for data: 777120400
I1101 06:09:02.637166 11248 layer_factory.hpp:77] Creating layer poolcp5
I1101 06:09:02.637166 11248 net.cpp:91] Creating Layer poolcp5
I1101 06:09:02.637166 11248 net.cpp:425] poolcp5 <- cccp5
I1101 06:09:02.637166 11248 net.cpp:399] poolcp5 -> poolcp5
I1101 06:09:02.637166 11248 net.cpp:141] Setting up poolcp5
I1101 06:09:02.637166 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.637166 11248 net.cpp:156] Memory required for data: 777530000
I1101 06:09:02.637166 11248 layer_factory.hpp:77] Creating layer cccp6
I1101 06:09:02.637166 11248 net.cpp:91] Creating Layer cccp6
I1101 06:09:02.637166 11248 net.cpp:425] cccp6 <- poolcp5
I1101 06:09:02.637166 11248 net.cpp:399] cccp6 -> cccp6
I1101 06:09:02.696208 11248 net.cpp:141] Setting up cccp6
I1101 06:09:02.696208 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.698210 11248 net.cpp:156] Memory required for data: 777939600
I1101 06:09:02.698210 11248 layer_factory.hpp:77] Creating layer relu_cccp6
I1101 06:09:02.698710 11248 net.cpp:91] Creating Layer relu_cccp6
I1101 06:09:02.698710 11248 net.cpp:425] relu_cccp6 <- cccp6
I1101 06:09:02.698710 11248 net.cpp:386] relu_cccp6 -> cccp6 (in-place)
I1101 06:09:02.699210 11248 net.cpp:141] Setting up relu_cccp6
I1101 06:09:02.699210 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.699210 11248 net.cpp:156] Memory required for data: 778349200
I1101 06:09:02.699210 11248 layer_factory.hpp:77] Creating layer poolcp6
I1101 06:09:02.699210 11248 net.cpp:91] Creating Layer poolcp6
I1101 06:09:02.699210 11248 net.cpp:425] poolcp6 <- cccp6
I1101 06:09:02.699210 11248 net.cpp:399] poolcp6 -> poolcp6
I1101 06:09:02.699210 11248 net.cpp:141] Setting up poolcp6
I1101 06:09:02.699210 11248 net.cpp:148] Top shape: 100 256 1 1 (25600)
I1101 06:09:02.699210 11248 net.cpp:156] Memory required for data: 778451600
I1101 06:09:02.699210 11248 layer_factory.hpp:77] Creating layer ip1
I1101 06:09:02.699210 11248 net.cpp:91] Creating Layer ip1
I1101 06:09:02.699210 11248 net.cpp:425] ip1 <- poolcp6
I1101 06:09:02.699710 11248 net.cpp:399] ip1 -> ip1
I1101 06:09:02.699710 11248 net.cpp:141] Setting up ip1
I1101 06:09:02.699710 11248 net.cpp:148] Top shape: 100 10 (1000)
I1101 06:09:02.699710 11248 net.cpp:156] Memory required for data: 778455600
I1101 06:09:02.699710 11248 layer_factory.hpp:77] Creating layer loss
I1101 06:09:02.699710 11248 net.cpp:91] Creating Layer loss
I1101 06:09:02.699710 11248 net.cpp:425] loss <- ip1
I1101 06:09:02.699710 11248 net.cpp:425] loss <- label
I1101 06:09:02.699710 11248 net.cpp:399] loss -> loss
I1101 06:09:02.699710 11248 layer_factory.hpp:77] Creating layer loss
I1101 06:09:02.700211 11248 net.cpp:141] Setting up loss
I1101 06:09:02.700211 11248 net.cpp:148] Top shape: (1)
I1101 06:09:02.700211 11248 net.cpp:151]     with loss weight 1
I1101 06:09:02.700211 11248 net.cpp:156] Memory required for data: 778455604
I1101 06:09:02.700211 11248 net.cpp:217] loss needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] ip1 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] poolcp6 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] relu_cccp6 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] cccp6 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] poolcp5 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] relu_cccp5 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] cccp5 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] relu_cccp4 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] cccp4 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] relu4_0 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] scale4_0 needs backward computation.
I1101 06:09:02.700211 11248 net.cpp:217] bn4_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv4_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] pool4_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu4_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale4_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn4_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv4_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu4_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale4_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn4_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv4_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu4 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale4 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn4 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] pool4 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv4 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu3 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale3 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn3 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv3 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu2_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale2_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn2_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv2_2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] pool2_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu2_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale2_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn2_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv2_1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv2 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu1_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale1_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn1_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv1_0 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] relu1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] scale1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] bn1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:217] conv1 needs backward computation.
I1101 06:09:02.700711 11248 net.cpp:219] mnist does not need backward computation.
I1101 06:09:02.700711 11248 net.cpp:261] This network produces output loss
I1101 06:09:02.700711 11248 net.cpp:274] Network initialization done.
I1101 06:09:02.701711 11248 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/simpnet_nodrp_train_test.prototxt
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1101 06:09:02.702213 11248 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1101 06:09:02.702713 11248 net.cpp:49] Initializing net from parameters: 
name: "SimpNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2048
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1101 06:09:02.703212 11248 layer_factory.hpp:77] Creating layer mnist
I1101 06:09:02.703713 11248 net.cpp:91] Creating Layer mnist
I1101 06:09:02.703713 11248 net.cpp:399] mnist -> data
I1101 06:09:02.703713 11248 net.cpp:399] mnist -> label
I1101 06:09:02.704713  8656 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.768611  8656 db_lmdb.cpp:52] Opened lmdb examples/mnist/mnist_test_lmdb_norm2
I1101 06:09:02.769111 11248 data_layer.cpp:41] output data size: 100,1,28,28
I1101 06:09:02.770612 11248 net.cpp:141] Setting up mnist
I1101 06:09:02.770612 11248 net.cpp:148] Top shape: 100 1 28 28 (78400)
I1101 06:09:02.770612 11248 net.cpp:148] Top shape: 100 (100)
I1101 06:09:02.770612 11248 net.cpp:156] Memory required for data: 314000
I1101 06:09:02.770612 11248 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1101 06:09:02.770612 11248 net.cpp:91] Creating Layer label_mnist_1_split
I1101 06:09:02.770612 11248 net.cpp:425] label_mnist_1_split <- label
I1101 06:09:02.770612 11248 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I1101 06:09:02.770612 11248 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I1101 06:09:02.770612 11248 net.cpp:141] Setting up label_mnist_1_split
I1101 06:09:02.771113 11248 net.cpp:148] Top shape: 100 (100)
I1101 06:09:02.771113 11248 net.cpp:148] Top shape: 100 (100)
I1101 06:09:02.771113 11248 net.cpp:156] Memory required for data: 314800
I1101 06:09:02.771113 11248 layer_factory.hpp:77] Creating layer conv1
I1101 06:09:02.771113 11248 net.cpp:91] Creating Layer conv1
I1101 06:09:02.771113 11248 net.cpp:425] conv1 <- data
I1101 06:09:02.771113 11248 net.cpp:399] conv1 -> conv1
I1101 06:09:02.773114  8444 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1101 06:09:02.777145 11248 net.cpp:141] Setting up conv1
I1101 06:09:02.777145 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.777145 11248 net.cpp:156] Memory required for data: 20385200
I1101 06:09:02.777145 11248 layer_factory.hpp:77] Creating layer bn1
I1101 06:09:02.777647 11248 net.cpp:91] Creating Layer bn1
I1101 06:09:02.777647 11248 net.cpp:425] bn1 <- conv1
I1101 06:09:02.777647 11248 net.cpp:399] bn1 -> bn1
I1101 06:09:02.777647 11248 net.cpp:141] Setting up bn1
I1101 06:09:02.777647 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.777647 11248 net.cpp:156] Memory required for data: 40455600
I1101 06:09:02.777647 11248 layer_factory.hpp:77] Creating layer scale1
I1101 06:09:02.777647 11248 net.cpp:91] Creating Layer scale1
I1101 06:09:02.777647 11248 net.cpp:425] scale1 <- bn1
I1101 06:09:02.777647 11248 net.cpp:399] scale1 -> scale1
I1101 06:09:02.777647 11248 layer_factory.hpp:77] Creating layer scale1
I1101 06:09:02.777647 11248 net.cpp:141] Setting up scale1
I1101 06:09:02.778147 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.778147 11248 net.cpp:156] Memory required for data: 60526000
I1101 06:09:02.778147 11248 layer_factory.hpp:77] Creating layer relu1
I1101 06:09:02.778147 11248 net.cpp:91] Creating Layer relu1
I1101 06:09:02.778147 11248 net.cpp:425] relu1 <- scale1
I1101 06:09:02.778147 11248 net.cpp:399] relu1 -> relu1
I1101 06:09:02.778147 11248 net.cpp:141] Setting up relu1
I1101 06:09:02.778147 11248 net.cpp:148] Top shape: 100 64 28 28 (5017600)
I1101 06:09:02.778147 11248 net.cpp:156] Memory required for data: 80596400
I1101 06:09:02.778147 11248 layer_factory.hpp:77] Creating layer conv1_0
I1101 06:09:02.778147 11248 net.cpp:91] Creating Layer conv1_0
I1101 06:09:02.778147 11248 net.cpp:425] conv1_0 <- relu1
I1101 06:09:02.778147 11248 net.cpp:399] conv1_0 -> conv1_0
I1101 06:09:02.780796 11248 net.cpp:141] Setting up conv1_0
I1101 06:09:02.780796 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.780796 11248 net.cpp:156] Memory required for data: 120737200
I1101 06:09:02.780796 11248 layer_factory.hpp:77] Creating layer bn1_0
I1101 06:09:02.780796 11248 net.cpp:91] Creating Layer bn1_0
I1101 06:09:02.780796 11248 net.cpp:425] bn1_0 <- conv1_0
I1101 06:09:02.780796 11248 net.cpp:399] bn1_0 -> bn1_0
I1101 06:09:02.781296 11248 net.cpp:141] Setting up bn1_0
I1101 06:09:02.781296 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.781296 11248 net.cpp:156] Memory required for data: 160878000
I1101 06:09:02.781296 11248 layer_factory.hpp:77] Creating layer scale1_0
I1101 06:09:02.781296 11248 net.cpp:91] Creating Layer scale1_0
I1101 06:09:02.781296 11248 net.cpp:425] scale1_0 <- bn1_0
I1101 06:09:02.781296 11248 net.cpp:399] scale1_0 -> scale1_0
I1101 06:09:02.781296 11248 layer_factory.hpp:77] Creating layer scale1_0
I1101 06:09:02.781296 11248 net.cpp:141] Setting up scale1_0
I1101 06:09:02.781296 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.781296 11248 net.cpp:156] Memory required for data: 201018800
I1101 06:09:02.781296 11248 layer_factory.hpp:77] Creating layer relu1_0
I1101 06:09:02.781296 11248 net.cpp:91] Creating Layer relu1_0
I1101 06:09:02.781296 11248 net.cpp:425] relu1_0 <- scale1_0
I1101 06:09:02.781296 11248 net.cpp:399] relu1_0 -> relu1_0
I1101 06:09:02.782296 11248 net.cpp:141] Setting up relu1_0
I1101 06:09:02.782296 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.782296 11248 net.cpp:156] Memory required for data: 241159600
I1101 06:09:02.782296 11248 layer_factory.hpp:77] Creating layer conv2
I1101 06:09:02.782296 11248 net.cpp:91] Creating Layer conv2
I1101 06:09:02.782296 11248 net.cpp:425] conv2 <- relu1_0
I1101 06:09:02.782296 11248 net.cpp:399] conv2 -> conv2
I1101 06:09:02.785799 11248 net.cpp:141] Setting up conv2
I1101 06:09:02.785799 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.785799 11248 net.cpp:156] Memory required for data: 281300400
I1101 06:09:02.785799 11248 layer_factory.hpp:77] Creating layer bn2
I1101 06:09:02.785799 11248 net.cpp:91] Creating Layer bn2
I1101 06:09:02.785799 11248 net.cpp:425] bn2 <- conv2
I1101 06:09:02.785799 11248 net.cpp:399] bn2 -> bn2
I1101 06:09:02.785799 11248 net.cpp:141] Setting up bn2
I1101 06:09:02.785799 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.785799 11248 net.cpp:156] Memory required for data: 321441200
I1101 06:09:02.785799 11248 layer_factory.hpp:77] Creating layer scale2
I1101 06:09:02.785799 11248 net.cpp:91] Creating Layer scale2
I1101 06:09:02.785799 11248 net.cpp:425] scale2 <- bn2
I1101 06:09:02.785799 11248 net.cpp:399] scale2 -> scale2
I1101 06:09:02.785799 11248 layer_factory.hpp:77] Creating layer scale2
I1101 06:09:02.786299 11248 net.cpp:141] Setting up scale2
I1101 06:09:02.786299 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.786299 11248 net.cpp:156] Memory required for data: 361582000
I1101 06:09:02.786299 11248 layer_factory.hpp:77] Creating layer relu2
I1101 06:09:02.786299 11248 net.cpp:91] Creating Layer relu2
I1101 06:09:02.786299 11248 net.cpp:425] relu2 <- scale2
I1101 06:09:02.786299 11248 net.cpp:399] relu2 -> relu2
I1101 06:09:02.786299 11248 net.cpp:141] Setting up relu2
I1101 06:09:02.786299 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.786299 11248 net.cpp:156] Memory required for data: 401722800
I1101 06:09:02.786299 11248 layer_factory.hpp:77] Creating layer conv2_1
I1101 06:09:02.786299 11248 net.cpp:91] Creating Layer conv2_1
I1101 06:09:02.786299 11248 net.cpp:425] conv2_1 <- relu2
I1101 06:09:02.786299 11248 net.cpp:399] conv2_1 -> conv2_1
I1101 06:09:02.790530 11248 net.cpp:141] Setting up conv2_1
I1101 06:09:02.790530 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.790530 11248 net.cpp:156] Memory required for data: 441863600
I1101 06:09:02.790530 11248 layer_factory.hpp:77] Creating layer bn2_1
I1101 06:09:02.790530 11248 net.cpp:91] Creating Layer bn2_1
I1101 06:09:02.790530 11248 net.cpp:425] bn2_1 <- conv2_1
I1101 06:09:02.790530 11248 net.cpp:399] bn2_1 -> bn2_1
I1101 06:09:02.790530 11248 net.cpp:141] Setting up bn2_1
I1101 06:09:02.790530 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.790530 11248 net.cpp:156] Memory required for data: 482004400
I1101 06:09:02.790530 11248 layer_factory.hpp:77] Creating layer scale2_1
I1101 06:09:02.790530 11248 net.cpp:91] Creating Layer scale2_1
I1101 06:09:02.790530 11248 net.cpp:425] scale2_1 <- bn2_1
I1101 06:09:02.790530 11248 net.cpp:399] scale2_1 -> scale2_1
I1101 06:09:02.790530 11248 layer_factory.hpp:77] Creating layer scale2_1
I1101 06:09:02.791030 11248 net.cpp:141] Setting up scale2_1
I1101 06:09:02.791030 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.791030 11248 net.cpp:156] Memory required for data: 522145200
I1101 06:09:02.791030 11248 layer_factory.hpp:77] Creating layer relu2_1
I1101 06:09:02.791030 11248 net.cpp:91] Creating Layer relu2_1
I1101 06:09:02.791030 11248 net.cpp:425] relu2_1 <- scale2_1
I1101 06:09:02.791030 11248 net.cpp:399] relu2_1 -> relu2_1
I1101 06:09:02.791030 11248 net.cpp:141] Setting up relu2_1
I1101 06:09:02.791030 11248 net.cpp:148] Top shape: 100 128 28 28 (10035200)
I1101 06:09:02.791030 11248 net.cpp:156] Memory required for data: 562286000
I1101 06:09:02.791030 11248 layer_factory.hpp:77] Creating layer pool2_1
I1101 06:09:02.791530 11248 net.cpp:91] Creating Layer pool2_1
I1101 06:09:02.791530 11248 net.cpp:425] pool2_1 <- relu2_1
I1101 06:09:02.791530 11248 net.cpp:399] pool2_1 -> pool2_1
I1101 06:09:02.791530 11248 net.cpp:141] Setting up pool2_1
I1101 06:09:02.791530 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.791530 11248 net.cpp:156] Memory required for data: 572321200
I1101 06:09:02.791530 11248 layer_factory.hpp:77] Creating layer conv2_2
I1101 06:09:02.791530 11248 net.cpp:91] Creating Layer conv2_2
I1101 06:09:02.791530 11248 net.cpp:425] conv2_2 <- pool2_1
I1101 06:09:02.791530 11248 net.cpp:399] conv2_2 -> conv2_2
I1101 06:09:02.795835 11248 net.cpp:141] Setting up conv2_2
I1101 06:09:02.795835 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.795835 11248 net.cpp:156] Memory required for data: 582356400
I1101 06:09:02.795835 11248 layer_factory.hpp:77] Creating layer bn2_2
I1101 06:09:02.795835 11248 net.cpp:91] Creating Layer bn2_2
I1101 06:09:02.795835 11248 net.cpp:425] bn2_2 <- conv2_2
I1101 06:09:02.795835 11248 net.cpp:399] bn2_2 -> bn2_2
I1101 06:09:02.795835 11248 net.cpp:141] Setting up bn2_2
I1101 06:09:02.795835 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.795835 11248 net.cpp:156] Memory required for data: 592391600
I1101 06:09:02.795835 11248 layer_factory.hpp:77] Creating layer scale2_2
I1101 06:09:02.795835 11248 net.cpp:91] Creating Layer scale2_2
I1101 06:09:02.795835 11248 net.cpp:425] scale2_2 <- bn2_2
I1101 06:09:02.795835 11248 net.cpp:399] scale2_2 -> scale2_2
I1101 06:09:02.795835 11248 layer_factory.hpp:77] Creating layer scale2_2
I1101 06:09:02.796336 11248 net.cpp:141] Setting up scale2_2
I1101 06:09:02.796336 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.796336 11248 net.cpp:156] Memory required for data: 602426800
I1101 06:09:02.796336 11248 layer_factory.hpp:77] Creating layer relu2_2
I1101 06:09:02.796336 11248 net.cpp:91] Creating Layer relu2_2
I1101 06:09:02.796336 11248 net.cpp:425] relu2_2 <- scale2_2
I1101 06:09:02.796336 11248 net.cpp:399] relu2_2 -> relu2_2
I1101 06:09:02.796836 11248 net.cpp:141] Setting up relu2_2
I1101 06:09:02.796836 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.796836 11248 net.cpp:156] Memory required for data: 612462000
I1101 06:09:02.796836 11248 layer_factory.hpp:77] Creating layer conv3
I1101 06:09:02.796836 11248 net.cpp:91] Creating Layer conv3
I1101 06:09:02.796836 11248 net.cpp:425] conv3 <- relu2_2
I1101 06:09:02.796836 11248 net.cpp:399] conv3 -> conv3
I1101 06:09:02.800473 11248 net.cpp:141] Setting up conv3
I1101 06:09:02.800473 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.800473 11248 net.cpp:156] Memory required for data: 622497200
I1101 06:09:02.800473 11248 layer_factory.hpp:77] Creating layer bn3
I1101 06:09:02.800473 11248 net.cpp:91] Creating Layer bn3
I1101 06:09:02.800473 11248 net.cpp:425] bn3 <- conv3
I1101 06:09:02.800473 11248 net.cpp:399] bn3 -> bn3
I1101 06:09:02.800973 11248 net.cpp:141] Setting up bn3
I1101 06:09:02.800973 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.800973 11248 net.cpp:156] Memory required for data: 632532400
I1101 06:09:02.800973 11248 layer_factory.hpp:77] Creating layer scale3
I1101 06:09:02.800973 11248 net.cpp:91] Creating Layer scale3
I1101 06:09:02.800973 11248 net.cpp:425] scale3 <- bn3
I1101 06:09:02.800973 11248 net.cpp:399] scale3 -> scale3
I1101 06:09:02.800973 11248 layer_factory.hpp:77] Creating layer scale3
I1101 06:09:02.800973 11248 net.cpp:141] Setting up scale3
I1101 06:09:02.800973 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.800973 11248 net.cpp:156] Memory required for data: 642567600
I1101 06:09:02.800973 11248 layer_factory.hpp:77] Creating layer relu3
I1101 06:09:02.800973 11248 net.cpp:91] Creating Layer relu3
I1101 06:09:02.800973 11248 net.cpp:425] relu3 <- scale3
I1101 06:09:02.800973 11248 net.cpp:399] relu3 -> relu3
I1101 06:09:02.801475 11248 net.cpp:141] Setting up relu3
I1101 06:09:02.801475 11248 net.cpp:148] Top shape: 100 128 14 14 (2508800)
I1101 06:09:02.801475 11248 net.cpp:156] Memory required for data: 652602800
I1101 06:09:02.801475 11248 layer_factory.hpp:77] Creating layer conv4
I1101 06:09:02.801475 11248 net.cpp:91] Creating Layer conv4
I1101 06:09:02.801475 11248 net.cpp:425] conv4 <- relu3
I1101 06:09:02.801475 11248 net.cpp:399] conv4 -> conv4
I1101 06:09:02.806766 11248 net.cpp:141] Setting up conv4
I1101 06:09:02.806766 11248 net.cpp:148] Top shape: 100 256 14 14 (5017600)
I1101 06:09:02.806766 11248 net.cpp:156] Memory required for data: 672673200
I1101 06:09:02.806766 11248 layer_factory.hpp:77] Creating layer pool4
I1101 06:09:02.806766 11248 net.cpp:91] Creating Layer pool4
I1101 06:09:02.806766 11248 net.cpp:425] pool4 <- conv4
I1101 06:09:02.806766 11248 net.cpp:399] pool4 -> pool4
I1101 06:09:02.806766 11248 net.cpp:141] Setting up pool4
I1101 06:09:02.806766 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.806766 11248 net.cpp:156] Memory required for data: 677690800
I1101 06:09:02.806766 11248 layer_factory.hpp:77] Creating layer bn4
I1101 06:09:02.806766 11248 net.cpp:91] Creating Layer bn4
I1101 06:09:02.806766 11248 net.cpp:425] bn4 <- pool4
I1101 06:09:02.807267 11248 net.cpp:399] bn4 -> bn4
I1101 06:09:02.807267 11248 net.cpp:141] Setting up bn4
I1101 06:09:02.807267 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.807267 11248 net.cpp:156] Memory required for data: 682708400
I1101 06:09:02.807267 11248 layer_factory.hpp:77] Creating layer scale4
I1101 06:09:02.807267 11248 net.cpp:91] Creating Layer scale4
I1101 06:09:02.807267 11248 net.cpp:425] scale4 <- bn4
I1101 06:09:02.807267 11248 net.cpp:399] scale4 -> scale4
I1101 06:09:02.807267 11248 layer_factory.hpp:77] Creating layer scale4
I1101 06:09:02.807267 11248 net.cpp:141] Setting up scale4
I1101 06:09:02.807766 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.807766 11248 net.cpp:156] Memory required for data: 687726000
I1101 06:09:02.807766 11248 layer_factory.hpp:77] Creating layer relu4
I1101 06:09:02.807766 11248 net.cpp:91] Creating Layer relu4
I1101 06:09:02.807766 11248 net.cpp:425] relu4 <- scale4
I1101 06:09:02.807766 11248 net.cpp:399] relu4 -> relu4
I1101 06:09:02.807766 11248 net.cpp:141] Setting up relu4
I1101 06:09:02.807766 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.807766 11248 net.cpp:156] Memory required for data: 692743600
I1101 06:09:02.807766 11248 layer_factory.hpp:77] Creating layer conv4_1
I1101 06:09:02.807766 11248 net.cpp:91] Creating Layer conv4_1
I1101 06:09:02.807766 11248 net.cpp:425] conv4_1 <- relu4
I1101 06:09:02.807766 11248 net.cpp:399] conv4_1 -> conv4_1
I1101 06:09:02.814771 11248 net.cpp:141] Setting up conv4_1
I1101 06:09:02.814771 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.814771 11248 net.cpp:156] Memory required for data: 697761200
I1101 06:09:02.814771 11248 layer_factory.hpp:77] Creating layer bn4_1
I1101 06:09:02.814771 11248 net.cpp:91] Creating Layer bn4_1
I1101 06:09:02.814771 11248 net.cpp:425] bn4_1 <- conv4_1
I1101 06:09:02.814771 11248 net.cpp:399] bn4_1 -> bn4_1
I1101 06:09:02.815271 11248 net.cpp:141] Setting up bn4_1
I1101 06:09:02.815271 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.815271 11248 net.cpp:156] Memory required for data: 702778800
I1101 06:09:02.815271 11248 layer_factory.hpp:77] Creating layer scale4_1
I1101 06:09:02.815271 11248 net.cpp:91] Creating Layer scale4_1
I1101 06:09:02.815271 11248 net.cpp:425] scale4_1 <- bn4_1
I1101 06:09:02.815271 11248 net.cpp:399] scale4_1 -> scale4_1
I1101 06:09:02.815271 11248 layer_factory.hpp:77] Creating layer scale4_1
I1101 06:09:02.815773 11248 net.cpp:141] Setting up scale4_1
I1101 06:09:02.815773 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.815773 11248 net.cpp:156] Memory required for data: 707796400
I1101 06:09:02.815773 11248 layer_factory.hpp:77] Creating layer relu4_1
I1101 06:09:02.815773 11248 net.cpp:91] Creating Layer relu4_1
I1101 06:09:02.815773 11248 net.cpp:425] relu4_1 <- scale4_1
I1101 06:09:02.815773 11248 net.cpp:399] relu4_1 -> relu4_1
I1101 06:09:02.819275 11248 net.cpp:141] Setting up relu4_1
I1101 06:09:02.819275 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.819275 11248 net.cpp:156] Memory required for data: 712814000
I1101 06:09:02.819275 11248 layer_factory.hpp:77] Creating layer conv4_2
I1101 06:09:02.819275 11248 net.cpp:91] Creating Layer conv4_2
I1101 06:09:02.819275 11248 net.cpp:425] conv4_2 <- relu4_1
I1101 06:09:02.819275 11248 net.cpp:399] conv4_2 -> conv4_2
I1101 06:09:02.825937 11248 net.cpp:141] Setting up conv4_2
I1101 06:09:02.826437 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.826437 11248 net.cpp:156] Memory required for data: 717831600
I1101 06:09:02.826437 11248 layer_factory.hpp:77] Creating layer bn4_2
I1101 06:09:02.826437 11248 net.cpp:91] Creating Layer bn4_2
I1101 06:09:02.826437 11248 net.cpp:425] bn4_2 <- conv4_2
I1101 06:09:02.826437 11248 net.cpp:399] bn4_2 -> bn4_2
I1101 06:09:02.826437 11248 net.cpp:141] Setting up bn4_2
I1101 06:09:02.826437 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.826437 11248 net.cpp:156] Memory required for data: 722849200
I1101 06:09:02.826437 11248 layer_factory.hpp:77] Creating layer scale4_2
I1101 06:09:02.826437 11248 net.cpp:91] Creating Layer scale4_2
I1101 06:09:02.826437 11248 net.cpp:425] scale4_2 <- bn4_2
I1101 06:09:02.826437 11248 net.cpp:399] scale4_2 -> scale4_2
I1101 06:09:02.826437 11248 layer_factory.hpp:77] Creating layer scale4_2
I1101 06:09:02.826937 11248 net.cpp:141] Setting up scale4_2
I1101 06:09:02.826937 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.826937 11248 net.cpp:156] Memory required for data: 727866800
I1101 06:09:02.826937 11248 layer_factory.hpp:77] Creating layer relu4_2
I1101 06:09:02.826937 11248 net.cpp:91] Creating Layer relu4_2
I1101 06:09:02.826937 11248 net.cpp:425] relu4_2 <- scale4_2
I1101 06:09:02.826937 11248 net.cpp:399] relu4_2 -> relu4_2
I1101 06:09:02.826937 11248 net.cpp:141] Setting up relu4_2
I1101 06:09:02.826937 11248 net.cpp:148] Top shape: 100 256 7 7 (1254400)
I1101 06:09:02.826937 11248 net.cpp:156] Memory required for data: 732884400
I1101 06:09:02.826937 11248 layer_factory.hpp:77] Creating layer pool4_2
I1101 06:09:02.826937 11248 net.cpp:91] Creating Layer pool4_2
I1101 06:09:02.826937 11248 net.cpp:425] pool4_2 <- relu4_2
I1101 06:09:02.827437 11248 net.cpp:399] pool4_2 -> pool4_2
I1101 06:09:02.827437 11248 net.cpp:141] Setting up pool4_2
I1101 06:09:02.827437 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.827437 11248 net.cpp:156] Memory required for data: 734522800
I1101 06:09:02.827437 11248 layer_factory.hpp:77] Creating layer conv4_0
I1101 06:09:02.827437 11248 net.cpp:91] Creating Layer conv4_0
I1101 06:09:02.827437 11248 net.cpp:425] conv4_0 <- pool4_2
I1101 06:09:02.827437 11248 net.cpp:399] conv4_0 -> conv4_0
I1101 06:09:02.839370 11248 net.cpp:141] Setting up conv4_0
I1101 06:09:02.839370 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.839370 11248 net.cpp:156] Memory required for data: 737799600
I1101 06:09:02.839370 11248 layer_factory.hpp:77] Creating layer bn4_0
I1101 06:09:02.839370 11248 net.cpp:91] Creating Layer bn4_0
I1101 06:09:02.839370 11248 net.cpp:425] bn4_0 <- conv4_0
I1101 06:09:02.839370 11248 net.cpp:399] bn4_0 -> bn4_0
I1101 06:09:02.840370 11248 net.cpp:141] Setting up bn4_0
I1101 06:09:02.840370 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.840370 11248 net.cpp:156] Memory required for data: 741076400
I1101 06:09:02.840370 11248 layer_factory.hpp:77] Creating layer scale4_0
I1101 06:09:02.840370 11248 net.cpp:91] Creating Layer scale4_0
I1101 06:09:02.840370 11248 net.cpp:425] scale4_0 <- bn4_0
I1101 06:09:02.840370 11248 net.cpp:399] scale4_0 -> scale4_0
I1101 06:09:02.840370 11248 layer_factory.hpp:77] Creating layer scale4_0
I1101 06:09:02.840370 11248 net.cpp:141] Setting up scale4_0
I1101 06:09:02.840872 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.840872 11248 net.cpp:156] Memory required for data: 744353200
I1101 06:09:02.840872 11248 layer_factory.hpp:77] Creating layer relu4_0
I1101 06:09:02.840872 11248 net.cpp:91] Creating Layer relu4_0
I1101 06:09:02.840872 11248 net.cpp:425] relu4_0 <- scale4_0
I1101 06:09:02.840872 11248 net.cpp:399] relu4_0 -> relu4_0
I1101 06:09:02.840872 11248 net.cpp:141] Setting up relu4_0
I1101 06:09:02.840872 11248 net.cpp:148] Top shape: 100 512 4 4 (819200)
I1101 06:09:02.840872 11248 net.cpp:156] Memory required for data: 747630000
I1101 06:09:02.840872 11248 layer_factory.hpp:77] Creating layer cccp4
I1101 06:09:02.840872 11248 net.cpp:91] Creating Layer cccp4
I1101 06:09:02.840872 11248 net.cpp:425] cccp4 <- relu4_0
I1101 06:09:02.840872 11248 net.cpp:399] cccp4 -> cccp4
I1101 06:09:02.851378 11248 net.cpp:141] Setting up cccp4
I1101 06:09:02.851378 11248 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 06:09:02.851378 11248 net.cpp:156] Memory required for data: 760737200
I1101 06:09:02.851378 11248 layer_factory.hpp:77] Creating layer relu_cccp4
I1101 06:09:02.851378 11248 net.cpp:91] Creating Layer relu_cccp4
I1101 06:09:02.851378 11248 net.cpp:425] relu_cccp4 <- cccp4
I1101 06:09:02.851378 11248 net.cpp:386] relu_cccp4 -> cccp4 (in-place)
I1101 06:09:02.851878 11248 net.cpp:141] Setting up relu_cccp4
I1101 06:09:02.851878 11248 net.cpp:148] Top shape: 100 2048 4 4 (3276800)
I1101 06:09:02.851878 11248 net.cpp:156] Memory required for data: 773844400
I1101 06:09:02.851878 11248 layer_factory.hpp:77] Creating layer cccp5
I1101 06:09:02.851878 11248 net.cpp:91] Creating Layer cccp5
I1101 06:09:02.851878 11248 net.cpp:425] cccp5 <- cccp4
I1101 06:09:02.851878 11248 net.cpp:399] cccp5 -> cccp5
I1101 06:09:02.858177 11248 net.cpp:141] Setting up cccp5
I1101 06:09:02.858677 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.858677 11248 net.cpp:156] Memory required for data: 775482800
I1101 06:09:02.858677 11248 layer_factory.hpp:77] Creating layer relu_cccp5
I1101 06:09:02.858677 11248 net.cpp:91] Creating Layer relu_cccp5
I1101 06:09:02.858677 11248 net.cpp:425] relu_cccp5 <- cccp5
I1101 06:09:02.858677 11248 net.cpp:386] relu_cccp5 -> cccp5 (in-place)
I1101 06:09:02.858677 11248 net.cpp:141] Setting up relu_cccp5
I1101 06:09:02.858677 11248 net.cpp:148] Top shape: 100 256 4 4 (409600)
I1101 06:09:02.858677 11248 net.cpp:156] Memory required for data: 777121200
I1101 06:09:02.858677 11248 layer_factory.hpp:77] Creating layer poolcp5
I1101 06:09:02.858677 11248 net.cpp:91] Creating Layer poolcp5
I1101 06:09:02.858677 11248 net.cpp:425] poolcp5 <- cccp5
I1101 06:09:02.858677 11248 net.cpp:399] poolcp5 -> poolcp5
I1101 06:09:02.858677 11248 net.cpp:141] Setting up poolcp5
I1101 06:09:02.858677 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.858677 11248 net.cpp:156] Memory required for data: 777530800
I1101 06:09:02.858677 11248 layer_factory.hpp:77] Creating layer cccp6
I1101 06:09:02.859177 11248 net.cpp:91] Creating Layer cccp6
I1101 06:09:02.859177 11248 net.cpp:425] cccp6 <- poolcp5
I1101 06:09:02.859177 11248 net.cpp:399] cccp6 -> cccp6
I1101 06:09:02.866526 11248 net.cpp:141] Setting up cccp6
I1101 06:09:02.866526 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.866526 11248 net.cpp:156] Memory required for data: 777940400
I1101 06:09:02.866526 11248 layer_factory.hpp:77] Creating layer relu_cccp6
I1101 06:09:02.866526 11248 net.cpp:91] Creating Layer relu_cccp6
I1101 06:09:02.866526 11248 net.cpp:425] relu_cccp6 <- cccp6
I1101 06:09:02.866526 11248 net.cpp:386] relu_cccp6 -> cccp6 (in-place)
I1101 06:09:02.866526 11248 net.cpp:141] Setting up relu_cccp6
I1101 06:09:02.866526 11248 net.cpp:148] Top shape: 100 256 2 2 (102400)
I1101 06:09:02.866526 11248 net.cpp:156] Memory required for data: 778350000
I1101 06:09:02.866526 11248 layer_factory.hpp:77] Creating layer poolcp6
I1101 06:09:02.866526 11248 net.cpp:91] Creating Layer poolcp6
I1101 06:09:02.866526 11248 net.cpp:425] poolcp6 <- cccp6
I1101 06:09:02.866526 11248 net.cpp:399] poolcp6 -> poolcp6
I1101 06:09:02.866526 11248 net.cpp:141] Setting up poolcp6
I1101 06:09:02.867027 11248 net.cpp:148] Top shape: 100 256 1 1 (25600)
I1101 06:09:02.867027 11248 net.cpp:156] Memory required for data: 778452400
I1101 06:09:02.867027 11248 layer_factory.hpp:77] Creating layer ip1
I1101 06:09:02.867027 11248 net.cpp:91] Creating Layer ip1
I1101 06:09:02.867027 11248 net.cpp:425] ip1 <- poolcp6
I1101 06:09:02.867027 11248 net.cpp:399] ip1 -> ip1
I1101 06:09:02.867027 11248 net.cpp:141] Setting up ip1
I1101 06:09:02.867027 11248 net.cpp:148] Top shape: 100 10 (1000)
I1101 06:09:02.867027 11248 net.cpp:156] Memory required for data: 778456400
I1101 06:09:02.867027 11248 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I1101 06:09:02.867027 11248 net.cpp:91] Creating Layer ip1_ip1_0_split
I1101 06:09:02.867027 11248 net.cpp:425] ip1_ip1_0_split <- ip1
I1101 06:09:02.867027 11248 net.cpp:399] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1101 06:09:02.867027 11248 net.cpp:399] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1101 06:09:02.867027 11248 net.cpp:141] Setting up ip1_ip1_0_split
I1101 06:09:02.867027 11248 net.cpp:148] Top shape: 100 10 (1000)
I1101 06:09:02.867027 11248 net.cpp:148] Top shape: 100 10 (1000)
I1101 06:09:02.867027 11248 net.cpp:156] Memory required for data: 778464400
I1101 06:09:02.867027 11248 layer_factory.hpp:77] Creating layer accuracy
I1101 06:09:02.867027 11248 net.cpp:91] Creating Layer accuracy
I1101 06:09:02.867027 11248 net.cpp:425] accuracy <- ip1_ip1_0_split_0
I1101 06:09:02.867027 11248 net.cpp:425] accuracy <- label_mnist_1_split_0
I1101 06:09:02.867027 11248 net.cpp:399] accuracy -> accuracy
I1101 06:09:02.867027 11248 net.cpp:141] Setting up accuracy
I1101 06:09:02.867027 11248 net.cpp:148] Top shape: (1)
I1101 06:09:02.867027 11248 net.cpp:156] Memory required for data: 778464404
I1101 06:09:02.867027 11248 layer_factory.hpp:77] Creating layer loss
I1101 06:09:02.867027 11248 net.cpp:91] Creating Layer loss
I1101 06:09:02.867027 11248 net.cpp:425] loss <- ip1_ip1_0_split_1
I1101 06:09:02.867027 11248 net.cpp:425] loss <- label_mnist_1_split_1
I1101 06:09:02.867027 11248 net.cpp:399] loss -> loss
I1101 06:09:02.867027 11248 layer_factory.hpp:77] Creating layer loss
I1101 06:09:02.868027 11248 net.cpp:141] Setting up loss
I1101 06:09:02.868027 11248 net.cpp:148] Top shape: (1)
I1101 06:09:02.868027 11248 net.cpp:151]     with loss weight 1
I1101 06:09:02.868027 11248 net.cpp:156] Memory required for data: 778464408
I1101 06:09:02.868027 11248 net.cpp:217] loss needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:219] accuracy does not need backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] ip1_ip1_0_split needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] ip1 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] poolcp6 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu_cccp6 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] cccp6 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] poolcp5 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu_cccp5 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] cccp5 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu_cccp4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] cccp4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu4_0 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale4_0 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn4_0 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv4_0 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] pool4_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu4_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale4_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn4_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv4_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu4_1 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale4_1 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn4_1 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv4_1 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] pool4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv4 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu3 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale3 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn3 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv3 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] relu2_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] scale2_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] bn2_2 needs backward computation.
I1101 06:09:02.868027 11248 net.cpp:217] conv2_2 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] pool2_1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] relu2_1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] scale2_1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] bn2_1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] conv2_1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] relu2 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] scale2 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] bn2 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] conv2 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] relu1_0 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] scale1_0 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] bn1_0 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] conv1_0 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] relu1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] scale1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] bn1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:217] conv1 needs backward computation.
I1101 06:09:02.868527 11248 net.cpp:219] label_mnist_1_split does not need backward computation.
I1101 06:09:02.868527 11248 net.cpp:219] mnist does not need backward computation.
I1101 06:09:02.868527 11248 net.cpp:261] This network produces output accuracy
I1101 06:09:02.868527 11248 net.cpp:261] This network produces output loss
I1101 06:09:02.868527 11248 net.cpp:274] Network initialization done.
I1101 06:09:02.868527 11248 solver.cpp:60] Solver scaffolding done.
I1101 06:09:02.875532 11248 caffe.cpp:220] Starting Optimization
I1101 06:09:02.875532 11248 solver.cpp:279] Solving SimpNet
I1101 06:09:02.875532 11248 solver.cpp:280] Learning Rate Policy: multistep
I1101 06:09:02.880956 11248 solver.cpp:337] Iteration 0, Testing net (#0)
I1101 06:09:07.845988 11248 solver.cpp:404]     Test net output #0: accuracy = 0.1032
I1101 06:09:07.846488 11248 solver.cpp:404]     Test net output #1: loss = 78.3234 (* 1 = 78.3234 loss)
I1101 06:09:08.141551 11248 solver.cpp:228] Iteration 0, loss = 2.49874
I1101 06:09:08.141551 11248 solver.cpp:244]     Train net output #0: loss = 2.49874 (* 1 = 2.49874 loss)
I1101 06:09:08.141551 11248 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I1101 06:09:23.804548 11248 solver.cpp:228] Iteration 100, loss = 0.193079
I1101 06:09:23.804548 11248 solver.cpp:244]     Train net output #0: loss = 0.193079 (* 1 = 0.193079 loss)
I1101 06:09:23.804548 11248 sgd_solver.cpp:106] Iteration 100, lr = 0.1
I1101 06:09:39.472107 11248 solver.cpp:228] Iteration 200, loss = 0.0883893
I1101 06:09:39.472107 11248 solver.cpp:244]     Train net output #0: loss = 0.0883893 (* 1 = 0.0883893 loss)
I1101 06:09:39.472107 11248 sgd_solver.cpp:106] Iteration 200, lr = 0.1
I1101 06:09:54.965219 11248 solver.cpp:228] Iteration 300, loss = 0.0494932
I1101 06:09:54.965219 11248 solver.cpp:244]     Train net output #0: loss = 0.0494932 (* 1 = 0.0494932 loss)
I1101 06:09:54.965219 11248 sgd_solver.cpp:106] Iteration 300, lr = 0.1
I1101 06:10:10.460500 11248 solver.cpp:228] Iteration 400, loss = 0.0716056
I1101 06:10:10.460500 11248 solver.cpp:244]     Train net output #0: loss = 0.0716056 (* 1 = 0.0716056 loss)
I1101 06:10:10.460500 11248 sgd_solver.cpp:106] Iteration 400, lr = 0.1
I1101 06:10:25.926625 11248 solver.cpp:228] Iteration 500, loss = 0.0320523
I1101 06:10:25.926625 11248 solver.cpp:244]     Train net output #0: loss = 0.0320523 (* 1 = 0.0320523 loss)
I1101 06:10:25.926625 11248 sgd_solver.cpp:106] Iteration 500, lr = 0.1
I1101 06:10:41.390785 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_600.caffemodel
I1101 06:10:41.563824 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_600.solverstate
I1101 06:10:41.660877 11248 solver.cpp:337] Iteration 600, Testing net (#0)
I1101 06:10:46.390647 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9827
I1101 06:10:46.391649 11248 solver.cpp:404]     Test net output #1: loss = 0.0559863 (* 1 = 0.0559863 loss)
I1101 06:10:46.456707 11248 solver.cpp:228] Iteration 600, loss = 0.0445445
I1101 06:10:46.456707 11248 solver.cpp:244]     Train net output #0: loss = 0.0445445 (* 1 = 0.0445445 loss)
I1101 06:10:46.456707 11248 sgd_solver.cpp:106] Iteration 600, lr = 0.1
I1101 06:11:01.933320 11248 solver.cpp:228] Iteration 700, loss = 0.0560711
I1101 06:11:01.933320 11248 solver.cpp:244]     Train net output #0: loss = 0.0560711 (* 1 = 0.0560711 loss)
I1101 06:11:01.933320 11248 sgd_solver.cpp:106] Iteration 700, lr = 0.1
I1101 06:11:17.420310 11248 solver.cpp:228] Iteration 800, loss = 0.0355913
I1101 06:11:17.420310 11248 solver.cpp:244]     Train net output #0: loss = 0.0355913 (* 1 = 0.0355913 loss)
I1101 06:11:17.420310 11248 sgd_solver.cpp:106] Iteration 800, lr = 0.1
I1101 06:11:32.909682 11248 solver.cpp:228] Iteration 900, loss = 0.0284311
I1101 06:11:32.909682 11248 solver.cpp:244]     Train net output #0: loss = 0.028431 (* 1 = 0.028431 loss)
I1101 06:11:32.909682 11248 sgd_solver.cpp:106] Iteration 900, lr = 0.1
I1101 06:11:48.423066 11248 solver.cpp:228] Iteration 1000, loss = 0.0292985
I1101 06:11:48.423066 11248 solver.cpp:244]     Train net output #0: loss = 0.0292984 (* 1 = 0.0292984 loss)
I1101 06:11:48.423066 11248 sgd_solver.cpp:106] Iteration 1000, lr = 0.1
I1101 06:12:03.915745 11248 solver.cpp:228] Iteration 1100, loss = 0.0105367
I1101 06:12:03.915745 11248 solver.cpp:244]     Train net output #0: loss = 0.0105366 (* 1 = 0.0105366 loss)
I1101 06:12:03.915745 11248 sgd_solver.cpp:106] Iteration 1100, lr = 0.1
I1101 06:12:19.368485 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_1200.caffemodel
I1101 06:12:19.528599 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_1200.solverstate
I1101 06:12:19.627670 11248 solver.cpp:337] Iteration 1200, Testing net (#0)
I1101 06:12:24.288987 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9635
I1101 06:12:24.288987 11248 solver.cpp:404]     Test net output #1: loss = 0.117115 (* 1 = 0.117115 loss)
I1101 06:12:24.354046 11248 solver.cpp:228] Iteration 1200, loss = 0.02383
I1101 06:12:24.354046 11248 solver.cpp:244]     Train net output #0: loss = 0.02383 (* 1 = 0.02383 loss)
I1101 06:12:24.354046 11248 sgd_solver.cpp:106] Iteration 1200, lr = 0.1
I1101 06:12:39.840059 11248 solver.cpp:228] Iteration 1300, loss = 0.0317868
I1101 06:12:39.840059 11248 solver.cpp:244]     Train net output #0: loss = 0.0317867 (* 1 = 0.0317867 loss)
I1101 06:12:39.840059 11248 sgd_solver.cpp:106] Iteration 1300, lr = 0.1
I1101 06:12:55.366317 11248 solver.cpp:228] Iteration 1400, loss = 0.0204141
I1101 06:12:55.366317 11248 solver.cpp:244]     Train net output #0: loss = 0.0204141 (* 1 = 0.0204141 loss)
I1101 06:12:55.366317 11248 sgd_solver.cpp:106] Iteration 1400, lr = 0.1
I1101 06:13:11.725922 11248 solver.cpp:228] Iteration 1500, loss = 0.026569
I1101 06:13:11.725922 11248 solver.cpp:244]     Train net output #0: loss = 0.0265689 (* 1 = 0.0265689 loss)
I1101 06:13:11.725922 11248 sgd_solver.cpp:106] Iteration 1500, lr = 0.1
I1101 06:13:27.615607 11248 solver.cpp:228] Iteration 1600, loss = 0.00794862
I1101 06:13:27.615607 11248 solver.cpp:244]     Train net output #0: loss = 0.00794855 (* 1 = 0.00794855 loss)
I1101 06:13:27.615607 11248 sgd_solver.cpp:106] Iteration 1600, lr = 0.1
I1101 06:13:43.485955 11248 solver.cpp:228] Iteration 1700, loss = 0.0349351
I1101 06:13:43.485955 11248 solver.cpp:244]     Train net output #0: loss = 0.0349351 (* 1 = 0.0349351 loss)
I1101 06:13:43.485955 11248 sgd_solver.cpp:106] Iteration 1700, lr = 0.1
I1101 06:13:59.311389 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_1800.caffemodel
I1101 06:13:59.480512 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_1800.solverstate
I1101 06:13:59.582583 11248 solver.cpp:337] Iteration 1800, Testing net (#0)
I1101 06:14:04.378811 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9828
I1101 06:14:04.378811 11248 solver.cpp:404]     Test net output #1: loss = 0.0611486 (* 1 = 0.0611486 loss)
I1101 06:14:04.445812 11248 solver.cpp:228] Iteration 1800, loss = 0.0500533
I1101 06:14:04.445812 11248 solver.cpp:244]     Train net output #0: loss = 0.0500533 (* 1 = 0.0500533 loss)
I1101 06:14:04.445812 11248 sgd_solver.cpp:106] Iteration 1800, lr = 0.1
I1101 06:14:20.526003 11248 solver.cpp:228] Iteration 1900, loss = 0.0818913
I1101 06:14:20.526003 11248 solver.cpp:244]     Train net output #0: loss = 0.0818913 (* 1 = 0.0818913 loss)
I1101 06:14:20.526003 11248 sgd_solver.cpp:106] Iteration 1900, lr = 0.1
I1101 06:14:36.427408 11248 solver.cpp:228] Iteration 2000, loss = 0.0452563
I1101 06:14:36.427408 11248 solver.cpp:244]     Train net output #0: loss = 0.0452563 (* 1 = 0.0452563 loss)
I1101 06:14:36.427408 11248 sgd_solver.cpp:106] Iteration 2000, lr = 0.1
I1101 06:14:52.529134 11248 solver.cpp:228] Iteration 2100, loss = 0.0311605
I1101 06:14:52.529134 11248 solver.cpp:244]     Train net output #0: loss = 0.0311605 (* 1 = 0.0311605 loss)
I1101 06:14:52.529134 11248 sgd_solver.cpp:106] Iteration 2100, lr = 0.1
I1101 06:15:08.486567 11248 solver.cpp:228] Iteration 2200, loss = 0.483697
I1101 06:15:08.486567 11248 solver.cpp:244]     Train net output #0: loss = 0.483697 (* 1 = 0.483697 loss)
I1101 06:15:08.486567 11248 sgd_solver.cpp:106] Iteration 2200, lr = 0.1
I1101 06:15:24.347744 11248 solver.cpp:228] Iteration 2300, loss = 0.0181059
I1101 06:15:24.347744 11248 solver.cpp:244]     Train net output #0: loss = 0.018106 (* 1 = 0.018106 loss)
I1101 06:15:24.347744 11248 sgd_solver.cpp:106] Iteration 2300, lr = 0.1
I1101 06:15:40.006553 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_2400.caffemodel
I1101 06:15:40.165743 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_2400.solverstate
I1101 06:15:40.273820 11248 solver.cpp:337] Iteration 2400, Testing net (#0)
I1101 06:15:45.002660 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9897
I1101 06:15:45.003161 11248 solver.cpp:404]     Test net output #1: loss = 0.0449939 (* 1 = 0.0449939 loss)
I1101 06:15:45.069672 11248 solver.cpp:228] Iteration 2400, loss = 0.0195577
I1101 06:15:45.069672 11248 solver.cpp:244]     Train net output #0: loss = 0.0195577 (* 1 = 0.0195577 loss)
I1101 06:15:45.069672 11248 sgd_solver.cpp:106] Iteration 2400, lr = 0.1
I1101 06:16:01.195168 11248 solver.cpp:228] Iteration 2500, loss = 0.0430976
I1101 06:16:01.195168 11248 solver.cpp:244]     Train net output #0: loss = 0.0430976 (* 1 = 0.0430976 loss)
I1101 06:16:01.195168 11248 sgd_solver.cpp:106] Iteration 2500, lr = 0.1
I1101 06:16:17.136754 11248 solver.cpp:228] Iteration 2600, loss = 0.0205731
I1101 06:16:17.137254 11248 solver.cpp:244]     Train net output #0: loss = 0.0205731 (* 1 = 0.0205731 loss)
I1101 06:16:17.137254 11248 sgd_solver.cpp:106] Iteration 2600, lr = 0.1
I1101 06:16:33.041443 11248 solver.cpp:228] Iteration 2700, loss = 0.0228005
I1101 06:16:33.041443 11248 solver.cpp:244]     Train net output #0: loss = 0.0228004 (* 1 = 0.0228004 loss)
I1101 06:16:33.041443 11248 sgd_solver.cpp:106] Iteration 2700, lr = 0.1
I1101 06:16:48.914825 11248 solver.cpp:228] Iteration 2800, loss = 0.00693539
I1101 06:16:48.914825 11248 solver.cpp:244]     Train net output #0: loss = 0.00693537 (* 1 = 0.00693537 loss)
I1101 06:16:48.914825 11248 sgd_solver.cpp:106] Iteration 2800, lr = 0.1
I1101 06:17:04.813129 11248 solver.cpp:228] Iteration 2900, loss = 0.0263434
I1101 06:17:04.813129 11248 solver.cpp:244]     Train net output #0: loss = 0.0263433 (* 1 = 0.0263433 loss)
I1101 06:17:04.813129 11248 sgd_solver.cpp:106] Iteration 2900, lr = 0.1
I1101 06:17:20.696461 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_3000.caffemodel
I1101 06:17:20.857430 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_3000.solverstate
I1101 06:17:20.959002 11248 solver.cpp:337] Iteration 3000, Testing net (#0)
I1101 06:17:25.703068 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9882
I1101 06:17:25.703068 11248 solver.cpp:404]     Test net output #1: loss = 0.0473299 (* 1 = 0.0473299 loss)
I1101 06:17:25.769687 11248 solver.cpp:228] Iteration 3000, loss = 0.0189151
I1101 06:17:25.769687 11248 solver.cpp:244]     Train net output #0: loss = 0.0189151 (* 1 = 0.0189151 loss)
I1101 06:17:25.769687 11248 sgd_solver.cpp:106] Iteration 3000, lr = 0.1
I1101 06:17:41.446173 11248 solver.cpp:228] Iteration 3100, loss = 0.0223262
I1101 06:17:41.446173 11248 solver.cpp:244]     Train net output #0: loss = 0.0223262 (* 1 = 0.0223262 loss)
I1101 06:17:41.446173 11248 sgd_solver.cpp:106] Iteration 3100, lr = 0.1
I1101 06:17:57.274534 11248 solver.cpp:228] Iteration 3200, loss = 0.0583104
I1101 06:17:57.274534 11248 solver.cpp:244]     Train net output #0: loss = 0.0583104 (* 1 = 0.0583104 loss)
I1101 06:17:57.274534 11248 sgd_solver.cpp:106] Iteration 3200, lr = 0.1
I1101 06:18:13.190642 11248 solver.cpp:228] Iteration 3300, loss = 0.047693
I1101 06:18:13.190642 11248 solver.cpp:244]     Train net output #0: loss = 0.0476931 (* 1 = 0.0476931 loss)
I1101 06:18:13.190642 11248 sgd_solver.cpp:106] Iteration 3300, lr = 0.1
I1101 06:18:29.074313 11248 solver.cpp:228] Iteration 3400, loss = 0.0115489
I1101 06:18:29.074313 11248 solver.cpp:244]     Train net output #0: loss = 0.0115489 (* 1 = 0.0115489 loss)
I1101 06:18:29.074313 11248 sgd_solver.cpp:106] Iteration 3400, lr = 0.1
I1101 06:18:44.950227 11248 solver.cpp:228] Iteration 3500, loss = 0.0161106
I1101 06:18:44.950727 11248 solver.cpp:244]     Train net output #0: loss = 0.0161106 (* 1 = 0.0161106 loss)
I1101 06:18:44.950727 11248 sgd_solver.cpp:106] Iteration 3500, lr = 0.1
I1101 06:19:00.773977 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_3600.caffemodel
I1101 06:19:00.951355 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_3600.solverstate
I1101 06:19:01.057431 11248 solver.cpp:337] Iteration 3600, Testing net (#0)
I1101 06:19:05.885002 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9869
I1101 06:19:05.885002 11248 solver.cpp:404]     Test net output #1: loss = 0.051027 (* 1 = 0.051027 loss)
I1101 06:19:05.951550 11248 solver.cpp:228] Iteration 3600, loss = 0.0134801
I1101 06:19:05.951550 11248 solver.cpp:244]     Train net output #0: loss = 0.0134801 (* 1 = 0.0134801 loss)
I1101 06:19:05.951550 11248 sgd_solver.cpp:106] Iteration 3600, lr = 0.1
I1101 06:19:22.044986 11248 solver.cpp:228] Iteration 3700, loss = 0.0233442
I1101 06:19:22.044986 11248 solver.cpp:244]     Train net output #0: loss = 0.0233442 (* 1 = 0.0233442 loss)
I1101 06:19:22.044986 11248 sgd_solver.cpp:106] Iteration 3700, lr = 0.1
I1101 06:19:38.055563 11248 solver.cpp:228] Iteration 3800, loss = 0.0187837
I1101 06:19:38.055563 11248 solver.cpp:244]     Train net output #0: loss = 0.0187837 (* 1 = 0.0187837 loss)
I1101 06:19:38.055563 11248 sgd_solver.cpp:106] Iteration 3800, lr = 0.1
I1101 06:19:54.070940 11248 solver.cpp:228] Iteration 3900, loss = 0.028016
I1101 06:19:54.070940 11248 solver.cpp:244]     Train net output #0: loss = 0.028016 (* 1 = 0.028016 loss)
I1101 06:19:54.070940 11248 sgd_solver.cpp:106] Iteration 3900, lr = 0.1
I1101 06:20:09.983007 11248 solver.cpp:228] Iteration 4000, loss = 0.00715868
I1101 06:20:09.983007 11248 solver.cpp:244]     Train net output #0: loss = 0.00715868 (* 1 = 0.00715868 loss)
I1101 06:20:09.983007 11248 sgd_solver.cpp:106] Iteration 4000, lr = 0.1
I1101 06:20:25.882375 11248 solver.cpp:228] Iteration 4100, loss = 0.0112363
I1101 06:20:25.882375 11248 solver.cpp:244]     Train net output #0: loss = 0.0112363 (* 1 = 0.0112363 loss)
I1101 06:20:25.882375 11248 sgd_solver.cpp:106] Iteration 4100, lr = 0.1
I1101 06:20:41.690858 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_4200.caffemodel
I1101 06:20:41.853976 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_4200.solverstate
I1101 06:20:41.955046 11248 solver.cpp:337] Iteration 4200, Testing net (#0)
I1101 06:20:46.739542 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9914
I1101 06:20:46.740041 11248 solver.cpp:404]     Test net output #1: loss = 0.0361214 (* 1 = 0.0361214 loss)
I1101 06:20:46.807598 11248 solver.cpp:228] Iteration 4200, loss = 0.0133158
I1101 06:20:46.807598 11248 solver.cpp:244]     Train net output #0: loss = 0.0133158 (* 1 = 0.0133158 loss)
I1101 06:20:46.808097 11248 sgd_solver.cpp:106] Iteration 4200, lr = 0.1
I1101 06:21:02.719754 11248 solver.cpp:228] Iteration 4300, loss = 0.0433468
I1101 06:21:02.719754 11248 solver.cpp:244]     Train net output #0: loss = 0.0433468 (* 1 = 0.0433468 loss)
I1101 06:21:02.719754 11248 sgd_solver.cpp:106] Iteration 4300, lr = 0.1
I1101 06:21:18.627049 11248 solver.cpp:228] Iteration 4400, loss = 0.0189194
I1101 06:21:18.627550 11248 solver.cpp:244]     Train net output #0: loss = 0.0189194 (* 1 = 0.0189194 loss)
I1101 06:21:18.627550 11248 sgd_solver.cpp:106] Iteration 4400, lr = 0.1
I1101 06:21:34.529428 11248 solver.cpp:228] Iteration 4500, loss = 0.0206191
I1101 06:21:34.529428 11248 solver.cpp:244]     Train net output #0: loss = 0.0206191 (* 1 = 0.0206191 loss)
I1101 06:21:34.529428 11248 sgd_solver.cpp:106] Iteration 4500, lr = 0.1
I1101 06:21:50.446660 11248 solver.cpp:228] Iteration 4600, loss = 0.00445486
I1101 06:21:50.446660 11248 solver.cpp:244]     Train net output #0: loss = 0.00445484 (* 1 = 0.00445484 loss)
I1101 06:21:50.446660 11248 sgd_solver.cpp:106] Iteration 4600, lr = 0.1
I1101 06:22:06.354876 11248 solver.cpp:228] Iteration 4700, loss = 0.0186076
I1101 06:22:06.354876 11248 solver.cpp:244]     Train net output #0: loss = 0.0186076 (* 1 = 0.0186076 loss)
I1101 06:22:06.354876 11248 sgd_solver.cpp:106] Iteration 4700, lr = 0.1
I1101 06:22:22.165748 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_4800.caffemodel
I1101 06:22:22.329454 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_4800.solverstate
I1101 06:22:22.430526 11248 solver.cpp:337] Iteration 4800, Testing net (#0)
I1101 06:22:27.232954 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9892
I1101 06:22:27.232954 11248 solver.cpp:404]     Test net output #1: loss = 0.0449651 (* 1 = 0.0449651 loss)
I1101 06:22:27.300565 11248 solver.cpp:228] Iteration 4800, loss = 0.010963
I1101 06:22:27.300565 11248 solver.cpp:244]     Train net output #0: loss = 0.010963 (* 1 = 0.010963 loss)
I1101 06:22:27.300565 11248 sgd_solver.cpp:106] Iteration 4800, lr = 0.1
I1101 06:22:43.185519 11248 solver.cpp:228] Iteration 4900, loss = 0.0329474
I1101 06:22:43.185519 11248 solver.cpp:244]     Train net output #0: loss = 0.0329474 (* 1 = 0.0329474 loss)
I1101 06:22:43.185519 11248 sgd_solver.cpp:106] Iteration 4900, lr = 0.1
I1101 06:22:59.067780 11248 solver.cpp:228] Iteration 5000, loss = 0.0288075
I1101 06:22:59.068279 11248 solver.cpp:244]     Train net output #0: loss = 0.0288075 (* 1 = 0.0288075 loss)
I1101 06:22:59.068279 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1101 06:22:59.068279 11248 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I1101 06:23:14.969182 11248 solver.cpp:228] Iteration 5100, loss = 0.0211047
I1101 06:23:14.969182 11248 solver.cpp:244]     Train net output #0: loss = 0.0211047 (* 1 = 0.0211047 loss)
I1101 06:23:14.969182 11248 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I1101 06:23:30.843629 11248 solver.cpp:228] Iteration 5200, loss = 0.00339662
I1101 06:23:30.843629 11248 solver.cpp:244]     Train net output #0: loss = 0.00339661 (* 1 = 0.00339661 loss)
I1101 06:23:30.843629 11248 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I1101 06:23:46.751747 11248 solver.cpp:228] Iteration 5300, loss = 0.00560301
I1101 06:23:46.751747 11248 solver.cpp:244]     Train net output #0: loss = 0.005603 (* 1 = 0.005603 loss)
I1101 06:23:46.751747 11248 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I1101 06:24:02.607509 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_5400.caffemodel
I1101 06:24:02.768123 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_5400.solverstate
I1101 06:24:02.902904 11248 solver.cpp:337] Iteration 5400, Testing net (#0)
I1101 06:24:07.683420 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 06:24:07.683420 11248 solver.cpp:404]     Test net output #1: loss = 0.0139212 (* 1 = 0.0139212 loss)
I1101 06:24:07.752154 11248 solver.cpp:228] Iteration 5400, loss = 0.00443851
I1101 06:24:07.752154 11248 solver.cpp:244]     Train net output #0: loss = 0.0044385 (* 1 = 0.0044385 loss)
I1101 06:24:07.752154 11248 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I1101 06:24:23.638216 11248 solver.cpp:228] Iteration 5500, loss = 0.00403005
I1101 06:24:23.638216 11248 solver.cpp:244]     Train net output #0: loss = 0.00403004 (* 1 = 0.00403004 loss)
I1101 06:24:23.638216 11248 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I1101 06:24:39.460674 11248 solver.cpp:228] Iteration 5600, loss = 0.00985742
I1101 06:24:39.460674 11248 solver.cpp:244]     Train net output #0: loss = 0.00985741 (* 1 = 0.00985741 loss)
I1101 06:24:39.460674 11248 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I1101 06:24:55.184368 11248 solver.cpp:228] Iteration 5700, loss = 0.0120363
I1101 06:24:55.184368 11248 solver.cpp:244]     Train net output #0: loss = 0.0120363 (* 1 = 0.0120363 loss)
I1101 06:24:55.184368 11248 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I1101 06:25:10.854022 11248 solver.cpp:228] Iteration 5800, loss = 0.00314414
I1101 06:25:10.854022 11248 solver.cpp:244]     Train net output #0: loss = 0.00314414 (* 1 = 0.00314414 loss)
I1101 06:25:10.854022 11248 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I1101 06:25:26.538954 11248 solver.cpp:228] Iteration 5900, loss = 0.00520722
I1101 06:25:26.538954 11248 solver.cpp:244]     Train net output #0: loss = 0.00520721 (* 1 = 0.00520721 loss)
I1101 06:25:26.538954 11248 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I1101 06:25:42.122256 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_6000.caffemodel
I1101 06:25:42.280869 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_6000.solverstate
I1101 06:25:42.407814 11248 solver.cpp:337] Iteration 6000, Testing net (#0)
I1101 06:25:47.134449 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9969
I1101 06:25:47.134449 11248 solver.cpp:404]     Test net output #1: loss = 0.0135872 (* 1 = 0.0135872 loss)
I1101 06:25:47.201319 11248 solver.cpp:228] Iteration 6000, loss = 0.00401076
I1101 06:25:47.201319 11248 solver.cpp:244]     Train net output #0: loss = 0.00401075 (* 1 = 0.00401075 loss)
I1101 06:25:47.201319 11248 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I1101 06:26:02.910423 11248 solver.cpp:228] Iteration 6100, loss = 0.00390247
I1101 06:26:02.910423 11248 solver.cpp:244]     Train net output #0: loss = 0.00390246 (* 1 = 0.00390246 loss)
I1101 06:26:02.910423 11248 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I1101 06:26:18.941571 11248 solver.cpp:228] Iteration 6200, loss = 0.00871507
I1101 06:26:18.941571 11248 solver.cpp:244]     Train net output #0: loss = 0.00871505 (* 1 = 0.00871505 loss)
I1101 06:26:18.941571 11248 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I1101 06:26:34.870501 11248 solver.cpp:228] Iteration 6300, loss = 0.0100345
I1101 06:26:34.870501 11248 solver.cpp:244]     Train net output #0: loss = 0.0100345 (* 1 = 0.0100345 loss)
I1101 06:26:34.870501 11248 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I1101 06:26:50.830116 11248 solver.cpp:228] Iteration 6400, loss = 0.0031227
I1101 06:26:50.830116 11248 solver.cpp:244]     Train net output #0: loss = 0.00312269 (* 1 = 0.00312269 loss)
I1101 06:26:50.830116 11248 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I1101 06:27:06.790961 11248 solver.cpp:228] Iteration 6500, loss = 0.00480638
I1101 06:27:06.790961 11248 solver.cpp:244]     Train net output #0: loss = 0.00480637 (* 1 = 0.00480637 loss)
I1101 06:27:06.790961 11248 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I1101 06:27:22.614413 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_6600.caffemodel
I1101 06:27:22.776031 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_6600.solverstate
I1101 06:27:22.879103 11248 solver.cpp:337] Iteration 6600, Testing net (#0)
I1101 06:27:27.677609 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9966
I1101 06:27:27.678110 11248 solver.cpp:404]     Test net output #1: loss = 0.013537 (* 1 = 0.013537 loss)
I1101 06:27:27.745626 11248 solver.cpp:228] Iteration 6600, loss = 0.00384422
I1101 06:27:27.745626 11248 solver.cpp:244]     Train net output #0: loss = 0.00384422 (* 1 = 0.00384422 loss)
I1101 06:27:27.745626 11248 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I1101 06:27:43.627979 11248 solver.cpp:228] Iteration 6700, loss = 0.00402086
I1101 06:27:43.627979 11248 solver.cpp:244]     Train net output #0: loss = 0.00402086 (* 1 = 0.00402086 loss)
I1101 06:27:43.627979 11248 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I1101 06:27:59.516234 11248 solver.cpp:228] Iteration 6800, loss = 0.00781979
I1101 06:27:59.516234 11248 solver.cpp:244]     Train net output #0: loss = 0.00781979 (* 1 = 0.00781979 loss)
I1101 06:27:59.516234 11248 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I1101 06:28:15.381357 11248 solver.cpp:228] Iteration 6900, loss = 0.00874974
I1101 06:28:15.381357 11248 solver.cpp:244]     Train net output #0: loss = 0.00874974 (* 1 = 0.00874974 loss)
I1101 06:28:15.381357 11248 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I1101 06:28:31.255239 11248 solver.cpp:228] Iteration 7000, loss = 0.00308044
I1101 06:28:31.255239 11248 solver.cpp:244]     Train net output #0: loss = 0.00308044 (* 1 = 0.00308044 loss)
I1101 06:28:31.255239 11248 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I1101 06:28:47.123486 11248 solver.cpp:228] Iteration 7100, loss = 0.00447255
I1101 06:28:47.123486 11248 solver.cpp:244]     Train net output #0: loss = 0.00447255 (* 1 = 0.00447255 loss)
I1101 06:28:47.123486 11248 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I1101 06:29:02.934614 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_7200.caffemodel
I1101 06:29:03.098852 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_7200.solverstate
I1101 06:29:03.205430 11248 solver.cpp:337] Iteration 7200, Testing net (#0)
I1101 06:29:07.995880 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9965
I1101 06:29:07.995880 11248 solver.cpp:404]     Test net output #1: loss = 0.0135165 (* 1 = 0.0135165 loss)
I1101 06:29:08.063446 11248 solver.cpp:228] Iteration 7200, loss = 0.00365934
I1101 06:29:08.063446 11248 solver.cpp:244]     Train net output #0: loss = 0.00365934 (* 1 = 0.00365934 loss)
I1101 06:29:08.063446 11248 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I1101 06:29:23.935654 11248 solver.cpp:228] Iteration 7300, loss = 0.00413434
I1101 06:29:23.935654 11248 solver.cpp:244]     Train net output #0: loss = 0.00413434 (* 1 = 0.00413434 loss)
I1101 06:29:23.935654 11248 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I1101 06:29:39.805865 11248 solver.cpp:228] Iteration 7400, loss = 0.00723222
I1101 06:29:39.805865 11248 solver.cpp:244]     Train net output #0: loss = 0.00723221 (* 1 = 0.00723221 loss)
I1101 06:29:39.806365 11248 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I1101 06:29:55.754596 11248 solver.cpp:228] Iteration 7500, loss = 0.00779274
I1101 06:29:55.755096 11248 solver.cpp:244]     Train net output #0: loss = 0.00779274 (* 1 = 0.00779274 loss)
I1101 06:29:55.755096 11248 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I1101 06:30:11.694443 11248 solver.cpp:228] Iteration 7600, loss = 0.00304406
I1101 06:30:11.694443 11248 solver.cpp:244]     Train net output #0: loss = 0.00304406 (* 1 = 0.00304406 loss)
I1101 06:30:11.694443 11248 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I1101 06:30:27.748414 11248 solver.cpp:228] Iteration 7700, loss = 0.00420949
I1101 06:30:27.748414 11248 solver.cpp:244]     Train net output #0: loss = 0.00420949 (* 1 = 0.00420949 loss)
I1101 06:30:27.748414 11248 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I1101 06:30:43.221699 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_7800.caffemodel
I1101 06:30:43.378811 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_7800.solverstate
I1101 06:30:43.478883 11248 solver.cpp:337] Iteration 7800, Testing net (#0)
I1101 06:30:48.167080 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9965
I1101 06:30:48.167080 11248 solver.cpp:404]     Test net output #1: loss = 0.0135012 (* 1 = 0.0135012 loss)
I1101 06:30:48.232578 11248 solver.cpp:228] Iteration 7800, loss = 0.00351787
I1101 06:30:48.232578 11248 solver.cpp:244]     Train net output #0: loss = 0.00351787 (* 1 = 0.00351787 loss)
I1101 06:30:48.232578 11248 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I1101 06:31:04.025521 11248 solver.cpp:228] Iteration 7900, loss = 0.00423069
I1101 06:31:04.025521 11248 solver.cpp:244]     Train net output #0: loss = 0.00423069 (* 1 = 0.00423069 loss)
I1101 06:31:04.025521 11248 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I1101 06:31:19.592819 11248 solver.cpp:228] Iteration 8000, loss = 0.00674079
I1101 06:31:19.592819 11248 solver.cpp:244]     Train net output #0: loss = 0.00674079 (* 1 = 0.00674079 loss)
I1101 06:31:19.592819 11248 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I1101 06:31:35.202169 11248 solver.cpp:228] Iteration 8100, loss = 0.00703451
I1101 06:31:35.202169 11248 solver.cpp:244]     Train net output #0: loss = 0.00703451 (* 1 = 0.00703451 loss)
I1101 06:31:35.202169 11248 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I1101 06:31:50.721565 11248 solver.cpp:228] Iteration 8200, loss = 0.00295322
I1101 06:31:50.721565 11248 solver.cpp:244]     Train net output #0: loss = 0.00295322 (* 1 = 0.00295322 loss)
I1101 06:31:50.721565 11248 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I1101 06:32:06.278666 11248 solver.cpp:228] Iteration 8300, loss = 0.00400893
I1101 06:32:06.279167 11248 solver.cpp:244]     Train net output #0: loss = 0.00400893 (* 1 = 0.00400893 loss)
I1101 06:32:06.279167 11248 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I1101 06:32:21.828106 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_8400.caffemodel
I1101 06:32:21.980214 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_8400.solverstate
I1101 06:32:22.075283 11248 solver.cpp:337] Iteration 8400, Testing net (#0)
I1101 06:32:26.737601 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9965
I1101 06:32:26.737601 11248 solver.cpp:404]     Test net output #1: loss = 0.0135151 (* 1 = 0.0135151 loss)
I1101 06:32:26.802683 11248 solver.cpp:228] Iteration 8400, loss = 0.00336932
I1101 06:32:26.802683 11248 solver.cpp:244]     Train net output #0: loss = 0.00336932 (* 1 = 0.00336932 loss)
I1101 06:32:26.802683 11248 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
I1101 06:32:42.283828 11248 solver.cpp:228] Iteration 8500, loss = 0.00429029
I1101 06:32:42.283828 11248 solver.cpp:244]     Train net output #0: loss = 0.0042903 (* 1 = 0.0042903 loss)
I1101 06:32:42.283828 11248 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I1101 06:32:57.775435 11248 solver.cpp:228] Iteration 8600, loss = 0.00635237
I1101 06:32:57.775435 11248 solver.cpp:244]     Train net output #0: loss = 0.00635238 (* 1 = 0.00635238 loss)
I1101 06:32:57.775435 11248 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I1101 06:33:13.253500 11248 solver.cpp:228] Iteration 8700, loss = 0.00645268
I1101 06:33:13.253500 11248 solver.cpp:244]     Train net output #0: loss = 0.00645269 (* 1 = 0.00645269 loss)
I1101 06:33:13.253500 11248 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I1101 06:33:28.741549 11248 solver.cpp:228] Iteration 8800, loss = 0.00287499
I1101 06:33:28.741549 11248 solver.cpp:244]     Train net output #0: loss = 0.00287499 (* 1 = 0.00287499 loss)
I1101 06:33:28.741549 11248 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I1101 06:33:44.232674 11248 solver.cpp:228] Iteration 8900, loss = 0.00384687
I1101 06:33:44.232674 11248 solver.cpp:244]     Train net output #0: loss = 0.00384688 (* 1 = 0.00384688 loss)
I1101 06:33:44.232674 11248 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I1101 06:33:59.638682 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_9000.caffemodel
I1101 06:33:59.788789 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_9000.solverstate
I1101 06:33:59.885439 11248 solver.cpp:337] Iteration 9000, Testing net (#0)
I1101 06:34:04.560767 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I1101 06:34:04.560767 11248 solver.cpp:404]     Test net output #1: loss = 0.013558 (* 1 = 0.013558 loss)
I1101 06:34:04.626883 11248 solver.cpp:228] Iteration 9000, loss = 0.00329068
I1101 06:34:04.626883 11248 solver.cpp:244]     Train net output #0: loss = 0.00329068 (* 1 = 0.00329068 loss)
I1101 06:34:04.626883 11248 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I1101 06:34:20.268239 11248 solver.cpp:228] Iteration 9100, loss = 0.00433131
I1101 06:34:20.268239 11248 solver.cpp:244]     Train net output #0: loss = 0.00433132 (* 1 = 0.00433132 loss)
I1101 06:34:20.268239 11248 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I1101 06:34:35.761981 11248 solver.cpp:228] Iteration 9200, loss = 0.00602724
I1101 06:34:35.761981 11248 solver.cpp:244]     Train net output #0: loss = 0.00602725 (* 1 = 0.00602725 loss)
I1101 06:34:35.761981 11248 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I1101 06:34:51.258124 11248 solver.cpp:228] Iteration 9300, loss = 0.00599436
I1101 06:34:51.258124 11248 solver.cpp:244]     Train net output #0: loss = 0.00599437 (* 1 = 0.00599437 loss)
I1101 06:34:51.258124 11248 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I1101 06:35:06.760409 11248 solver.cpp:228] Iteration 9400, loss = 0.0028009
I1101 06:35:06.760409 11248 solver.cpp:244]     Train net output #0: loss = 0.00280091 (* 1 = 0.00280091 loss)
I1101 06:35:06.760409 11248 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
I1101 06:35:22.292244 11248 solver.cpp:228] Iteration 9500, loss = 0.00372639
I1101 06:35:22.292244 11248 solver.cpp:244]     Train net output #0: loss = 0.0037264 (* 1 = 0.0037264 loss)
I1101 06:35:22.292244 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1101 06:35:22.292244 11248 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1101 06:35:37.746417 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_9600.caffemodel
I1101 06:35:37.897527 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_9600.solverstate
I1101 06:35:37.993594 11248 solver.cpp:337] Iteration 9600, Testing net (#0)
I1101 06:35:42.666286 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:35:42.666286 11248 solver.cpp:404]     Test net output #1: loss = 0.0131175 (* 1 = 0.0131175 loss)
I1101 06:35:42.732789 11248 solver.cpp:228] Iteration 9600, loss = 0.0034341
I1101 06:35:42.732789 11248 solver.cpp:244]     Train net output #0: loss = 0.00343411 (* 1 = 0.00343411 loss)
I1101 06:35:42.732789 11248 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1101 06:35:58.245435 11248 solver.cpp:228] Iteration 9700, loss = 0.0048858
I1101 06:35:58.245435 11248 solver.cpp:244]     Train net output #0: loss = 0.00488581 (* 1 = 0.00488581 loss)
I1101 06:35:58.245435 11248 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1101 06:36:13.770776 11248 solver.cpp:228] Iteration 9800, loss = 0.00580535
I1101 06:36:13.770776 11248 solver.cpp:244]     Train net output #0: loss = 0.00580536 (* 1 = 0.00580536 loss)
I1101 06:36:13.770776 11248 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1101 06:36:29.280344 11248 solver.cpp:228] Iteration 9900, loss = 0.00494913
I1101 06:36:29.280344 11248 solver.cpp:244]     Train net output #0: loss = 0.00494914 (* 1 = 0.00494914 loss)
I1101 06:36:29.280344 11248 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1101 06:36:44.827448 11248 solver.cpp:228] Iteration 10000, loss = 0.00250946
I1101 06:36:44.827448 11248 solver.cpp:244]     Train net output #0: loss = 0.00250947 (* 1 = 0.00250947 loss)
I1101 06:36:44.827448 11248 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1101 06:37:00.348203 11248 solver.cpp:228] Iteration 10100, loss = 0.00340457
I1101 06:37:00.348203 11248 solver.cpp:244]     Train net output #0: loss = 0.00340458 (* 1 = 0.00340458 loss)
I1101 06:37:00.348203 11248 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1101 06:37:15.800952 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_10200.caffemodel
I1101 06:37:15.954561 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_10200.solverstate
I1101 06:37:16.051630 11248 solver.cpp:337] Iteration 10200, Testing net (#0)
I1101 06:37:20.726161 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:37:20.726161 11248 solver.cpp:404]     Test net output #1: loss = 0.0131412 (* 1 = 0.0131412 loss)
I1101 06:37:20.791378 11248 solver.cpp:228] Iteration 10200, loss = 0.00328006
I1101 06:37:20.791378 11248 solver.cpp:244]     Train net output #0: loss = 0.00328007 (* 1 = 0.00328007 loss)
I1101 06:37:20.791378 11248 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1101 06:37:36.306159 11248 solver.cpp:228] Iteration 10300, loss = 0.00438939
I1101 06:37:36.306159 11248 solver.cpp:244]     Train net output #0: loss = 0.0043894 (* 1 = 0.0043894 loss)
I1101 06:37:36.306159 11248 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1101 06:37:51.822942 11248 solver.cpp:228] Iteration 10400, loss = 0.00555007
I1101 06:37:51.823443 11248 solver.cpp:244]     Train net output #0: loss = 0.00555008 (* 1 = 0.00555008 loss)
I1101 06:37:51.823443 11248 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I1101 06:38:07.318634 11248 solver.cpp:228] Iteration 10500, loss = 0.00479061
I1101 06:38:07.318634 11248 solver.cpp:244]     Train net output #0: loss = 0.00479062 (* 1 = 0.00479062 loss)
I1101 06:38:07.318634 11248 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1101 06:38:22.824216 11248 solver.cpp:228] Iteration 10600, loss = 0.00252474
I1101 06:38:22.824216 11248 solver.cpp:244]     Train net output #0: loss = 0.00252475 (* 1 = 0.00252475 loss)
I1101 06:38:22.824216 11248 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1101 06:38:38.331040 11248 solver.cpp:228] Iteration 10700, loss = 0.00337296
I1101 06:38:38.331040 11248 solver.cpp:244]     Train net output #0: loss = 0.00337297 (* 1 = 0.00337297 loss)
I1101 06:38:38.331040 11248 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1101 06:38:53.778697 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_10800.caffemodel
I1101 06:38:53.927803 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_10800.solverstate
I1101 06:38:54.020882 11248 solver.cpp:337] Iteration 10800, Testing net (#0)
I1101 06:38:58.684165 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 06:38:58.684165 11248 solver.cpp:404]     Test net output #1: loss = 0.0131583 (* 1 = 0.0131583 loss)
I1101 06:38:58.750679 11248 solver.cpp:228] Iteration 10800, loss = 0.00326715
I1101 06:38:58.750679 11248 solver.cpp:244]     Train net output #0: loss = 0.00326716 (* 1 = 0.00326716 loss)
I1101 06:38:58.750679 11248 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1101 06:39:14.273509 11248 solver.cpp:228] Iteration 10900, loss = 0.00429674
I1101 06:39:14.273509 11248 solver.cpp:244]     Train net output #0: loss = 0.00429675 (* 1 = 0.00429675 loss)
I1101 06:39:14.273509 11248 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1101 06:39:29.794739 11248 solver.cpp:228] Iteration 11000, loss = 0.00547738
I1101 06:39:29.794739 11248 solver.cpp:244]     Train net output #0: loss = 0.00547739 (* 1 = 0.00547739 loss)
I1101 06:39:29.794739 11248 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1101 06:39:45.286839 11248 solver.cpp:228] Iteration 11100, loss = 0.00476206
I1101 06:39:45.286839 11248 solver.cpp:244]     Train net output #0: loss = 0.00476207 (* 1 = 0.00476207 loss)
I1101 06:39:45.286839 11248 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1101 06:40:00.803097 11248 solver.cpp:228] Iteration 11200, loss = 0.0025267
I1101 06:40:00.803097 11248 solver.cpp:244]     Train net output #0: loss = 0.0025267 (* 1 = 0.0025267 loss)
I1101 06:40:00.803097 11248 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1101 06:40:16.298957 11248 solver.cpp:228] Iteration 11300, loss = 0.00337358
I1101 06:40:16.298957 11248 solver.cpp:244]     Train net output #0: loss = 0.00337358 (* 1 = 0.00337358 loss)
I1101 06:40:16.298957 11248 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1101 06:40:31.770589 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_11400.caffemodel
I1101 06:40:31.920696 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_11400.solverstate
I1101 06:40:32.013762 11248 solver.cpp:337] Iteration 11400, Testing net (#0)
I1101 06:40:36.679502 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:40:36.680003 11248 solver.cpp:404]     Test net output #1: loss = 0.0131696 (* 1 = 0.0131696 loss)
I1101 06:40:36.744971 11248 solver.cpp:228] Iteration 11400, loss = 0.00326795
I1101 06:40:36.745471 11248 solver.cpp:244]     Train net output #0: loss = 0.00326795 (* 1 = 0.00326795 loss)
I1101 06:40:36.745471 11248 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I1101 06:40:52.249406 11248 solver.cpp:228] Iteration 11500, loss = 0.00426123
I1101 06:40:52.249406 11248 solver.cpp:244]     Train net output #0: loss = 0.00426123 (* 1 = 0.00426123 loss)
I1101 06:40:52.249406 11248 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I1101 06:41:07.738864 11248 solver.cpp:228] Iteration 11600, loss = 0.00543626
I1101 06:41:07.738864 11248 solver.cpp:244]     Train net output #0: loss = 0.00543627 (* 1 = 0.00543627 loss)
I1101 06:41:07.738864 11248 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I1101 06:41:23.215695 11248 solver.cpp:228] Iteration 11700, loss = 0.00474843
I1101 06:41:23.215695 11248 solver.cpp:244]     Train net output #0: loss = 0.00474843 (* 1 = 0.00474843 loss)
I1101 06:41:23.215695 11248 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I1101 06:41:38.713078 11248 solver.cpp:228] Iteration 11800, loss = 0.00252482
I1101 06:41:38.713078 11248 solver.cpp:244]     Train net output #0: loss = 0.00252482 (* 1 = 0.00252482 loss)
I1101 06:41:38.713078 11248 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I1101 06:41:54.264240 11248 solver.cpp:228] Iteration 11900, loss = 0.00337381
I1101 06:41:54.264240 11248 solver.cpp:244]     Train net output #0: loss = 0.00337382 (* 1 = 0.00337382 loss)
I1101 06:41:54.264240 11248 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I1101 06:42:09.712481 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_12000.caffemodel
I1101 06:42:09.864089 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_12000.solverstate
I1101 06:42:09.957655 11248 solver.cpp:337] Iteration 12000, Testing net (#0)
I1101 06:42:14.636160 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:42:14.636662 11248 solver.cpp:404]     Test net output #1: loss = 0.0131784 (* 1 = 0.0131784 loss)
I1101 06:42:14.701732 11248 solver.cpp:228] Iteration 12000, loss = 0.0032674
I1101 06:42:14.701732 11248 solver.cpp:244]     Train net output #0: loss = 0.00326742 (* 1 = 0.00326742 loss)
I1101 06:42:14.701732 11248 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I1101 06:42:30.255537 11248 solver.cpp:228] Iteration 12100, loss = 0.00424036
I1101 06:42:30.255537 11248 solver.cpp:244]     Train net output #0: loss = 0.00424038 (* 1 = 0.00424038 loss)
I1101 06:42:30.255537 11248 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I1101 06:42:45.780427 11248 solver.cpp:228] Iteration 12200, loss = 0.00540449
I1101 06:42:45.780427 11248 solver.cpp:244]     Train net output #0: loss = 0.0054045 (* 1 = 0.0054045 loss)
I1101 06:42:45.780427 11248 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I1101 06:43:01.312718 11248 solver.cpp:228] Iteration 12300, loss = 0.00473506
I1101 06:43:01.312718 11248 solver.cpp:244]     Train net output #0: loss = 0.00473508 (* 1 = 0.00473508 loss)
I1101 06:43:01.312718 11248 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I1101 06:43:16.827232 11248 solver.cpp:228] Iteration 12400, loss = 0.00252256
I1101 06:43:16.827232 11248 solver.cpp:244]     Train net output #0: loss = 0.00252258 (* 1 = 0.00252258 loss)
I1101 06:43:16.827232 11248 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I1101 06:43:32.654870 11248 solver.cpp:228] Iteration 12500, loss = 0.00337335
I1101 06:43:32.654870 11248 solver.cpp:244]     Train net output #0: loss = 0.00337336 (* 1 = 0.00337336 loss)
I1101 06:43:32.654870 11248 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I1101 06:43:48.175811 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_12600.caffemodel
I1101 06:43:48.325917 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_12600.solverstate
I1101 06:43:48.417980 11248 solver.cpp:337] Iteration 12600, Testing net (#0)
I1101 06:43:53.093549 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:43:53.094549 11248 solver.cpp:404]     Test net output #1: loss = 0.013185 (* 1 = 0.013185 loss)
I1101 06:43:53.159596 11248 solver.cpp:228] Iteration 12600, loss = 0.00326613
I1101 06:43:53.159596 11248 solver.cpp:244]     Train net output #0: loss = 0.00326614 (* 1 = 0.00326614 loss)
I1101 06:43:53.159596 11248 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I1101 06:44:08.653012 11248 solver.cpp:228] Iteration 12700, loss = 0.00422179
I1101 06:44:08.653012 11248 solver.cpp:244]     Train net output #0: loss = 0.0042218 (* 1 = 0.0042218 loss)
I1101 06:44:08.653012 11248 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I1101 06:44:24.160673 11248 solver.cpp:228] Iteration 12800, loss = 0.00537802
I1101 06:44:24.160673 11248 solver.cpp:244]     Train net output #0: loss = 0.00537803 (* 1 = 0.00537803 loss)
I1101 06:44:24.160673 11248 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I1101 06:44:39.673297 11248 solver.cpp:228] Iteration 12900, loss = 0.00472471
I1101 06:44:39.673297 11248 solver.cpp:244]     Train net output #0: loss = 0.00472473 (* 1 = 0.00472473 loss)
I1101 06:44:39.673297 11248 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I1101 06:44:55.193243 11248 solver.cpp:228] Iteration 13000, loss = 0.00252147
I1101 06:44:55.193243 11248 solver.cpp:244]     Train net output #0: loss = 0.00252148 (* 1 = 0.00252148 loss)
I1101 06:44:55.193243 11248 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I1101 06:45:10.727612 11248 solver.cpp:228] Iteration 13100, loss = 0.00337203
I1101 06:45:10.727612 11248 solver.cpp:244]     Train net output #0: loss = 0.00337204 (* 1 = 0.00337204 loss)
I1101 06:45:10.727612 11248 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I1101 06:45:26.242244 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_13200.caffemodel
I1101 06:45:26.393352 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_13200.solverstate
I1101 06:45:26.485417 11248 solver.cpp:337] Iteration 13200, Testing net (#0)
I1101 06:45:31.179491 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:45:31.179491 11248 solver.cpp:404]     Test net output #1: loss = 0.0131893 (* 1 = 0.0131893 loss)
I1101 06:45:31.244539 11248 solver.cpp:228] Iteration 13200, loss = 0.0032648
I1101 06:45:31.244539 11248 solver.cpp:244]     Train net output #0: loss = 0.00326481 (* 1 = 0.00326481 loss)
I1101 06:45:31.244539 11248 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I1101 06:45:46.893697 11248 solver.cpp:228] Iteration 13300, loss = 0.00420402
I1101 06:45:46.893697 11248 solver.cpp:244]     Train net output #0: loss = 0.00420403 (* 1 = 0.00420403 loss)
I1101 06:45:46.893697 11248 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I1101 06:46:02.384099 11248 solver.cpp:228] Iteration 13400, loss = 0.005352
I1101 06:46:02.384099 11248 solver.cpp:244]     Train net output #0: loss = 0.00535201 (* 1 = 0.00535201 loss)
I1101 06:46:02.384099 11248 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I1101 06:46:17.872841 11248 solver.cpp:228] Iteration 13500, loss = 0.00471481
I1101 06:46:17.872841 11248 solver.cpp:244]     Train net output #0: loss = 0.00471482 (* 1 = 0.00471482 loss)
I1101 06:46:17.872841 11248 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I1101 06:46:33.357290 11248 solver.cpp:228] Iteration 13600, loss = 0.00252003
I1101 06:46:33.357290 11248 solver.cpp:244]     Train net output #0: loss = 0.00252004 (* 1 = 0.00252004 loss)
I1101 06:46:33.357290 11248 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I1101 06:46:48.845399 11248 solver.cpp:228] Iteration 13700, loss = 0.0033691
I1101 06:46:48.845399 11248 solver.cpp:244]     Train net output #0: loss = 0.00336911 (* 1 = 0.00336911 loss)
I1101 06:46:48.845399 11248 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I1101 06:47:04.261917 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_13800.caffemodel
I1101 06:47:04.413024 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_13800.solverstate
I1101 06:47:04.508092 11248 solver.cpp:337] Iteration 13800, Testing net (#0)
I1101 06:47:09.173413 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:47:09.174413 11248 solver.cpp:404]     Test net output #1: loss = 0.0131944 (* 1 = 0.0131944 loss)
I1101 06:47:09.240463 11248 solver.cpp:228] Iteration 13800, loss = 0.00326247
I1101 06:47:09.240463 11248 solver.cpp:244]     Train net output #0: loss = 0.00326248 (* 1 = 0.00326248 loss)
I1101 06:47:09.240463 11248 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I1101 06:47:24.736490 11248 solver.cpp:228] Iteration 13900, loss = 0.00419049
I1101 06:47:24.736490 11248 solver.cpp:244]     Train net output #0: loss = 0.0041905 (* 1 = 0.0041905 loss)
I1101 06:47:24.736490 11248 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I1101 06:47:40.227519 11248 solver.cpp:228] Iteration 14000, loss = 0.00532677
I1101 06:47:40.227519 11248 solver.cpp:244]     Train net output #0: loss = 0.00532678 (* 1 = 0.00532678 loss)
I1101 06:47:40.227519 11248 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I1101 06:47:55.716583 11248 solver.cpp:228] Iteration 14100, loss = 0.00470321
I1101 06:47:55.716583 11248 solver.cpp:244]     Train net output #0: loss = 0.00470322 (* 1 = 0.00470322 loss)
I1101 06:47:55.716583 11248 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I1101 06:48:11.212357 11248 solver.cpp:228] Iteration 14200, loss = 0.0025189
I1101 06:48:11.212357 11248 solver.cpp:244]     Train net output #0: loss = 0.0025189 (* 1 = 0.0025189 loss)
I1101 06:48:11.212357 11248 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I1101 06:48:26.696408 11248 solver.cpp:228] Iteration 14300, loss = 0.00336768
I1101 06:48:26.696408 11248 solver.cpp:244]     Train net output #0: loss = 0.00336769 (* 1 = 0.00336769 loss)
I1101 06:48:26.696408 11248 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I1101 06:48:42.130154 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_14400.caffemodel
I1101 06:48:42.295274 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_14400.solverstate
I1101 06:48:42.388339 11248 solver.cpp:337] Iteration 14400, Testing net (#0)
I1101 06:48:47.040650 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:48:47.040650 11248 solver.cpp:404]     Test net output #1: loss = 0.0131986 (* 1 = 0.0131986 loss)
I1101 06:48:47.105710 11248 solver.cpp:228] Iteration 14400, loss = 0.00325761
I1101 06:48:47.105710 11248 solver.cpp:244]     Train net output #0: loss = 0.00325761 (* 1 = 0.00325761 loss)
I1101 06:48:47.105710 11248 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I1101 06:49:02.581712 11248 solver.cpp:228] Iteration 14500, loss = 0.00417383
I1101 06:49:02.581712 11248 solver.cpp:244]     Train net output #0: loss = 0.00417384 (* 1 = 0.00417384 loss)
I1101 06:49:02.581712 11248 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I1101 06:49:18.057766 11248 solver.cpp:228] Iteration 14600, loss = 0.0052999
I1101 06:49:18.057766 11248 solver.cpp:244]     Train net output #0: loss = 0.00529991 (* 1 = 0.00529991 loss)
I1101 06:49:18.057766 11248 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I1101 06:49:33.550793 11248 solver.cpp:228] Iteration 14700, loss = 0.00468935
I1101 06:49:33.550793 11248 solver.cpp:244]     Train net output #0: loss = 0.00468936 (* 1 = 0.00468936 loss)
I1101 06:49:33.550793 11248 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I1101 06:49:49.033565 11248 solver.cpp:228] Iteration 14800, loss = 0.00251737
I1101 06:49:49.033565 11248 solver.cpp:244]     Train net output #0: loss = 0.00251738 (* 1 = 0.00251738 loss)
I1101 06:49:49.033565 11248 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I1101 06:50:04.521885 11248 solver.cpp:228] Iteration 14900, loss = 0.00336597
I1101 06:50:04.521885 11248 solver.cpp:244]     Train net output #0: loss = 0.00336598 (* 1 = 0.00336598 loss)
I1101 06:50:04.521885 11248 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I1101 06:50:19.929972 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_15000.caffemodel
I1101 06:50:20.080080 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_15000.solverstate
I1101 06:50:20.183099 11248 solver.cpp:337] Iteration 15000, Testing net (#0)
I1101 06:50:24.846418 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:50:24.846418 11248 solver.cpp:404]     Test net output #1: loss = 0.013202 (* 1 = 0.013202 loss)
I1101 06:50:24.912500 11248 solver.cpp:228] Iteration 15000, loss = 0.00325393
I1101 06:50:24.912500 11248 solver.cpp:244]     Train net output #0: loss = 0.00325394 (* 1 = 0.00325394 loss)
I1101 06:50:24.912500 11248 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I1101 06:50:40.398576 11248 solver.cpp:228] Iteration 15100, loss = 0.00415898
I1101 06:50:40.398576 11248 solver.cpp:244]     Train net output #0: loss = 0.00415898 (* 1 = 0.00415898 loss)
I1101 06:50:40.398576 11248 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I1101 06:50:55.884565 11248 solver.cpp:228] Iteration 15200, loss = 0.00527644
I1101 06:50:55.884565 11248 solver.cpp:244]     Train net output #0: loss = 0.00527645 (* 1 = 0.00527645 loss)
I1101 06:50:55.884565 11248 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I1101 06:51:11.360615 11248 solver.cpp:228] Iteration 15300, loss = 0.0046793
I1101 06:51:11.360615 11248 solver.cpp:244]     Train net output #0: loss = 0.0046793 (* 1 = 0.0046793 loss)
I1101 06:51:11.360615 11248 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I1101 06:51:26.837630 11248 solver.cpp:228] Iteration 15400, loss = 0.00251737
I1101 06:51:26.837630 11248 solver.cpp:244]     Train net output #0: loss = 0.00251737 (* 1 = 0.00251737 loss)
I1101 06:51:26.837630 11248 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I1101 06:51:42.314795 11248 solver.cpp:228] Iteration 15500, loss = 0.00336219
I1101 06:51:42.314795 11248 solver.cpp:244]     Train net output #0: loss = 0.00336219 (* 1 = 0.00336219 loss)
I1101 06:51:42.314795 11248 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I1101 06:51:57.735170 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_15600.caffemodel
I1101 06:51:57.887280 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_15600.solverstate
I1101 06:51:57.980345 11248 solver.cpp:337] Iteration 15600, Testing net (#0)
I1101 06:52:02.643237 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:52:02.643237 11248 solver.cpp:404]     Test net output #1: loss = 0.0132036 (* 1 = 0.0132036 loss)
I1101 06:52:02.708297 11248 solver.cpp:228] Iteration 15600, loss = 0.00324887
I1101 06:52:02.708297 11248 solver.cpp:244]     Train net output #0: loss = 0.00324888 (* 1 = 0.00324888 loss)
I1101 06:52:02.708297 11248 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I1101 06:52:18.190354 11248 solver.cpp:228] Iteration 15700, loss = 0.00414603
I1101 06:52:18.190354 11248 solver.cpp:244]     Train net output #0: loss = 0.00414603 (* 1 = 0.00414603 loss)
I1101 06:52:18.190354 11248 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I1101 06:52:33.674568 11248 solver.cpp:228] Iteration 15800, loss = 0.0052478
I1101 06:52:33.674568 11248 solver.cpp:244]     Train net output #0: loss = 0.0052478 (* 1 = 0.0052478 loss)
I1101 06:52:33.674568 11248 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I1101 06:52:49.160459 11248 solver.cpp:228] Iteration 15900, loss = 0.00467396
I1101 06:52:49.160459 11248 solver.cpp:244]     Train net output #0: loss = 0.00467397 (* 1 = 0.00467397 loss)
I1101 06:52:49.160459 11248 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I1101 06:53:04.645159 11248 solver.cpp:228] Iteration 16000, loss = 0.00251594
I1101 06:53:04.645159 11248 solver.cpp:244]     Train net output #0: loss = 0.00251595 (* 1 = 0.00251595 loss)
I1101 06:53:04.645159 11248 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I1101 06:53:20.131283 11248 solver.cpp:228] Iteration 16100, loss = 0.00336259
I1101 06:53:20.131283 11248 solver.cpp:244]     Train net output #0: loss = 0.0033626 (* 1 = 0.0033626 loss)
I1101 06:53:20.131283 11248 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I1101 06:53:35.553406 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_16200.caffemodel
I1101 06:53:35.704522 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_16200.solverstate
I1101 06:53:35.797580 11248 solver.cpp:337] Iteration 16200, Testing net (#0)
I1101 06:53:40.460899 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:53:40.460899 11248 solver.cpp:404]     Test net output #1: loss = 0.0132065 (* 1 = 0.0132065 loss)
I1101 06:53:40.526948 11248 solver.cpp:228] Iteration 16200, loss = 0.00324594
I1101 06:53:40.526948 11248 solver.cpp:244]     Train net output #0: loss = 0.00324595 (* 1 = 0.00324595 loss)
I1101 06:53:40.526948 11248 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I1101 06:53:55.998996 11248 solver.cpp:228] Iteration 16300, loss = 0.00412866
I1101 06:53:55.998996 11248 solver.cpp:244]     Train net output #0: loss = 0.00412867 (* 1 = 0.00412867 loss)
I1101 06:53:55.998996 11248 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I1101 06:54:11.511098 11248 solver.cpp:228] Iteration 16400, loss = 0.00522078
I1101 06:54:11.511098 11248 solver.cpp:244]     Train net output #0: loss = 0.00522078 (* 1 = 0.00522078 loss)
I1101 06:54:11.511098 11248 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I1101 06:54:26.986182 11248 solver.cpp:228] Iteration 16500, loss = 0.00466483
I1101 06:54:26.986182 11248 solver.cpp:244]     Train net output #0: loss = 0.00466484 (* 1 = 0.00466484 loss)
I1101 06:54:26.986182 11248 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I1101 06:54:42.476917 11248 solver.cpp:228] Iteration 16600, loss = 0.00251608
I1101 06:54:42.476917 11248 solver.cpp:244]     Train net output #0: loss = 0.00251609 (* 1 = 0.00251609 loss)
I1101 06:54:42.476917 11248 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I1101 06:54:57.963011 11248 solver.cpp:228] Iteration 16700, loss = 0.00335998
I1101 06:54:57.963011 11248 solver.cpp:244]     Train net output #0: loss = 0.00335999 (* 1 = 0.00335999 loss)
I1101 06:54:57.963011 11248 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I1101 06:55:13.384373 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_16800.caffemodel
I1101 06:55:13.538483 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_16800.solverstate
I1101 06:55:13.631549 11248 solver.cpp:337] Iteration 16800, Testing net (#0)
I1101 06:55:18.294868 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:55:18.294868 11248 solver.cpp:404]     Test net output #1: loss = 0.0132096 (* 1 = 0.0132096 loss)
I1101 06:55:18.360918 11248 solver.cpp:228] Iteration 16800, loss = 0.00324072
I1101 06:55:18.360918 11248 solver.cpp:244]     Train net output #0: loss = 0.00324072 (* 1 = 0.00324072 loss)
I1101 06:55:18.360918 11248 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I1101 06:55:33.845082 11248 solver.cpp:228] Iteration 16900, loss = 0.00411521
I1101 06:55:33.845082 11248 solver.cpp:244]     Train net output #0: loss = 0.00411522 (* 1 = 0.00411522 loss)
I1101 06:55:33.845082 11248 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I1101 06:55:49.381657 11248 solver.cpp:228] Iteration 17000, loss = 0.00519301
I1101 06:55:49.381657 11248 solver.cpp:244]     Train net output #0: loss = 0.00519302 (* 1 = 0.00519302 loss)
I1101 06:55:49.381657 11248 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I1101 06:56:04.835285 11248 solver.cpp:228] Iteration 17100, loss = 0.0046616
I1101 06:56:04.835285 11248 solver.cpp:244]     Train net output #0: loss = 0.00466161 (* 1 = 0.00466161 loss)
I1101 06:56:04.835285 11248 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I1101 06:56:20.288353 11248 solver.cpp:228] Iteration 17200, loss = 0.00251498
I1101 06:56:20.288353 11248 solver.cpp:244]     Train net output #0: loss = 0.00251499 (* 1 = 0.00251499 loss)
I1101 06:56:20.288353 11248 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I1101 06:56:35.754362 11248 solver.cpp:228] Iteration 17300, loss = 0.00335829
I1101 06:56:35.754362 11248 solver.cpp:244]     Train net output #0: loss = 0.0033583 (* 1 = 0.0033583 loss)
I1101 06:56:35.754362 11248 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I1101 06:56:51.141788 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_17400.caffemodel
I1101 06:56:51.297900 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_17400.solverstate
I1101 06:56:51.398329 11248 solver.cpp:337] Iteration 17400, Testing net (#0)
I1101 06:56:56.051654 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:56:56.051654 11248 solver.cpp:404]     Test net output #1: loss = 0.0132109 (* 1 = 0.0132109 loss)
I1101 06:56:56.117753 11248 solver.cpp:228] Iteration 17400, loss = 0.00323774
I1101 06:56:56.117753 11248 solver.cpp:244]     Train net output #0: loss = 0.00323774 (* 1 = 0.00323774 loss)
I1101 06:56:56.117753 11248 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I1101 06:57:11.569751 11248 solver.cpp:228] Iteration 17500, loss = 0.00410065
I1101 06:57:11.569751 11248 solver.cpp:244]     Train net output #0: loss = 0.00410065 (* 1 = 0.00410065 loss)
I1101 06:57:11.569751 11248 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I1101 06:57:27.023717 11248 solver.cpp:228] Iteration 17600, loss = 0.00517175
I1101 06:57:27.023717 11248 solver.cpp:244]     Train net output #0: loss = 0.00517175 (* 1 = 0.00517175 loss)
I1101 06:57:27.023717 11248 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I1101 06:57:42.487491 11248 solver.cpp:228] Iteration 17700, loss = 0.00465447
I1101 06:57:42.487491 11248 solver.cpp:244]     Train net output #0: loss = 0.00465448 (* 1 = 0.00465448 loss)
I1101 06:57:42.487491 11248 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I1101 06:57:57.941536 11248 solver.cpp:228] Iteration 17800, loss = 0.00251489
I1101 06:57:57.941536 11248 solver.cpp:244]     Train net output #0: loss = 0.0025149 (* 1 = 0.0025149 loss)
I1101 06:57:57.941536 11248 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I1101 06:58:13.406544 11248 solver.cpp:228] Iteration 17900, loss = 0.0033567
I1101 06:58:13.406544 11248 solver.cpp:244]     Train net output #0: loss = 0.0033567 (* 1 = 0.0033567 loss)
I1101 06:58:13.406544 11248 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I1101 06:58:28.801600 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_18000.caffemodel
I1101 06:58:28.950709 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_18000.solverstate
I1101 06:58:29.043773 11248 solver.cpp:337] Iteration 18000, Testing net (#0)
I1101 06:58:33.687165 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 06:58:33.687165 11248 solver.cpp:404]     Test net output #1: loss = 0.0132133 (* 1 = 0.0132133 loss)
I1101 06:58:33.752238 11248 solver.cpp:228] Iteration 18000, loss = 0.00323207
I1101 06:58:33.752238 11248 solver.cpp:244]     Train net output #0: loss = 0.00323207 (* 1 = 0.00323207 loss)
I1101 06:58:33.752238 11248 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I1101 06:58:49.206260 11248 solver.cpp:228] Iteration 18100, loss = 0.00408623
I1101 06:58:49.206260 11248 solver.cpp:244]     Train net output #0: loss = 0.00408624 (* 1 = 0.00408624 loss)
I1101 06:58:49.206260 11248 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I1101 06:59:04.669322 11248 solver.cpp:228] Iteration 18200, loss = 0.00514577
I1101 06:59:04.669322 11248 solver.cpp:244]     Train net output #0: loss = 0.00514577 (* 1 = 0.00514577 loss)
I1101 06:59:04.669322 11248 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I1101 06:59:20.135097 11248 solver.cpp:228] Iteration 18300, loss = 0.00465073
I1101 06:59:20.135097 11248 solver.cpp:244]     Train net output #0: loss = 0.00465073 (* 1 = 0.00465073 loss)
I1101 06:59:20.135097 11248 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I1101 06:59:35.592958 11248 solver.cpp:228] Iteration 18400, loss = 0.00251521
I1101 06:59:35.592958 11248 solver.cpp:244]     Train net output #0: loss = 0.00251521 (* 1 = 0.00251521 loss)
I1101 06:59:35.592958 11248 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I1101 06:59:51.058437 11248 solver.cpp:228] Iteration 18500, loss = 0.00335293
I1101 06:59:51.058437 11248 solver.cpp:244]     Train net output #0: loss = 0.00335293 (* 1 = 0.00335293 loss)
I1101 06:59:51.058437 11248 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I1101 07:00:06.703199 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_18600.caffemodel
I1101 07:00:06.855309 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_18600.solverstate
I1101 07:00:06.949374 11248 solver.cpp:337] Iteration 18600, Testing net (#0)
I1101 07:00:11.604688 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:00:11.604688 11248 solver.cpp:404]     Test net output #1: loss = 0.013218 (* 1 = 0.013218 loss)
I1101 07:00:11.669737 11248 solver.cpp:228] Iteration 18600, loss = 0.00322675
I1101 07:00:11.669737 11248 solver.cpp:244]     Train net output #0: loss = 0.00322675 (* 1 = 0.00322675 loss)
I1101 07:00:11.669737 11248 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I1101 07:00:27.215373 11248 solver.cpp:228] Iteration 18700, loss = 0.00407052
I1101 07:00:27.215373 11248 solver.cpp:244]     Train net output #0: loss = 0.00407052 (* 1 = 0.00407052 loss)
I1101 07:00:27.215373 11248 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I1101 07:00:42.686419 11248 solver.cpp:228] Iteration 18800, loss = 0.00512412
I1101 07:00:42.686419 11248 solver.cpp:244]     Train net output #0: loss = 0.00512412 (* 1 = 0.00512412 loss)
I1101 07:00:42.686419 11248 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I1101 07:00:58.153378 11248 solver.cpp:228] Iteration 18900, loss = 0.00463937
I1101 07:00:58.153378 11248 solver.cpp:244]     Train net output #0: loss = 0.00463937 (* 1 = 0.00463937 loss)
I1101 07:00:58.153378 11248 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I1101 07:01:13.939581 11248 solver.cpp:228] Iteration 19000, loss = 0.00251475
I1101 07:01:13.939581 11248 solver.cpp:244]     Train net output #0: loss = 0.00251475 (* 1 = 0.00251475 loss)
I1101 07:01:13.939581 11248 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I1101 07:01:29.574539 11248 solver.cpp:228] Iteration 19100, loss = 0.00334954
I1101 07:01:29.574539 11248 solver.cpp:244]     Train net output #0: loss = 0.00334954 (* 1 = 0.00334954 loss)
I1101 07:01:29.574539 11248 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I1101 07:01:45.099002 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_19200.caffemodel
I1101 07:01:45.286638 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_19200.solverstate
I1101 07:01:45.393713 11248 solver.cpp:337] Iteration 19200, Testing net (#0)
I1101 07:01:50.250859 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:01:50.250859 11248 solver.cpp:404]     Test net output #1: loss = 0.0132173 (* 1 = 0.0132173 loss)
I1101 07:01:50.317409 11248 solver.cpp:228] Iteration 19200, loss = 0.00322258
I1101 07:01:50.317409 11248 solver.cpp:244]     Train net output #0: loss = 0.00322259 (* 1 = 0.00322259 loss)
I1101 07:01:50.317409 11248 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I1101 07:02:05.987844 11248 solver.cpp:228] Iteration 19300, loss = 0.00405595
I1101 07:02:05.987844 11248 solver.cpp:244]     Train net output #0: loss = 0.00405595 (* 1 = 0.00405595 loss)
I1101 07:02:05.987844 11248 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I1101 07:02:23.203610 11248 solver.cpp:228] Iteration 19400, loss = 0.00510277
I1101 07:02:23.204110 11248 solver.cpp:244]     Train net output #0: loss = 0.00510278 (* 1 = 0.00510278 loss)
I1101 07:02:23.204110 11248 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I1101 07:02:40.209070 11248 solver.cpp:228] Iteration 19500, loss = 0.00463412
I1101 07:02:40.209070 11248 solver.cpp:244]     Train net output #0: loss = 0.00463413 (* 1 = 0.00463413 loss)
I1101 07:02:40.209070 11248 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I1101 07:02:57.123925 11248 solver.cpp:228] Iteration 19600, loss = 0.00251525
I1101 07:02:57.123925 11248 solver.cpp:244]     Train net output #0: loss = 0.00251525 (* 1 = 0.00251525 loss)
I1101 07:02:57.123925 11248 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I1101 07:03:14.014153 11248 solver.cpp:228] Iteration 19700, loss = 0.00334531
I1101 07:03:14.014153 11248 solver.cpp:244]     Train net output #0: loss = 0.00334531 (* 1 = 0.00334531 loss)
I1101 07:03:14.014153 11248 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I1101 07:03:30.845113 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_19800.caffemodel
I1101 07:03:31.010232 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_19800.solverstate
I1101 07:03:31.114166 11248 solver.cpp:337] Iteration 19800, Testing net (#0)
I1101 07:03:36.189723 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:03:36.189723 11248 solver.cpp:404]     Test net output #1: loss = 0.0132178 (* 1 = 0.0132178 loss)
I1101 07:03:36.262275 11248 solver.cpp:228] Iteration 19800, loss = 0.00321874
I1101 07:03:36.263777 11248 solver.cpp:244]     Train net output #0: loss = 0.00321875 (* 1 = 0.00321875 loss)
I1101 07:03:36.264276 11248 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I1101 07:03:53.213387 11248 solver.cpp:228] Iteration 19900, loss = 0.0040436
I1101 07:03:53.213387 11248 solver.cpp:244]     Train net output #0: loss = 0.00404361 (* 1 = 0.00404361 loss)
I1101 07:03:53.213387 11248 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I1101 07:04:10.091992 11248 solver.cpp:228] Iteration 20000, loss = 0.0050827
I1101 07:04:10.091992 11248 solver.cpp:244]     Train net output #0: loss = 0.00508271 (* 1 = 0.00508271 loss)
I1101 07:04:10.091992 11248 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I1101 07:04:26.956336 11248 solver.cpp:228] Iteration 20100, loss = 0.00462532
I1101 07:04:26.956336 11248 solver.cpp:244]     Train net output #0: loss = 0.00462532 (* 1 = 0.00462532 loss)
I1101 07:04:26.956336 11248 sgd_solver.cpp:106] Iteration 20100, lr = 0.001
I1101 07:04:43.829071 11248 solver.cpp:228] Iteration 20200, loss = 0.00251445
I1101 07:04:43.829071 11248 solver.cpp:244]     Train net output #0: loss = 0.00251445 (* 1 = 0.00251445 loss)
I1101 07:04:43.829071 11248 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I1101 07:05:00.792111 11248 solver.cpp:228] Iteration 20300, loss = 0.00334238
I1101 07:05:00.792111 11248 solver.cpp:244]     Train net output #0: loss = 0.00334238 (* 1 = 0.00334238 loss)
I1101 07:05:00.792111 11248 sgd_solver.cpp:106] Iteration 20300, lr = 0.001
I1101 07:05:17.621405 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_20400.caffemodel
I1101 07:05:17.783023 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_20400.solverstate
I1101 07:05:17.883095 11248 solver.cpp:337] Iteration 20400, Testing net (#0)
I1101 07:05:22.928815 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:05:22.928815 11248 solver.cpp:404]     Test net output #1: loss = 0.0132241 (* 1 = 0.0132241 loss)
I1101 07:05:22.999977 11248 solver.cpp:228] Iteration 20400, loss = 0.00321518
I1101 07:05:22.999977 11248 solver.cpp:244]     Train net output #0: loss = 0.00321518 (* 1 = 0.00321518 loss)
I1101 07:05:22.999977 11248 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I1101 07:05:39.848799 11248 solver.cpp:228] Iteration 20500, loss = 0.0040297
I1101 07:05:39.848799 11248 solver.cpp:244]     Train net output #0: loss = 0.0040297 (* 1 = 0.0040297 loss)
I1101 07:05:39.848799 11248 sgd_solver.cpp:106] Iteration 20500, lr = 0.001
I1101 07:05:56.727723 11248 solver.cpp:228] Iteration 20600, loss = 0.00506138
I1101 07:05:56.727723 11248 solver.cpp:244]     Train net output #0: loss = 0.00506138 (* 1 = 0.00506138 loss)
I1101 07:05:56.727723 11248 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I1101 07:06:13.701977 11248 solver.cpp:228] Iteration 20700, loss = 0.00461574
I1101 07:06:13.701977 11248 solver.cpp:244]     Train net output #0: loss = 0.00461574 (* 1 = 0.00461574 loss)
I1101 07:06:13.701977 11248 sgd_solver.cpp:106] Iteration 20700, lr = 0.001
I1101 07:06:30.548732 11248 solver.cpp:228] Iteration 20800, loss = 0.00251492
I1101 07:06:30.548732 11248 solver.cpp:244]     Train net output #0: loss = 0.00251492 (* 1 = 0.00251492 loss)
I1101 07:06:30.548732 11248 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I1101 07:06:47.472985 11248 solver.cpp:228] Iteration 20900, loss = 0.00333761
I1101 07:06:47.472985 11248 solver.cpp:244]     Train net output #0: loss = 0.00333762 (* 1 = 0.00333762 loss)
I1101 07:06:47.472985 11248 sgd_solver.cpp:106] Iteration 20900, lr = 0.001
I1101 07:07:04.279434 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_21000.caffemodel
I1101 07:07:04.439049 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_21000.solverstate
I1101 07:07:04.538493 11248 solver.cpp:337] Iteration 21000, Testing net (#0)
I1101 07:07:09.588780 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:07:09.588780 11248 solver.cpp:404]     Test net output #1: loss = 0.0132224 (* 1 = 0.0132224 loss)
I1101 07:07:09.660507 11248 solver.cpp:228] Iteration 21000, loss = 0.00321061
I1101 07:07:09.660507 11248 solver.cpp:244]     Train net output #0: loss = 0.00321062 (* 1 = 0.00321062 loss)
I1101 07:07:09.660507 11248 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I1101 07:07:26.557160 11248 solver.cpp:228] Iteration 21100, loss = 0.00401844
I1101 07:07:26.557160 11248 solver.cpp:244]     Train net output #0: loss = 0.00401845 (* 1 = 0.00401845 loss)
I1101 07:07:26.557160 11248 sgd_solver.cpp:106] Iteration 21100, lr = 0.001
I1101 07:07:43.415186 11248 solver.cpp:228] Iteration 21200, loss = 0.00504104
I1101 07:07:43.415186 11248 solver.cpp:244]     Train net output #0: loss = 0.00504105 (* 1 = 0.00504105 loss)
I1101 07:07:43.415186 11248 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I1101 07:08:00.285337 11248 solver.cpp:228] Iteration 21300, loss = 0.00461138
I1101 07:08:00.285337 11248 solver.cpp:244]     Train net output #0: loss = 0.00461138 (* 1 = 0.00461138 loss)
I1101 07:08:00.285337 11248 sgd_solver.cpp:106] Iteration 21300, lr = 0.001
I1101 07:08:17.141510 11248 solver.cpp:228] Iteration 21400, loss = 0.00251496
I1101 07:08:17.141510 11248 solver.cpp:244]     Train net output #0: loss = 0.00251497 (* 1 = 0.00251497 loss)
I1101 07:08:17.141510 11248 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I1101 07:08:33.996212 11248 solver.cpp:228] Iteration 21500, loss = 0.00333281
I1101 07:08:33.996212 11248 solver.cpp:244]     Train net output #0: loss = 0.00333282 (* 1 = 0.00333282 loss)
I1101 07:08:33.996212 11248 sgd_solver.cpp:106] Iteration 21500, lr = 0.001
I1101 07:08:50.785815 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_21600.caffemodel
I1101 07:08:50.943929 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_21600.solverstate
I1101 07:08:51.043999 11248 solver.cpp:337] Iteration 21600, Testing net (#0)
I1101 07:08:56.095130 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:08:56.095130 11248 solver.cpp:404]     Test net output #1: loss = 0.0132263 (* 1 = 0.0132263 loss)
I1101 07:08:56.172260 11248 solver.cpp:228] Iteration 21600, loss = 0.00320747
I1101 07:08:56.172260 11248 solver.cpp:244]     Train net output #0: loss = 0.00320747 (* 1 = 0.00320747 loss)
I1101 07:08:56.172260 11248 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I1101 07:09:13.023396 11248 solver.cpp:228] Iteration 21700, loss = 0.00400648
I1101 07:09:13.023396 11248 solver.cpp:244]     Train net output #0: loss = 0.00400648 (* 1 = 0.00400648 loss)
I1101 07:09:13.023396 11248 sgd_solver.cpp:106] Iteration 21700, lr = 0.001
I1101 07:09:29.902592 11248 solver.cpp:228] Iteration 21800, loss = 0.0050217
I1101 07:09:29.902592 11248 solver.cpp:244]     Train net output #0: loss = 0.0050217 (* 1 = 0.0050217 loss)
I1101 07:09:29.902592 11248 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I1101 07:09:46.784440 11248 solver.cpp:228] Iteration 21900, loss = 0.00460074
I1101 07:09:46.784440 11248 solver.cpp:244]     Train net output #0: loss = 0.00460074 (* 1 = 0.00460074 loss)
I1101 07:09:46.784440 11248 sgd_solver.cpp:106] Iteration 21900, lr = 0.001
I1101 07:10:03.740963 11248 solver.cpp:228] Iteration 22000, loss = 0.00251589
I1101 07:10:03.740963 11248 solver.cpp:244]     Train net output #0: loss = 0.00251589 (* 1 = 0.00251589 loss)
I1101 07:10:03.740963 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 3
I1101 07:10:03.740963 11248 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I1101 07:10:20.955946 11248 solver.cpp:228] Iteration 22100, loss = 0.0033207
I1101 07:10:20.955946 11248 solver.cpp:244]     Train net output #0: loss = 0.00332071 (* 1 = 0.00332071 loss)
I1101 07:10:20.955946 11248 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I1101 07:10:37.707864 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_22200.caffemodel
I1101 07:10:37.931504 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_22200.solverstate
I1101 07:10:38.025571 11248 solver.cpp:337] Iteration 22200, Testing net (#0)
I1101 07:10:42.990242 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:10:42.990242 11248 solver.cpp:404]     Test net output #1: loss = 0.0132229 (* 1 = 0.0132229 loss)
I1101 07:10:43.059430 11248 solver.cpp:228] Iteration 22200, loss = 0.00319972
I1101 07:10:43.059430 11248 solver.cpp:244]     Train net output #0: loss = 0.00319972 (* 1 = 0.00319972 loss)
I1101 07:10:43.059430 11248 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I1101 07:10:59.622851 11248 solver.cpp:228] Iteration 22300, loss = 0.00392517
I1101 07:10:59.622851 11248 solver.cpp:244]     Train net output #0: loss = 0.00392517 (* 1 = 0.00392517 loss)
I1101 07:10:59.622851 11248 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I1101 07:11:16.481271 11248 solver.cpp:228] Iteration 22400, loss = 0.00494761
I1101 07:11:16.481271 11248 solver.cpp:244]     Train net output #0: loss = 0.00494762 (* 1 = 0.00494762 loss)
I1101 07:11:16.481271 11248 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I1101 07:11:33.454557 11248 solver.cpp:228] Iteration 22500, loss = 0.0044864
I1101 07:11:33.454557 11248 solver.cpp:244]     Train net output #0: loss = 0.0044864 (* 1 = 0.0044864 loss)
I1101 07:11:33.454557 11248 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I1101 07:11:50.453322 11248 solver.cpp:228] Iteration 22600, loss = 0.00251443
I1101 07:11:50.453322 11248 solver.cpp:244]     Train net output #0: loss = 0.00251443 (* 1 = 0.00251443 loss)
I1101 07:11:50.453322 11248 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I1101 07:12:07.352394 11248 solver.cpp:228] Iteration 22700, loss = 0.00332014
I1101 07:12:07.352394 11248 solver.cpp:244]     Train net output #0: loss = 0.00332015 (* 1 = 0.00332015 loss)
I1101 07:12:07.352394 11248 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I1101 07:12:24.056879 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_22800.caffemodel
I1101 07:12:24.216478 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_22800.solverstate
I1101 07:12:24.316049 11248 solver.cpp:337] Iteration 22800, Testing net (#0)
I1101 07:12:29.330801 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:12:29.330801 11248 solver.cpp:404]     Test net output #1: loss = 0.0132202 (* 1 = 0.0132202 loss)
I1101 07:12:29.402040 11248 solver.cpp:228] Iteration 22800, loss = 0.00320284
I1101 07:12:29.402040 11248 solver.cpp:244]     Train net output #0: loss = 0.00320284 (* 1 = 0.00320284 loss)
I1101 07:12:29.402040 11248 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I1101 07:12:46.171488 11248 solver.cpp:228] Iteration 22900, loss = 0.00392518
I1101 07:12:46.171488 11248 solver.cpp:244]     Train net output #0: loss = 0.00392519 (* 1 = 0.00392519 loss)
I1101 07:12:46.171488 11248 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I1101 07:13:03.267815 11248 solver.cpp:228] Iteration 23000, loss = 0.00494901
I1101 07:13:03.267815 11248 solver.cpp:244]     Train net output #0: loss = 0.00494902 (* 1 = 0.00494902 loss)
I1101 07:13:03.267815 11248 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I1101 07:13:20.404178 11248 solver.cpp:228] Iteration 23100, loss = 0.00449139
I1101 07:13:20.404178 11248 solver.cpp:244]     Train net output #0: loss = 0.00449139 (* 1 = 0.00449139 loss)
I1101 07:13:20.404178 11248 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I1101 07:13:37.339169 11248 solver.cpp:228] Iteration 23200, loss = 0.00251317
I1101 07:13:37.339169 11248 solver.cpp:244]     Train net output #0: loss = 0.00251317 (* 1 = 0.00251317 loss)
I1101 07:13:37.339169 11248 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I1101 07:13:54.793557 11248 solver.cpp:228] Iteration 23300, loss = 0.00331949
I1101 07:13:54.793557 11248 solver.cpp:244]     Train net output #0: loss = 0.0033195 (* 1 = 0.0033195 loss)
I1101 07:13:54.793557 11248 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I1101 07:14:11.663833 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_23400.caffemodel
I1101 07:14:11.827950 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_23400.solverstate
I1101 07:14:12.061938 11248 solver.cpp:337] Iteration 23400, Testing net (#0)
I1101 07:14:17.137828 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:14:17.137828 11248 solver.cpp:404]     Test net output #1: loss = 0.013218 (* 1 = 0.013218 loss)
I1101 07:14:17.209040 11248 solver.cpp:228] Iteration 23400, loss = 0.00320524
I1101 07:14:17.209040 11248 solver.cpp:244]     Train net output #0: loss = 0.00320524 (* 1 = 0.00320524 loss)
I1101 07:14:17.209040 11248 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I1101 07:14:34.083554 11248 solver.cpp:228] Iteration 23500, loss = 0.00392499
I1101 07:14:34.083554 11248 solver.cpp:244]     Train net output #0: loss = 0.00392499 (* 1 = 0.00392499 loss)
I1101 07:14:34.083554 11248 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I1101 07:14:50.970506 11248 solver.cpp:228] Iteration 23600, loss = 0.00494958
I1101 07:14:50.970506 11248 solver.cpp:244]     Train net output #0: loss = 0.00494958 (* 1 = 0.00494958 loss)
I1101 07:14:50.970506 11248 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I1101 07:15:07.870301 11248 solver.cpp:228] Iteration 23700, loss = 0.00449543
I1101 07:15:07.870301 11248 solver.cpp:244]     Train net output #0: loss = 0.00449544 (* 1 = 0.00449544 loss)
I1101 07:15:07.870301 11248 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I1101 07:15:24.775008 11248 solver.cpp:228] Iteration 23800, loss = 0.00251213
I1101 07:15:24.775008 11248 solver.cpp:244]     Train net output #0: loss = 0.00251214 (* 1 = 0.00251214 loss)
I1101 07:15:24.775008 11248 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I1101 07:15:41.679080 11248 solver.cpp:228] Iteration 23900, loss = 0.0033187
I1101 07:15:41.679080 11248 solver.cpp:244]     Train net output #0: loss = 0.0033187 (* 1 = 0.0033187 loss)
I1101 07:15:41.679080 11248 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I1101 07:15:58.502398 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_24000.caffemodel
I1101 07:15:58.670516 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_24000.solverstate
I1101 07:15:58.772089 11248 solver.cpp:337] Iteration 24000, Testing net (#0)
I1101 07:16:03.808428 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9967
I1101 07:16:03.808428 11248 solver.cpp:404]     Test net output #1: loss = 0.0132161 (* 1 = 0.0132161 loss)
I1101 07:16:03.880877 11248 solver.cpp:228] Iteration 24000, loss = 0.00320711
I1101 07:16:03.880877 11248 solver.cpp:244]     Train net output #0: loss = 0.00320711 (* 1 = 0.00320711 loss)
I1101 07:16:03.880877 11248 sgd_solver.cpp:106] Iteration 24000, lr = 0.0001
I1101 07:16:20.787998 11248 solver.cpp:228] Iteration 24100, loss = 0.00392463
I1101 07:16:20.787998 11248 solver.cpp:244]     Train net output #0: loss = 0.00392463 (* 1 = 0.00392463 loss)
I1101 07:16:20.787998 11248 sgd_solver.cpp:106] Iteration 24100, lr = 0.0001
I1101 07:16:37.661795 11248 solver.cpp:228] Iteration 24200, loss = 0.00494965
I1101 07:16:37.661795 11248 solver.cpp:244]     Train net output #0: loss = 0.00494966 (* 1 = 0.00494966 loss)
I1101 07:16:37.661795 11248 sgd_solver.cpp:106] Iteration 24200, lr = 0.0001
I1101 07:16:54.443826 11248 solver.cpp:228] Iteration 24300, loss = 0.00449846
I1101 07:16:54.443826 11248 solver.cpp:244]     Train net output #0: loss = 0.00449846 (* 1 = 0.00449846 loss)
I1101 07:16:54.443826 11248 sgd_solver.cpp:106] Iteration 24300, lr = 0.0001
I1101 07:17:11.530891 11248 solver.cpp:228] Iteration 24400, loss = 0.00251128
I1101 07:17:11.530891 11248 solver.cpp:244]     Train net output #0: loss = 0.00251128 (* 1 = 0.00251128 loss)
I1101 07:17:11.530891 11248 sgd_solver.cpp:106] Iteration 24400, lr = 0.0001
I1101 07:17:28.426530 11248 solver.cpp:228] Iteration 24500, loss = 0.00331786
I1101 07:17:28.426530 11248 solver.cpp:244]     Train net output #0: loss = 0.00331786 (* 1 = 0.00331786 loss)
I1101 07:17:28.426530 11248 sgd_solver.cpp:106] Iteration 24500, lr = 0.0001
I1101 07:17:45.237862 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_24600.caffemodel
I1101 07:17:45.400594 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_24600.solverstate
I1101 07:17:45.502667 11248 solver.cpp:337] Iteration 24600, Testing net (#0)
I1101 07:17:50.563669 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:17:50.563669 11248 solver.cpp:404]     Test net output #1: loss = 0.0132148 (* 1 = 0.0132148 loss)
I1101 07:17:50.635614 11248 solver.cpp:228] Iteration 24600, loss = 0.00320853
I1101 07:17:50.635614 11248 solver.cpp:244]     Train net output #0: loss = 0.00320853 (* 1 = 0.00320853 loss)
I1101 07:17:50.635614 11248 sgd_solver.cpp:106] Iteration 24600, lr = 0.0001
I1101 07:18:07.373348 11248 solver.cpp:228] Iteration 24700, loss = 0.00392426
I1101 07:18:07.373348 11248 solver.cpp:244]     Train net output #0: loss = 0.00392427 (* 1 = 0.00392427 loss)
I1101 07:18:07.373348 11248 sgd_solver.cpp:106] Iteration 24700, lr = 0.0001
I1101 07:18:24.136888 11248 solver.cpp:228] Iteration 24800, loss = 0.00494932
I1101 07:18:24.136888 11248 solver.cpp:244]     Train net output #0: loss = 0.00494933 (* 1 = 0.00494933 loss)
I1101 07:18:24.136888 11248 sgd_solver.cpp:106] Iteration 24800, lr = 0.0001
I1101 07:18:40.694087 11248 solver.cpp:228] Iteration 24900, loss = 0.00450077
I1101 07:18:40.694087 11248 solver.cpp:244]     Train net output #0: loss = 0.00450077 (* 1 = 0.00450077 loss)
I1101 07:18:40.694087 11248 sgd_solver.cpp:106] Iteration 24900, lr = 0.0001
I1101 07:18:57.180328 11248 solver.cpp:228] Iteration 25000, loss = 0.00251059
I1101 07:18:57.180328 11248 solver.cpp:244]     Train net output #0: loss = 0.00251059 (* 1 = 0.00251059 loss)
I1101 07:18:57.180328 11248 sgd_solver.cpp:106] Iteration 25000, lr = 0.0001
I1101 07:19:13.738983 11248 solver.cpp:228] Iteration 25100, loss = 0.00331706
I1101 07:19:13.738983 11248 solver.cpp:244]     Train net output #0: loss = 0.00331706 (* 1 = 0.00331706 loss)
I1101 07:19:13.738983 11248 sgd_solver.cpp:106] Iteration 25100, lr = 0.0001
I1101 07:19:30.145776 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_25200.caffemodel
I1101 07:19:30.307893 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_25200.solverstate
I1101 07:19:30.402958 11248 solver.cpp:337] Iteration 25200, Testing net (#0)
I1101 07:19:35.343736 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:19:35.343736 11248 solver.cpp:404]     Test net output #1: loss = 0.0132135 (* 1 = 0.0132135 loss)
I1101 07:19:35.412787 11248 solver.cpp:228] Iteration 25200, loss = 0.00320947
I1101 07:19:35.412787 11248 solver.cpp:244]     Train net output #0: loss = 0.00320948 (* 1 = 0.00320948 loss)
I1101 07:19:35.412787 11248 sgd_solver.cpp:106] Iteration 25200, lr = 0.0001
I1101 07:19:51.985796 11248 solver.cpp:228] Iteration 25300, loss = 0.00392359
I1101 07:19:51.985796 11248 solver.cpp:244]     Train net output #0: loss = 0.0039236 (* 1 = 0.0039236 loss)
I1101 07:19:51.985796 11248 sgd_solver.cpp:106] Iteration 25300, lr = 0.0001
I1101 07:20:08.595832 11248 solver.cpp:228] Iteration 25400, loss = 0.00494861
I1101 07:20:08.595832 11248 solver.cpp:244]     Train net output #0: loss = 0.00494862 (* 1 = 0.00494862 loss)
I1101 07:20:08.595832 11248 sgd_solver.cpp:106] Iteration 25400, lr = 0.0001
I1101 07:20:25.053704 11248 solver.cpp:228] Iteration 25500, loss = 0.00450257
I1101 07:20:25.053704 11248 solver.cpp:244]     Train net output #0: loss = 0.00450257 (* 1 = 0.00450257 loss)
I1101 07:20:25.053704 11248 sgd_solver.cpp:106] Iteration 25500, lr = 0.0001
I1101 07:20:41.510560 11248 solver.cpp:228] Iteration 25600, loss = 0.00250995
I1101 07:20:41.510560 11248 solver.cpp:244]     Train net output #0: loss = 0.00250996 (* 1 = 0.00250996 loss)
I1101 07:20:41.510560 11248 sgd_solver.cpp:106] Iteration 25600, lr = 0.0001
I1101 07:20:57.986829 11248 solver.cpp:228] Iteration 25700, loss = 0.0033162
I1101 07:20:57.986829 11248 solver.cpp:244]     Train net output #0: loss = 0.00331621 (* 1 = 0.00331621 loss)
I1101 07:20:57.986829 11248 sgd_solver.cpp:106] Iteration 25700, lr = 0.0001
I1101 07:21:14.319469 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_25800.caffemodel
I1101 07:21:14.468075 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_25800.solverstate
I1101 07:21:14.564143 11248 solver.cpp:337] Iteration 25800, Testing net (#0)
I1101 07:21:19.496340 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:21:19.496340 11248 solver.cpp:404]     Test net output #1: loss = 0.0132129 (* 1 = 0.0132129 loss)
I1101 07:21:19.574468 11248 solver.cpp:228] Iteration 25800, loss = 0.00321041
I1101 07:21:19.574468 11248 solver.cpp:244]     Train net output #0: loss = 0.00321041 (* 1 = 0.00321041 loss)
I1101 07:21:19.574468 11248 sgd_solver.cpp:106] Iteration 25800, lr = 0.0001
I1101 07:21:36.093011 11248 solver.cpp:228] Iteration 25900, loss = 0.00392301
I1101 07:21:36.093011 11248 solver.cpp:244]     Train net output #0: loss = 0.00392301 (* 1 = 0.00392301 loss)
I1101 07:21:36.093011 11248 sgd_solver.cpp:106] Iteration 25900, lr = 0.0001
I1101 07:21:52.650275 11248 solver.cpp:228] Iteration 26000, loss = 0.00494761
I1101 07:21:52.650275 11248 solver.cpp:244]     Train net output #0: loss = 0.00494761 (* 1 = 0.00494761 loss)
I1101 07:21:52.650275 11248 sgd_solver.cpp:106] Iteration 26000, lr = 0.0001
I1101 07:22:09.194844 11248 solver.cpp:228] Iteration 26100, loss = 0.0045038
I1101 07:22:09.194844 11248 solver.cpp:244]     Train net output #0: loss = 0.00450381 (* 1 = 0.00450381 loss)
I1101 07:22:09.194844 11248 sgd_solver.cpp:106] Iteration 26100, lr = 0.0001
I1101 07:22:25.653084 11248 solver.cpp:228] Iteration 26200, loss = 0.00250952
I1101 07:22:25.653084 11248 solver.cpp:244]     Train net output #0: loss = 0.00250952 (* 1 = 0.00250952 loss)
I1101 07:22:25.653084 11248 sgd_solver.cpp:106] Iteration 26200, lr = 0.0001
I1101 07:22:42.109835 11248 solver.cpp:228] Iteration 26300, loss = 0.00331532
I1101 07:22:42.109835 11248 solver.cpp:244]     Train net output #0: loss = 0.00331533 (* 1 = 0.00331533 loss)
I1101 07:22:42.109835 11248 sgd_solver.cpp:106] Iteration 26300, lr = 0.0001
I1101 07:22:58.491763 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_26400.caffemodel
I1101 07:22:58.641870 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_26400.solverstate
I1101 07:22:58.738440 11248 solver.cpp:337] Iteration 26400, Testing net (#0)
I1101 07:23:03.663435 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:23:03.663435 11248 solver.cpp:404]     Test net output #1: loss = 0.0132121 (* 1 = 0.0132121 loss)
I1101 07:23:03.733021 11248 solver.cpp:228] Iteration 26400, loss = 0.00321078
I1101 07:23:03.733021 11248 solver.cpp:244]     Train net output #0: loss = 0.00321078 (* 1 = 0.00321078 loss)
I1101 07:23:03.733021 11248 sgd_solver.cpp:106] Iteration 26400, lr = 0.0001
I1101 07:23:20.194005 11248 solver.cpp:228] Iteration 26500, loss = 0.00392219
I1101 07:23:20.194506 11248 solver.cpp:244]     Train net output #0: loss = 0.00392219 (* 1 = 0.00392219 loss)
I1101 07:23:20.194506 11248 sgd_solver.cpp:106] Iteration 26500, lr = 0.0001
I1101 07:23:36.686393 11248 solver.cpp:228] Iteration 26600, loss = 0.00494631
I1101 07:23:36.686393 11248 solver.cpp:244]     Train net output #0: loss = 0.00494632 (* 1 = 0.00494632 loss)
I1101 07:23:36.686393 11248 sgd_solver.cpp:106] Iteration 26600, lr = 0.0001
I1101 07:23:53.145795 11248 solver.cpp:228] Iteration 26700, loss = 0.00450475
I1101 07:23:53.145795 11248 solver.cpp:244]     Train net output #0: loss = 0.00450476 (* 1 = 0.00450476 loss)
I1101 07:23:53.145795 11248 sgd_solver.cpp:106] Iteration 26700, lr = 0.0001
I1101 07:24:09.626659 11248 solver.cpp:228] Iteration 26800, loss = 0.0025091
I1101 07:24:09.626659 11248 solver.cpp:244]     Train net output #0: loss = 0.0025091 (* 1 = 0.0025091 loss)
I1101 07:24:09.626659 11248 sgd_solver.cpp:106] Iteration 26800, lr = 0.0001
I1101 07:24:26.084563 11248 solver.cpp:228] Iteration 26900, loss = 0.00331447
I1101 07:24:26.084563 11248 solver.cpp:244]     Train net output #0: loss = 0.00331448 (* 1 = 0.00331448 loss)
I1101 07:24:26.084563 11248 sgd_solver.cpp:106] Iteration 26900, lr = 0.0001
I1101 07:24:42.505058 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_27000.caffemodel
I1101 07:24:42.656164 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_27000.solverstate
I1101 07:24:42.750231 11248 solver.cpp:337] Iteration 27000, Testing net (#0)
I1101 07:24:47.677346 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:24:47.677346 11248 solver.cpp:404]     Test net output #1: loss = 0.0132115 (* 1 = 0.0132115 loss)
I1101 07:24:47.747131 11248 solver.cpp:228] Iteration 27000, loss = 0.00321108
I1101 07:24:47.747131 11248 solver.cpp:244]     Train net output #0: loss = 0.00321108 (* 1 = 0.00321108 loss)
I1101 07:24:47.747131 11248 sgd_solver.cpp:106] Iteration 27000, lr = 0.0001
I1101 07:25:04.207870 11248 solver.cpp:228] Iteration 27100, loss = 0.00392137
I1101 07:25:04.207870 11248 solver.cpp:244]     Train net output #0: loss = 0.00392138 (* 1 = 0.00392138 loss)
I1101 07:25:04.207870 11248 sgd_solver.cpp:106] Iteration 27100, lr = 0.0001
I1101 07:25:20.675624 11248 solver.cpp:228] Iteration 27200, loss = 0.00494493
I1101 07:25:20.675624 11248 solver.cpp:244]     Train net output #0: loss = 0.00494493 (* 1 = 0.00494493 loss)
I1101 07:25:20.675624 11248 sgd_solver.cpp:106] Iteration 27200, lr = 0.0001
I1101 07:25:37.120153 11248 solver.cpp:228] Iteration 27300, loss = 0.00450563
I1101 07:25:37.120153 11248 solver.cpp:244]     Train net output #0: loss = 0.00450563 (* 1 = 0.00450563 loss)
I1101 07:25:37.120153 11248 sgd_solver.cpp:106] Iteration 27300, lr = 0.0001
I1101 07:25:53.589570 11248 solver.cpp:228] Iteration 27400, loss = 0.00250881
I1101 07:25:53.589570 11248 solver.cpp:244]     Train net output #0: loss = 0.00250882 (* 1 = 0.00250882 loss)
I1101 07:25:53.589570 11248 sgd_solver.cpp:106] Iteration 27400, lr = 0.0001
I1101 07:26:10.045457 11248 solver.cpp:228] Iteration 27500, loss = 0.00331357
I1101 07:26:10.045457 11248 solver.cpp:244]     Train net output #0: loss = 0.00331357 (* 1 = 0.00331357 loss)
I1101 07:26:10.045457 11248 sgd_solver.cpp:106] Iteration 27500, lr = 0.0001
I1101 07:26:26.450333 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_27600.caffemodel
I1101 07:26:26.600440 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_27600.solverstate
I1101 07:26:26.694507 11248 solver.cpp:337] Iteration 27600, Testing net (#0)
I1101 07:26:31.608580 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:26:31.608580 11248 solver.cpp:404]     Test net output #1: loss = 0.0132111 (* 1 = 0.0132111 loss)
I1101 07:26:31.702517 11248 solver.cpp:228] Iteration 27600, loss = 0.00321116
I1101 07:26:31.702517 11248 solver.cpp:244]     Train net output #0: loss = 0.00321117 (* 1 = 0.00321117 loss)
I1101 07:26:31.702517 11248 sgd_solver.cpp:106] Iteration 27600, lr = 0.0001
I1101 07:26:48.140195 11248 solver.cpp:228] Iteration 27700, loss = 0.00392055
I1101 07:26:48.140195 11248 solver.cpp:244]     Train net output #0: loss = 0.00392055 (* 1 = 0.00392055 loss)
I1101 07:26:48.140195 11248 sgd_solver.cpp:106] Iteration 27700, lr = 0.0001
I1101 07:27:04.605335 11248 solver.cpp:228] Iteration 27800, loss = 0.00494334
I1101 07:27:04.605335 11248 solver.cpp:244]     Train net output #0: loss = 0.00494335 (* 1 = 0.00494335 loss)
I1101 07:27:04.605335 11248 sgd_solver.cpp:106] Iteration 27800, lr = 0.0001
I1101 07:27:21.073923 11248 solver.cpp:228] Iteration 27900, loss = 0.00450606
I1101 07:27:21.073923 11248 solver.cpp:244]     Train net output #0: loss = 0.00450606 (* 1 = 0.00450606 loss)
I1101 07:27:21.073923 11248 sgd_solver.cpp:106] Iteration 27900, lr = 0.0001
I1101 07:27:37.552922 11248 solver.cpp:228] Iteration 28000, loss = 0.00250851
I1101 07:27:37.552922 11248 solver.cpp:244]     Train net output #0: loss = 0.00250852 (* 1 = 0.00250852 loss)
I1101 07:27:37.552922 11248 sgd_solver.cpp:106] Iteration 28000, lr = 0.0001
I1101 07:27:54.023082 11248 solver.cpp:228] Iteration 28100, loss = 0.00331277
I1101 07:27:54.023082 11248 solver.cpp:244]     Train net output #0: loss = 0.00331277 (* 1 = 0.00331277 loss)
I1101 07:27:54.023082 11248 sgd_solver.cpp:106] Iteration 28100, lr = 0.0001
I1101 07:28:10.394798 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_28200.caffemodel
I1101 07:28:10.551051 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_28200.solverstate
I1101 07:28:10.644803 11248 solver.cpp:337] Iteration 28200, Testing net (#0)
I1101 07:28:15.584599 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:28:15.584599 11248 solver.cpp:404]     Test net output #1: loss = 0.0132109 (* 1 = 0.0132109 loss)
I1101 07:28:15.653712 11248 solver.cpp:228] Iteration 28200, loss = 0.00321107
I1101 07:28:15.653712 11248 solver.cpp:244]     Train net output #0: loss = 0.00321107 (* 1 = 0.00321107 loss)
I1101 07:28:15.653712 11248 sgd_solver.cpp:106] Iteration 28200, lr = 0.0001
I1101 07:28:32.126232 11248 solver.cpp:228] Iteration 28300, loss = 0.00391961
I1101 07:28:32.126232 11248 solver.cpp:244]     Train net output #0: loss = 0.00391961 (* 1 = 0.00391961 loss)
I1101 07:28:32.126232 11248 sgd_solver.cpp:106] Iteration 28300, lr = 0.0001
I1101 07:28:48.593592 11248 solver.cpp:228] Iteration 28400, loss = 0.00494174
I1101 07:28:48.593592 11248 solver.cpp:244]     Train net output #0: loss = 0.00494174 (* 1 = 0.00494174 loss)
I1101 07:28:48.593592 11248 sgd_solver.cpp:106] Iteration 28400, lr = 0.0001
I1101 07:29:05.048991 11248 solver.cpp:228] Iteration 28500, loss = 0.00450612
I1101 07:29:05.048991 11248 solver.cpp:244]     Train net output #0: loss = 0.00450612 (* 1 = 0.00450612 loss)
I1101 07:29:05.048991 11248 sgd_solver.cpp:106] Iteration 28500, lr = 0.0001
I1101 07:29:21.508894 11248 solver.cpp:228] Iteration 28600, loss = 0.00250834
I1101 07:29:21.508894 11248 solver.cpp:244]     Train net output #0: loss = 0.00250834 (* 1 = 0.00250834 loss)
I1101 07:29:21.508894 11248 sgd_solver.cpp:106] Iteration 28600, lr = 0.0001
I1101 07:29:37.980366 11248 solver.cpp:228] Iteration 28700, loss = 0.00331198
I1101 07:29:37.980366 11248 solver.cpp:244]     Train net output #0: loss = 0.00331198 (* 1 = 0.00331198 loss)
I1101 07:29:37.980366 11248 sgd_solver.cpp:106] Iteration 28700, lr = 0.0001
I1101 07:29:54.379266 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_28800.caffemodel
I1101 07:29:54.530375 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_28800.solverstate
I1101 07:29:54.627444 11248 solver.cpp:337] Iteration 28800, Testing net (#0)
I1101 07:29:59.561345 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:29:59.561345 11248 solver.cpp:404]     Test net output #1: loss = 0.0132105 (* 1 = 0.0132105 loss)
I1101 07:29:59.638185 11248 solver.cpp:228] Iteration 28800, loss = 0.00321084
I1101 07:29:59.638185 11248 solver.cpp:244]     Train net output #0: loss = 0.00321084 (* 1 = 0.00321084 loss)
I1101 07:29:59.638185 11248 sgd_solver.cpp:106] Iteration 28800, lr = 0.0001
I1101 07:30:16.137615 11248 solver.cpp:228] Iteration 28900, loss = 0.00391878
I1101 07:30:16.137615 11248 solver.cpp:244]     Train net output #0: loss = 0.00391878 (* 1 = 0.00391878 loss)
I1101 07:30:16.137615 11248 sgd_solver.cpp:106] Iteration 28900, lr = 0.0001
I1101 07:30:32.626354 11248 solver.cpp:228] Iteration 29000, loss = 0.00493996
I1101 07:30:32.626354 11248 solver.cpp:244]     Train net output #0: loss = 0.00493996 (* 1 = 0.00493996 loss)
I1101 07:30:32.626354 11248 sgd_solver.cpp:106] Iteration 29000, lr = 0.0001
I1101 07:30:49.072777 11248 solver.cpp:228] Iteration 29100, loss = 0.00450653
I1101 07:30:49.072777 11248 solver.cpp:244]     Train net output #0: loss = 0.00450653 (* 1 = 0.00450653 loss)
I1101 07:30:49.072777 11248 sgd_solver.cpp:106] Iteration 29100, lr = 0.0001
I1101 07:31:05.540065 11248 solver.cpp:228] Iteration 29200, loss = 0.00250807
I1101 07:31:05.540065 11248 solver.cpp:244]     Train net output #0: loss = 0.00250807 (* 1 = 0.00250807 loss)
I1101 07:31:05.540065 11248 sgd_solver.cpp:106] Iteration 29200, lr = 0.0001
I1101 07:31:21.997903 11248 solver.cpp:228] Iteration 29300, loss = 0.00331128
I1101 07:31:21.997903 11248 solver.cpp:244]     Train net output #0: loss = 0.00331128 (* 1 = 0.00331128 loss)
I1101 07:31:21.997903 11248 sgd_solver.cpp:106] Iteration 29300, lr = 0.0001
I1101 07:31:38.375274 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_29400.caffemodel
I1101 07:31:38.532079 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_29400.solverstate
I1101 07:31:38.629791 11248 solver.cpp:337] Iteration 29400, Testing net (#0)
I1101 07:31:43.563447 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:31:43.563447 11248 solver.cpp:404]     Test net output #1: loss = 0.0132107 (* 1 = 0.0132107 loss)
I1101 07:31:43.633611 11248 solver.cpp:228] Iteration 29400, loss = 0.00321069
I1101 07:31:43.633611 11248 solver.cpp:244]     Train net output #0: loss = 0.00321069 (* 1 = 0.00321069 loss)
I1101 07:31:43.633611 11248 sgd_solver.cpp:106] Iteration 29400, lr = 0.0001
I1101 07:32:00.084120 11248 solver.cpp:228] Iteration 29500, loss = 0.00391803
I1101 07:32:00.084120 11248 solver.cpp:244]     Train net output #0: loss = 0.00391803 (* 1 = 0.00391803 loss)
I1101 07:32:00.084120 11248 sgd_solver.cpp:106] Iteration 29500, lr = 0.0001
I1101 07:32:16.563567 11248 solver.cpp:228] Iteration 29600, loss = 0.00493813
I1101 07:32:16.563567 11248 solver.cpp:244]     Train net output #0: loss = 0.00493813 (* 1 = 0.00493813 loss)
I1101 07:32:16.563567 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 29600, step = 4
I1101 07:32:16.563567 11248 sgd_solver.cpp:106] Iteration 29600, lr = 1e-005
I1101 07:32:32.983044 11248 solver.cpp:228] Iteration 29700, loss = 0.00449939
I1101 07:32:32.983044 11248 solver.cpp:244]     Train net output #0: loss = 0.00449939 (* 1 = 0.00449939 loss)
I1101 07:32:32.983044 11248 sgd_solver.cpp:106] Iteration 29700, lr = 1e-005
I1101 07:32:49.401572 11248 solver.cpp:228] Iteration 29800, loss = 0.00250711
I1101 07:32:49.401572 11248 solver.cpp:244]     Train net output #0: loss = 0.00250711 (* 1 = 0.00250711 loss)
I1101 07:32:49.401572 11248 sgd_solver.cpp:106] Iteration 29800, lr = 1e-005
I1101 07:33:05.845787 11248 solver.cpp:228] Iteration 29900, loss = 0.00330792
I1101 07:33:05.845787 11248 solver.cpp:244]     Train net output #0: loss = 0.00330792 (* 1 = 0.00330792 loss)
I1101 07:33:05.845787 11248 sgd_solver.cpp:106] Iteration 29900, lr = 1e-005
I1101 07:33:22.189707 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_30000.caffemodel
I1101 07:33:22.322170 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_30000.solverstate
I1101 07:33:22.415920 11248 solver.cpp:337] Iteration 30000, Testing net (#0)
I1101 07:33:27.348168 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:33:27.348168 11248 solver.cpp:404]     Test net output #1: loss = 0.0132101 (* 1 = 0.0132101 loss)
I1101 07:33:27.426285 11248 solver.cpp:228] Iteration 30000, loss = 0.00321256
I1101 07:33:27.426285 11248 solver.cpp:244]     Train net output #0: loss = 0.00321256 (* 1 = 0.00321256 loss)
I1101 07:33:27.426285 11248 sgd_solver.cpp:106] Iteration 30000, lr = 1e-005
I1101 07:33:43.850111 11248 solver.cpp:228] Iteration 30100, loss = 0.00390862
I1101 07:33:43.850111 11248 solver.cpp:244]     Train net output #0: loss = 0.00390862 (* 1 = 0.00390862 loss)
I1101 07:33:43.850111 11248 sgd_solver.cpp:106] Iteration 30100, lr = 1e-005
I1101 07:34:00.267160 11248 solver.cpp:228] Iteration 30200, loss = 0.00493757
I1101 07:34:00.267160 11248 solver.cpp:244]     Train net output #0: loss = 0.00493757 (* 1 = 0.00493757 loss)
I1101 07:34:00.267160 11248 sgd_solver.cpp:106] Iteration 30200, lr = 1e-005
I1101 07:34:16.710847 11248 solver.cpp:228] Iteration 30300, loss = 0.00449916
I1101 07:34:16.710847 11248 solver.cpp:244]     Train net output #0: loss = 0.00449916 (* 1 = 0.00449916 loss)
I1101 07:34:16.710847 11248 sgd_solver.cpp:106] Iteration 30300, lr = 1e-005
I1101 07:34:33.140396 11248 solver.cpp:228] Iteration 30400, loss = 0.00250682
I1101 07:34:33.140396 11248 solver.cpp:244]     Train net output #0: loss = 0.00250682 (* 1 = 0.00250682 loss)
I1101 07:34:33.140396 11248 sgd_solver.cpp:106] Iteration 30400, lr = 1e-005
I1101 07:34:49.568835 11248 solver.cpp:228] Iteration 30500, loss = 0.00330758
I1101 07:34:49.568835 11248 solver.cpp:244]     Train net output #0: loss = 0.00330758 (* 1 = 0.00330758 loss)
I1101 07:34:49.568835 11248 sgd_solver.cpp:106] Iteration 30500, lr = 1e-005
I1101 07:35:05.911626 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_30600.caffemodel
I1101 07:35:06.059981 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_30600.solverstate
I1101 07:35:06.153733 11248 solver.cpp:337] Iteration 30600, Testing net (#0)
I1101 07:35:11.068498 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:35:11.068498 11248 solver.cpp:404]     Test net output #1: loss = 0.0132098 (* 1 = 0.0132098 loss)
I1101 07:35:11.146637 11248 solver.cpp:228] Iteration 30600, loss = 0.00321222
I1101 07:35:11.146637 11248 solver.cpp:244]     Train net output #0: loss = 0.00321221 (* 1 = 0.00321221 loss)
I1101 07:35:11.146637 11248 sgd_solver.cpp:106] Iteration 30600, lr = 1e-005
I1101 07:35:27.555186 11248 solver.cpp:228] Iteration 30700, loss = 0.00390829
I1101 07:35:27.555186 11248 solver.cpp:244]     Train net output #0: loss = 0.00390829 (* 1 = 0.00390829 loss)
I1101 07:35:27.555186 11248 sgd_solver.cpp:106] Iteration 30700, lr = 1e-005
I1101 07:35:43.995167 11248 solver.cpp:228] Iteration 30800, loss = 0.00493703
I1101 07:35:43.995167 11248 solver.cpp:244]     Train net output #0: loss = 0.00493703 (* 1 = 0.00493703 loss)
I1101 07:35:43.995167 11248 sgd_solver.cpp:106] Iteration 30800, lr = 1e-005
I1101 07:36:00.424198 11248 solver.cpp:228] Iteration 30900, loss = 0.00449892
I1101 07:36:00.424198 11248 solver.cpp:244]     Train net output #0: loss = 0.00449892 (* 1 = 0.00449892 loss)
I1101 07:36:00.424198 11248 sgd_solver.cpp:106] Iteration 30900, lr = 1e-005
I1101 07:36:16.865665 11248 solver.cpp:228] Iteration 31000, loss = 0.00250654
I1101 07:36:16.865665 11248 solver.cpp:244]     Train net output #0: loss = 0.00250654 (* 1 = 0.00250654 loss)
I1101 07:36:16.865665 11248 sgd_solver.cpp:106] Iteration 31000, lr = 1e-005
I1101 07:36:33.289526 11248 solver.cpp:228] Iteration 31100, loss = 0.00330725
I1101 07:36:33.289526 11248 solver.cpp:244]     Train net output #0: loss = 0.00330725 (* 1 = 0.00330725 loss)
I1101 07:36:33.289526 11248 sgd_solver.cpp:106] Iteration 31100, lr = 1e-005
I1101 07:36:49.628984 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_31200.caffemodel
I1101 07:36:49.782105 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_31200.solverstate
I1101 07:36:49.871279 11248 solver.cpp:337] Iteration 31200, Testing net (#0)
I1101 07:36:54.797114 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:36:54.797114 11248 solver.cpp:404]     Test net output #1: loss = 0.0132095 (* 1 = 0.0132095 loss)
I1101 07:36:54.875242 11248 solver.cpp:228] Iteration 31200, loss = 0.00321187
I1101 07:36:54.875242 11248 solver.cpp:244]     Train net output #0: loss = 0.00321187 (* 1 = 0.00321187 loss)
I1101 07:36:54.875242 11248 sgd_solver.cpp:106] Iteration 31200, lr = 1e-005
I1101 07:37:11.290923 11248 solver.cpp:228] Iteration 31300, loss = 0.00390796
I1101 07:37:11.290923 11248 solver.cpp:244]     Train net output #0: loss = 0.00390796 (* 1 = 0.00390796 loss)
I1101 07:37:11.290923 11248 sgd_solver.cpp:106] Iteration 31300, lr = 1e-005
I1101 07:37:27.730743 11248 solver.cpp:228] Iteration 31400, loss = 0.00493648
I1101 07:37:27.730743 11248 solver.cpp:244]     Train net output #0: loss = 0.00493648 (* 1 = 0.00493648 loss)
I1101 07:37:27.730743 11248 sgd_solver.cpp:106] Iteration 31400, lr = 1e-005
I1101 07:37:44.159314 11248 solver.cpp:228] Iteration 31500, loss = 0.00449868
I1101 07:37:44.159314 11248 solver.cpp:244]     Train net output #0: loss = 0.00449868 (* 1 = 0.00449868 loss)
I1101 07:37:44.159314 11248 sgd_solver.cpp:106] Iteration 31500, lr = 1e-005
I1101 07:38:00.595067 11248 solver.cpp:228] Iteration 31600, loss = 0.00250627
I1101 07:38:00.595067 11248 solver.cpp:244]     Train net output #0: loss = 0.00250627 (* 1 = 0.00250627 loss)
I1101 07:38:00.595067 11248 sgd_solver.cpp:106] Iteration 31600, lr = 1e-005
I1101 07:38:16.993172 11248 solver.cpp:228] Iteration 31700, loss = 0.00330694
I1101 07:38:16.993172 11248 solver.cpp:244]     Train net output #0: loss = 0.00330693 (* 1 = 0.00330693 loss)
I1101 07:38:16.993172 11248 sgd_solver.cpp:106] Iteration 31700, lr = 1e-005
I1101 07:38:33.346532 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_31800.caffemodel
I1101 07:38:33.483669 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_31800.solverstate
I1101 07:38:33.577433 11248 solver.cpp:337] Iteration 31800, Testing net (#0)
I1101 07:38:38.503226 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:38:38.503226 11248 solver.cpp:404]     Test net output #1: loss = 0.0132092 (* 1 = 0.0132092 loss)
I1101 07:38:38.596981 11248 solver.cpp:228] Iteration 31800, loss = 0.00321153
I1101 07:38:38.596981 11248 solver.cpp:244]     Train net output #0: loss = 0.00321153 (* 1 = 0.00321153 loss)
I1101 07:38:38.596981 11248 sgd_solver.cpp:106] Iteration 31800, lr = 1e-005
I1101 07:38:55.024502 11248 solver.cpp:228] Iteration 31900, loss = 0.00390762
I1101 07:38:55.024502 11248 solver.cpp:244]     Train net output #0: loss = 0.00390762 (* 1 = 0.00390762 loss)
I1101 07:38:55.024502 11248 sgd_solver.cpp:106] Iteration 31900, lr = 1e-005
I1101 07:39:11.463253 11248 solver.cpp:228] Iteration 32000, loss = 0.00493593
I1101 07:39:11.463253 11248 solver.cpp:244]     Train net output #0: loss = 0.00493593 (* 1 = 0.00493593 loss)
I1101 07:39:11.463253 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 32000, step = 5
I1101 07:39:11.463253 11248 sgd_solver.cpp:106] Iteration 32000, lr = 1e-006
I1101 07:39:27.875850 11248 solver.cpp:228] Iteration 32100, loss = 0.00449774
I1101 07:39:27.875850 11248 solver.cpp:244]     Train net output #0: loss = 0.00449774 (* 1 = 0.00449774 loss)
I1101 07:39:27.875850 11248 sgd_solver.cpp:106] Iteration 32100, lr = 1e-006
I1101 07:39:44.310552 11248 solver.cpp:228] Iteration 32200, loss = 0.00250599
I1101 07:39:44.310552 11248 solver.cpp:244]     Train net output #0: loss = 0.00250599 (* 1 = 0.00250599 loss)
I1101 07:39:44.310552 11248 sgd_solver.cpp:106] Iteration 32200, lr = 1e-006
I1101 07:40:00.767249 11248 solver.cpp:228] Iteration 32300, loss = 0.00330644
I1101 07:40:00.767249 11248 solver.cpp:244]     Train net output #0: loss = 0.00330644 (* 1 = 0.00330644 loss)
I1101 07:40:00.767249 11248 sgd_solver.cpp:106] Iteration 32300, lr = 1e-006
I1101 07:40:17.117707 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_32400.caffemodel
I1101 07:40:17.289585 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_32400.solverstate
I1101 07:40:17.367712 11248 solver.cpp:337] Iteration 32400, Testing net (#0)
I1101 07:40:22.313783 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:40:22.313783 11248 solver.cpp:404]     Test net output #1: loss = 0.013209 (* 1 = 0.013209 loss)
I1101 07:40:22.387768 11248 solver.cpp:228] Iteration 32400, loss = 0.00321161
I1101 07:40:22.387768 11248 solver.cpp:244]     Train net output #0: loss = 0.00321161 (* 1 = 0.00321161 loss)
I1101 07:40:22.387768 11248 sgd_solver.cpp:106] Iteration 32400, lr = 1e-006
I1101 07:40:38.801967 11248 solver.cpp:228] Iteration 32500, loss = 0.00390661
I1101 07:40:38.801967 11248 solver.cpp:244]     Train net output #0: loss = 0.00390661 (* 1 = 0.00390661 loss)
I1101 07:40:38.801967 11248 sgd_solver.cpp:106] Iteration 32500, lr = 1e-006
I1101 07:40:55.216596 11248 solver.cpp:228] Iteration 32600, loss = 0.00493584
I1101 07:40:55.216596 11248 solver.cpp:244]     Train net output #0: loss = 0.00493584 (* 1 = 0.00493584 loss)
I1101 07:40:55.216596 11248 sgd_solver.cpp:106] Iteration 32600, lr = 1e-006
I1101 07:41:11.648212 11248 solver.cpp:228] Iteration 32700, loss = 0.00449769
I1101 07:41:11.648212 11248 solver.cpp:244]     Train net output #0: loss = 0.00449769 (* 1 = 0.00449769 loss)
I1101 07:41:11.648212 11248 sgd_solver.cpp:106] Iteration 32700, lr = 1e-006
I1101 07:41:28.134716 11248 solver.cpp:228] Iteration 32800, loss = 0.00250597
I1101 07:41:28.134716 11248 solver.cpp:244]     Train net output #0: loss = 0.00250597 (* 1 = 0.00250597 loss)
I1101 07:41:28.134716 11248 sgd_solver.cpp:106] Iteration 32800, lr = 1e-006
I1101 07:41:44.582032 11248 solver.cpp:228] Iteration 32900, loss = 0.0033064
I1101 07:41:44.582032 11248 solver.cpp:244]     Train net output #0: loss = 0.0033064 (* 1 = 0.0033064 loss)
I1101 07:41:44.582032 11248 sgd_solver.cpp:106] Iteration 32900, lr = 1e-006
I1101 07:42:00.953375 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_33000.caffemodel
I1101 07:42:01.094004 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_33000.solverstate
I1101 07:42:01.214433 11248 solver.cpp:337] Iteration 33000, Testing net (#0)
I1101 07:42:06.130688 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:42:06.130688 11248 solver.cpp:404]     Test net output #1: loss = 0.013209 (* 1 = 0.013209 loss)
I1101 07:42:06.206178 11248 solver.cpp:228] Iteration 33000, loss = 0.00321157
I1101 07:42:06.206178 11248 solver.cpp:244]     Train net output #0: loss = 0.00321157 (* 1 = 0.00321157 loss)
I1101 07:42:06.206178 11248 sgd_solver.cpp:106] Iteration 33000, lr = 1e-006
I1101 07:42:22.640393 11248 solver.cpp:228] Iteration 33100, loss = 0.00390657
I1101 07:42:22.640393 11248 solver.cpp:244]     Train net output #0: loss = 0.00390657 (* 1 = 0.00390657 loss)
I1101 07:42:22.640393 11248 sgd_solver.cpp:106] Iteration 33100, lr = 1e-006
I1101 07:42:39.073554 11248 solver.cpp:228] Iteration 33200, loss = 0.00493578
I1101 07:42:39.073554 11248 solver.cpp:244]     Train net output #0: loss = 0.00493578 (* 1 = 0.00493578 loss)
I1101 07:42:39.073554 11248 sgd_solver.cpp:106] Iteration 33200, lr = 1e-006
I1101 07:42:55.495350 11248 solver.cpp:228] Iteration 33300, loss = 0.00449764
I1101 07:42:55.495350 11248 solver.cpp:244]     Train net output #0: loss = 0.00449764 (* 1 = 0.00449764 loss)
I1101 07:42:55.495350 11248 sgd_solver.cpp:106] Iteration 33300, lr = 1e-006
I1101 07:43:11.913943 11248 solver.cpp:228] Iteration 33400, loss = 0.00250595
I1101 07:43:11.913943 11248 solver.cpp:244]     Train net output #0: loss = 0.00250595 (* 1 = 0.00250595 loss)
I1101 07:43:11.913943 11248 sgd_solver.cpp:106] Iteration 33400, lr = 1e-006
I1101 07:43:28.409541 11248 solver.cpp:228] Iteration 33500, loss = 0.00330637
I1101 07:43:28.409541 11248 solver.cpp:244]     Train net output #0: loss = 0.00330637 (* 1 = 0.00330637 loss)
I1101 07:43:28.409541 11248 sgd_solver.cpp:106] Iteration 33500, lr = 1e-006
I1101 07:43:44.785316 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_33600.caffemodel
I1101 07:43:44.928001 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_33600.solverstate
I1101 07:43:45.021739 11248 solver.cpp:337] Iteration 33600, Testing net (#0)
I1101 07:43:49.951124 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:43:49.951124 11248 solver.cpp:404]     Test net output #1: loss = 0.0132089 (* 1 = 0.0132089 loss)
I1101 07:43:50.027289 11248 solver.cpp:228] Iteration 33600, loss = 0.00321153
I1101 07:43:50.027289 11248 solver.cpp:244]     Train net output #0: loss = 0.00321153 (* 1 = 0.00321153 loss)
I1101 07:43:50.027289 11248 sgd_solver.cpp:106] Iteration 33600, lr = 1e-006
I1101 07:44:06.449128 11248 solver.cpp:228] Iteration 33700, loss = 0.00390652
I1101 07:44:06.449128 11248 solver.cpp:244]     Train net output #0: loss = 0.00390652 (* 1 = 0.00390652 loss)
I1101 07:44:06.449128 11248 sgd_solver.cpp:106] Iteration 33700, lr = 1e-006
I1101 07:44:22.889906 11248 solver.cpp:228] Iteration 33800, loss = 0.00493569
I1101 07:44:22.889906 11248 solver.cpp:244]     Train net output #0: loss = 0.00493569 (* 1 = 0.00493569 loss)
I1101 07:44:22.889906 11248 sgd_solver.cpp:106] Iteration 33800, lr = 1e-006
I1101 07:44:39.315677 11248 solver.cpp:228] Iteration 33900, loss = 0.00449758
I1101 07:44:39.315677 11248 solver.cpp:244]     Train net output #0: loss = 0.00449758 (* 1 = 0.00449758 loss)
I1101 07:44:39.315677 11248 sgd_solver.cpp:106] Iteration 33900, lr = 1e-006
I1101 07:44:55.746912 11248 solver.cpp:228] Iteration 34000, loss = 0.00250593
I1101 07:44:55.746912 11248 solver.cpp:244]     Train net output #0: loss = 0.00250593 (* 1 = 0.00250593 loss)
I1101 07:44:55.746912 11248 sgd_solver.cpp:106] Iteration 34000, lr = 1e-006
I1101 07:45:12.187345 11248 solver.cpp:228] Iteration 34100, loss = 0.00330633
I1101 07:45:12.187345 11248 solver.cpp:244]     Train net output #0: loss = 0.00330633 (* 1 = 0.00330633 loss)
I1101 07:45:12.187345 11248 sgd_solver.cpp:106] Iteration 34100, lr = 1e-006
I1101 07:45:28.518828 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_34200.caffemodel
I1101 07:45:28.667435 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_34200.solverstate
I1101 07:45:28.761199 11248 solver.cpp:337] Iteration 34200, Testing net (#0)
I1101 07:45:33.678815 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:45:33.678815 11248 solver.cpp:404]     Test net output #1: loss = 0.0132089 (* 1 = 0.0132089 loss)
I1101 07:45:33.772568 11248 solver.cpp:228] Iteration 34200, loss = 0.00321148
I1101 07:45:33.772568 11248 solver.cpp:244]     Train net output #0: loss = 0.00321148 (* 1 = 0.00321148 loss)
I1101 07:45:33.772568 11248 sgd_solver.cpp:106] Iteration 34200, lr = 1e-006
I1101 07:45:50.199844 11248 solver.cpp:228] Iteration 34300, loss = 0.00390646
I1101 07:45:50.199844 11248 solver.cpp:244]     Train net output #0: loss = 0.00390646 (* 1 = 0.00390646 loss)
I1101 07:45:50.199844 11248 sgd_solver.cpp:106] Iteration 34300, lr = 1e-006
I1101 07:46:06.630869 11248 solver.cpp:228] Iteration 34400, loss = 0.00493562
I1101 07:46:06.630869 11248 solver.cpp:244]     Train net output #0: loss = 0.00493562 (* 1 = 0.00493562 loss)
I1101 07:46:06.630869 11248 sgd_solver.cpp:106] Iteration 34400, lr = 1e-006
I1101 07:46:23.054850 11248 solver.cpp:228] Iteration 34500, loss = 0.00449752
I1101 07:46:23.054850 11248 solver.cpp:244]     Train net output #0: loss = 0.00449752 (* 1 = 0.00449752 loss)
I1101 07:46:23.054850 11248 sgd_solver.cpp:106] Iteration 34500, lr = 1e-006
I1101 07:46:39.490692 11248 solver.cpp:228] Iteration 34600, loss = 0.0025059
I1101 07:46:39.490692 11248 solver.cpp:244]     Train net output #0: loss = 0.0025059 (* 1 = 0.0025059 loss)
I1101 07:46:39.490692 11248 sgd_solver.cpp:106] Iteration 34600, lr = 1e-006
I1101 07:46:55.921452 11248 solver.cpp:228] Iteration 34700, loss = 0.00330629
I1101 07:46:55.921452 11248 solver.cpp:244]     Train net output #0: loss = 0.00330629 (* 1 = 0.00330629 loss)
I1101 07:46:55.921452 11248 sgd_solver.cpp:106] Iteration 34700, lr = 1e-006
I1101 07:47:12.291286 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_34800.caffemodel
I1101 07:47:12.428500 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_34800.solverstate
I1101 07:47:12.534777 11248 solver.cpp:337] Iteration 34800, Testing net (#0)
I1101 07:47:17.462040 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:47:17.462040 11248 solver.cpp:404]     Test net output #1: loss = 0.0132089 (* 1 = 0.0132089 loss)
I1101 07:47:17.540202 11248 solver.cpp:228] Iteration 34800, loss = 0.00321143
I1101 07:47:17.540202 11248 solver.cpp:244]     Train net output #0: loss = 0.00321143 (* 1 = 0.00321143 loss)
I1101 07:47:17.540202 11248 sgd_solver.cpp:106] Iteration 34800, lr = 1e-006
I1101 07:47:33.967699 11248 solver.cpp:228] Iteration 34900, loss = 0.00390641
I1101 07:47:33.967699 11248 solver.cpp:244]     Train net output #0: loss = 0.00390641 (* 1 = 0.00390641 loss)
I1101 07:47:33.967699 11248 sgd_solver.cpp:106] Iteration 34900, lr = 1e-006
I1101 07:47:50.371675 11248 solver.cpp:228] Iteration 35000, loss = 0.00493554
I1101 07:47:50.371675 11248 solver.cpp:244]     Train net output #0: loss = 0.00493554 (* 1 = 0.00493554 loss)
I1101 07:47:50.371675 11248 sgd_solver.cpp:106] Iteration 35000, lr = 1e-006
I1101 07:48:06.811244 11248 solver.cpp:228] Iteration 35100, loss = 0.00449746
I1101 07:48:06.811244 11248 solver.cpp:244]     Train net output #0: loss = 0.00449746 (* 1 = 0.00449746 loss)
I1101 07:48:06.811244 11248 sgd_solver.cpp:106] Iteration 35100, lr = 1e-006
I1101 07:48:23.232348 11248 solver.cpp:228] Iteration 35200, loss = 0.00250588
I1101 07:48:23.232348 11248 solver.cpp:244]     Train net output #0: loss = 0.00250588 (* 1 = 0.00250588 loss)
I1101 07:48:23.232348 11248 sgd_solver.cpp:106] Iteration 35200, lr = 1e-006
I1101 07:48:39.677886 11248 solver.cpp:228] Iteration 35300, loss = 0.00330625
I1101 07:48:39.677886 11248 solver.cpp:244]     Train net output #0: loss = 0.00330625 (* 1 = 0.00330625 loss)
I1101 07:48:39.677886 11248 sgd_solver.cpp:106] Iteration 35300, lr = 1e-006
I1101 07:48:56.016391 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_35400.caffemodel
I1101 07:48:56.187381 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_35400.solverstate
I1101 07:48:56.282449 11248 solver.cpp:337] Iteration 35400, Testing net (#0)
I1101 07:49:01.240728 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:49:01.240728 11248 solver.cpp:404]     Test net output #1: loss = 0.0132088 (* 1 = 0.0132088 loss)
I1101 07:49:01.334482 11248 solver.cpp:228] Iteration 35400, loss = 0.00321138
I1101 07:49:01.334482 11248 solver.cpp:244]     Train net output #0: loss = 0.00321138 (* 1 = 0.00321138 loss)
I1101 07:49:01.334482 11248 sgd_solver.cpp:106] Iteration 35400, lr = 1e-006
I1101 07:49:17.759037 11248 solver.cpp:228] Iteration 35500, loss = 0.00390636
I1101 07:49:17.759037 11248 solver.cpp:244]     Train net output #0: loss = 0.00390636 (* 1 = 0.00390636 loss)
I1101 07:49:17.759037 11248 sgd_solver.cpp:106] Iteration 35500, lr = 1e-006
I1101 07:49:34.271566 11248 solver.cpp:228] Iteration 35600, loss = 0.00493546
I1101 07:49:34.271566 11248 solver.cpp:244]     Train net output #0: loss = 0.00493546 (* 1 = 0.00493546 loss)
I1101 07:49:34.271566 11248 sgd_solver.cpp:106] Iteration 35600, lr = 1e-006
I1101 07:49:50.781577 11248 solver.cpp:228] Iteration 35700, loss = 0.00449742
I1101 07:49:50.781577 11248 solver.cpp:244]     Train net output #0: loss = 0.00449742 (* 1 = 0.00449742 loss)
I1101 07:49:50.781577 11248 sgd_solver.cpp:106] Iteration 35700, lr = 1e-006
I1101 07:50:07.544510 11248 solver.cpp:228] Iteration 35800, loss = 0.00250585
I1101 07:50:07.544510 11248 solver.cpp:244]     Train net output #0: loss = 0.00250585 (* 1 = 0.00250585 loss)
I1101 07:50:07.544510 11248 sgd_solver.cpp:106] Iteration 35800, lr = 1e-006
I1101 07:50:24.072273 11248 solver.cpp:228] Iteration 35900, loss = 0.00330622
I1101 07:50:24.072273 11248 solver.cpp:244]     Train net output #0: loss = 0.00330622 (* 1 = 0.00330622 loss)
I1101 07:50:24.072273 11248 sgd_solver.cpp:106] Iteration 35900, lr = 1e-006
I1101 07:50:40.617388 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_36000.caffemodel
I1101 07:50:40.775501 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_36000.solverstate
I1101 07:50:40.873570 11248 solver.cpp:337] Iteration 36000, Testing net (#0)
I1101 07:50:45.878134 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:50:45.878134 11248 solver.cpp:404]     Test net output #1: loss = 0.0132088 (* 1 = 0.0132088 loss)
I1101 07:50:45.948184 11248 solver.cpp:228] Iteration 36000, loss = 0.00321134
I1101 07:50:45.948184 11248 solver.cpp:244]     Train net output #0: loss = 0.00321134 (* 1 = 0.00321134 loss)
I1101 07:50:45.948184 11248 sgd_solver.cpp:106] Iteration 36000, lr = 1e-006
I1101 07:51:02.466940 11248 solver.cpp:228] Iteration 36100, loss = 0.00390631
I1101 07:51:02.466940 11248 solver.cpp:244]     Train net output #0: loss = 0.00390631 (* 1 = 0.00390631 loss)
I1101 07:51:02.466940 11248 sgd_solver.cpp:106] Iteration 36100, lr = 1e-006
I1101 07:51:18.918400 11248 solver.cpp:228] Iteration 36200, loss = 0.00493539
I1101 07:51:18.918400 11248 solver.cpp:244]     Train net output #0: loss = 0.00493539 (* 1 = 0.00493539 loss)
I1101 07:51:18.918400 11248 sgd_solver.cpp:106] Iteration 36200, lr = 1e-006
I1101 07:51:35.412662 11248 solver.cpp:228] Iteration 36300, loss = 0.00449737
I1101 07:51:35.412662 11248 solver.cpp:244]     Train net output #0: loss = 0.00449737 (* 1 = 0.00449737 loss)
I1101 07:51:35.412662 11248 sgd_solver.cpp:106] Iteration 36300, lr = 1e-006
I1101 07:51:52.200028 11248 solver.cpp:228] Iteration 36400, loss = 0.00250583
I1101 07:51:52.200028 11248 solver.cpp:244]     Train net output #0: loss = 0.00250583 (* 1 = 0.00250583 loss)
I1101 07:51:52.200028 11248 sgd_solver.cpp:106] Iteration 36400, lr = 1e-006
I1101 07:52:09.583873 11248 solver.cpp:228] Iteration 36500, loss = 0.00330617
I1101 07:52:09.583873 11248 solver.cpp:244]     Train net output #0: loss = 0.00330617 (* 1 = 0.00330617 loss)
I1101 07:52:09.583873 11248 sgd_solver.cpp:106] Iteration 36500, lr = 1e-006
I1101 07:52:26.037133 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_36600.caffemodel
I1101 07:52:26.194247 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_36600.solverstate
I1101 07:52:26.322337 11248 solver.cpp:337] Iteration 36600, Testing net (#0)
I1101 07:52:31.297878 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:52:31.297878 11248 solver.cpp:404]     Test net output #1: loss = 0.0132088 (* 1 = 0.0132088 loss)
I1101 07:52:31.366927 11248 solver.cpp:228] Iteration 36600, loss = 0.0032113
I1101 07:52:31.366927 11248 solver.cpp:244]     Train net output #0: loss = 0.0032113 (* 1 = 0.0032113 loss)
I1101 07:52:31.366927 11248 sgd_solver.cpp:106] Iteration 36600, lr = 1e-006
I1101 07:52:47.901382 11248 solver.cpp:228] Iteration 36700, loss = 0.00390625
I1101 07:52:47.901382 11248 solver.cpp:244]     Train net output #0: loss = 0.00390625 (* 1 = 0.00390625 loss)
I1101 07:52:47.901382 11248 sgd_solver.cpp:106] Iteration 36700, lr = 1e-006
I1101 07:53:04.370134 11248 solver.cpp:228] Iteration 36800, loss = 0.00493531
I1101 07:53:04.370134 11248 solver.cpp:244]     Train net output #0: loss = 0.00493531 (* 1 = 0.00493531 loss)
I1101 07:53:04.370134 11248 sgd_solver.cpp:106] Iteration 36800, lr = 1e-006
I1101 07:53:20.883553 11248 solver.cpp:228] Iteration 36900, loss = 0.00449731
I1101 07:53:20.883553 11248 solver.cpp:244]     Train net output #0: loss = 0.00449731 (* 1 = 0.00449731 loss)
I1101 07:53:20.883553 11248 sgd_solver.cpp:106] Iteration 36900, lr = 1e-006
I1101 07:53:37.357286 11248 solver.cpp:228] Iteration 37000, loss = 0.00250582
I1101 07:53:37.357286 11248 solver.cpp:244]     Train net output #0: loss = 0.00250582 (* 1 = 0.00250582 loss)
I1101 07:53:37.357286 11248 sgd_solver.cpp:46] MultiStep Status: Iteration 37000, step = 6
I1101 07:53:37.357286 11248 sgd_solver.cpp:106] Iteration 37000, lr = 1e-007
I1101 07:53:53.812014 11248 solver.cpp:228] Iteration 37100, loss = 0.00330614
I1101 07:53:53.812014 11248 solver.cpp:244]     Train net output #0: loss = 0.00330614 (* 1 = 0.00330614 loss)
I1101 07:53:53.812014 11248 sgd_solver.cpp:106] Iteration 37100, lr = 1e-007
I1101 07:54:10.146391 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_37200.caffemodel
I1101 07:54:10.294534 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_37200.solverstate
I1101 07:54:10.388284 11248 solver.cpp:337] Iteration 37200, Testing net (#0)
I1101 07:54:15.326673 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:54:15.326673 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 07:54:15.404805 11248 solver.cpp:228] Iteration 37200, loss = 0.00321127
I1101 07:54:15.404805 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 07:54:15.404805 11248 sgd_solver.cpp:106] Iteration 37200, lr = 1e-007
I1101 07:54:31.826056 11248 solver.cpp:228] Iteration 37300, loss = 0.00390615
I1101 07:54:31.826056 11248 solver.cpp:244]     Train net output #0: loss = 0.00390615 (* 1 = 0.00390615 loss)
I1101 07:54:31.826056 11248 sgd_solver.cpp:106] Iteration 37300, lr = 1e-007
I1101 07:54:48.243011 11248 solver.cpp:228] Iteration 37400, loss = 0.0049352
I1101 07:54:48.243011 11248 solver.cpp:244]     Train net output #0: loss = 0.0049352 (* 1 = 0.0049352 loss)
I1101 07:54:48.243011 11248 sgd_solver.cpp:106] Iteration 37400, lr = 1e-007
I1101 07:55:04.688170 11248 solver.cpp:228] Iteration 37500, loss = 0.00449718
I1101 07:55:04.688170 11248 solver.cpp:244]     Train net output #0: loss = 0.00449718 (* 1 = 0.00449718 loss)
I1101 07:55:04.688170 11248 sgd_solver.cpp:106] Iteration 37500, lr = 1e-007
I1101 07:55:21.115576 11248 solver.cpp:228] Iteration 37600, loss = 0.00250581
I1101 07:55:21.115576 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 07:55:21.115576 11248 sgd_solver.cpp:106] Iteration 37600, lr = 1e-007
I1101 07:55:37.532032 11248 solver.cpp:228] Iteration 37700, loss = 0.00330614
I1101 07:55:37.532032 11248 solver.cpp:244]     Train net output #0: loss = 0.00330614 (* 1 = 0.00330614 loss)
I1101 07:55:37.532032 11248 sgd_solver.cpp:106] Iteration 37700, lr = 1e-007
I1101 07:55:53.862193 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_37800.caffemodel
I1101 07:55:54.021944 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_37800.solverstate
I1101 07:55:54.117522 11248 solver.cpp:337] Iteration 37800, Testing net (#0)
I1101 07:55:59.029489 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:55:59.029489 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 07:55:59.107620 11248 solver.cpp:228] Iteration 37800, loss = 0.00321127
I1101 07:55:59.107620 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 07:55:59.107620 11248 sgd_solver.cpp:106] Iteration 37800, lr = 1e-007
I1101 07:56:15.548075 11248 solver.cpp:228] Iteration 37900, loss = 0.00390615
I1101 07:56:15.548075 11248 solver.cpp:244]     Train net output #0: loss = 0.00390614 (* 1 = 0.00390614 loss)
I1101 07:56:15.548075 11248 sgd_solver.cpp:106] Iteration 37900, lr = 1e-007
I1101 07:56:31.971659 11248 solver.cpp:228] Iteration 38000, loss = 0.0049352
I1101 07:56:31.971659 11248 solver.cpp:244]     Train net output #0: loss = 0.0049352 (* 1 = 0.0049352 loss)
I1101 07:56:31.971659 11248 sgd_solver.cpp:106] Iteration 38000, lr = 1e-007
I1101 07:56:48.650259 11248 solver.cpp:228] Iteration 38100, loss = 0.00449717
I1101 07:56:48.650259 11248 solver.cpp:244]     Train net output #0: loss = 0.00449717 (* 1 = 0.00449717 loss)
I1101 07:56:48.650259 11248 sgd_solver.cpp:106] Iteration 38100, lr = 1e-007
I1101 07:57:05.258433 11248 solver.cpp:228] Iteration 38200, loss = 0.00250581
I1101 07:57:05.258433 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 07:57:05.258433 11248 sgd_solver.cpp:106] Iteration 38200, lr = 1e-007
I1101 07:57:21.728539 11248 solver.cpp:228] Iteration 38300, loss = 0.00330614
I1101 07:57:21.728539 11248 solver.cpp:244]     Train net output #0: loss = 0.00330614 (* 1 = 0.00330614 loss)
I1101 07:57:21.728539 11248 sgd_solver.cpp:106] Iteration 38300, lr = 1e-007
I1101 07:57:38.217854 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_38400.caffemodel
I1101 07:57:38.375161 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_38400.solverstate
I1101 07:57:38.468912 11248 solver.cpp:337] Iteration 38400, Testing net (#0)
I1101 07:57:43.426491 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:57:43.426491 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 07:57:43.497370 11248 solver.cpp:228] Iteration 38400, loss = 0.00321127
I1101 07:57:43.497370 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 07:57:43.497370 11248 sgd_solver.cpp:106] Iteration 38400, lr = 1e-007
I1101 07:58:00.040345 11248 solver.cpp:228] Iteration 38500, loss = 0.00390614
I1101 07:58:00.040345 11248 solver.cpp:244]     Train net output #0: loss = 0.00390614 (* 1 = 0.00390614 loss)
I1101 07:58:00.040345 11248 sgd_solver.cpp:106] Iteration 38500, lr = 1e-007
I1101 07:58:16.578738 11248 solver.cpp:228] Iteration 38600, loss = 0.0049352
I1101 07:58:16.578738 11248 solver.cpp:244]     Train net output #0: loss = 0.0049352 (* 1 = 0.0049352 loss)
I1101 07:58:16.578738 11248 sgd_solver.cpp:106] Iteration 38600, lr = 1e-007
I1101 07:58:33.100247 11248 solver.cpp:228] Iteration 38700, loss = 0.00449717
I1101 07:58:33.100247 11248 solver.cpp:244]     Train net output #0: loss = 0.00449717 (* 1 = 0.00449717 loss)
I1101 07:58:33.100247 11248 sgd_solver.cpp:106] Iteration 38700, lr = 1e-007
I1101 07:58:49.591725 11248 solver.cpp:228] Iteration 38800, loss = 0.00250582
I1101 07:58:49.591725 11248 solver.cpp:244]     Train net output #0: loss = 0.00250582 (* 1 = 0.00250582 loss)
I1101 07:58:49.591725 11248 sgd_solver.cpp:106] Iteration 38800, lr = 1e-007
I1101 07:59:06.054170 11248 solver.cpp:228] Iteration 38900, loss = 0.00330613
I1101 07:59:06.054170 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 07:59:06.054170 11248 sgd_solver.cpp:106] Iteration 38900, lr = 1e-007
I1101 07:59:22.472180 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_39000.caffemodel
I1101 07:59:22.621456 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_39000.solverstate
I1101 07:59:22.729218 11248 solver.cpp:337] Iteration 39000, Testing net (#0)
I1101 07:59:27.660158 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 07:59:27.660158 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 07:59:27.753912 11248 solver.cpp:228] Iteration 39000, loss = 0.00321127
I1101 07:59:27.753912 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 07:59:27.753912 11248 sgd_solver.cpp:106] Iteration 39000, lr = 1e-007
I1101 07:59:44.177006 11248 solver.cpp:228] Iteration 39100, loss = 0.00390614
I1101 07:59:44.177006 11248 solver.cpp:244]     Train net output #0: loss = 0.00390614 (* 1 = 0.00390614 loss)
I1101 07:59:44.177006 11248 sgd_solver.cpp:106] Iteration 39100, lr = 1e-007
I1101 08:00:00.710739 11248 solver.cpp:228] Iteration 39200, loss = 0.00493519
I1101 08:00:00.710739 11248 solver.cpp:244]     Train net output #0: loss = 0.00493519 (* 1 = 0.00493519 loss)
I1101 08:00:00.710739 11248 sgd_solver.cpp:106] Iteration 39200, lr = 1e-007
I1101 08:00:17.319392 11248 solver.cpp:228] Iteration 39300, loss = 0.00449716
I1101 08:00:17.319392 11248 solver.cpp:244]     Train net output #0: loss = 0.00449716 (* 1 = 0.00449716 loss)
I1101 08:00:17.319392 11248 sgd_solver.cpp:106] Iteration 39300, lr = 1e-007
I1101 08:00:33.794111 11248 solver.cpp:228] Iteration 39400, loss = 0.00250581
I1101 08:00:33.794111 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:00:33.794111 11248 sgd_solver.cpp:106] Iteration 39400, lr = 1e-007
I1101 08:00:50.319820 11248 solver.cpp:228] Iteration 39500, loss = 0.00330613
I1101 08:00:50.319820 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 08:00:50.319820 11248 sgd_solver.cpp:106] Iteration 39500, lr = 1e-007
I1101 08:01:06.725011 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_39600.caffemodel
I1101 08:01:06.884625 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_39600.solverstate
I1101 08:01:06.983696 11248 solver.cpp:337] Iteration 39600, Testing net (#0)
I1101 08:01:11.924463 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:01:11.924463 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:01:12.018215 11248 solver.cpp:228] Iteration 39600, loss = 0.00321127
I1101 08:01:12.018215 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 08:01:12.018215 11248 sgd_solver.cpp:106] Iteration 39600, lr = 1e-007
I1101 08:01:28.527722 11248 solver.cpp:228] Iteration 39700, loss = 0.00390613
I1101 08:01:28.527722 11248 solver.cpp:244]     Train net output #0: loss = 0.00390613 (* 1 = 0.00390613 loss)
I1101 08:01:28.527722 11248 sgd_solver.cpp:106] Iteration 39700, lr = 1e-007
I1101 08:01:45.085700 11248 solver.cpp:228] Iteration 39800, loss = 0.00493519
I1101 08:01:45.085700 11248 solver.cpp:244]     Train net output #0: loss = 0.00493519 (* 1 = 0.00493519 loss)
I1101 08:01:45.085700 11248 sgd_solver.cpp:106] Iteration 39800, lr = 1e-007
I1101 08:02:01.625560 11248 solver.cpp:228] Iteration 39900, loss = 0.00449715
I1101 08:02:01.625560 11248 solver.cpp:244]     Train net output #0: loss = 0.00449715 (* 1 = 0.00449715 loss)
I1101 08:02:01.625560 11248 sgd_solver.cpp:106] Iteration 39900, lr = 1e-007
I1101 08:02:17.989506 11248 solver.cpp:228] Iteration 40000, loss = 0.00250581
I1101 08:02:17.989506 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:02:17.989506 11248 sgd_solver.cpp:106] Iteration 40000, lr = 1e-007
I1101 08:02:34.498250 11248 solver.cpp:228] Iteration 40100, loss = 0.00330613
I1101 08:02:34.498250 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 08:02:34.498250 11248 sgd_solver.cpp:106] Iteration 40100, lr = 1e-007
I1101 08:02:50.933970 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_40200.caffemodel
I1101 08:02:51.083567 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_40200.solverstate
I1101 08:02:51.191421 11248 solver.cpp:337] Iteration 40200, Testing net (#0)
I1101 08:02:56.122373 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:02:56.122373 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:02:56.195190 11248 solver.cpp:228] Iteration 40200, loss = 0.00321127
I1101 08:02:56.195190 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 08:02:56.195190 11248 sgd_solver.cpp:106] Iteration 40200, lr = 1e-007
I1101 08:03:12.816795 11248 solver.cpp:228] Iteration 40300, loss = 0.00390613
I1101 08:03:12.816795 11248 solver.cpp:244]     Train net output #0: loss = 0.00390613 (* 1 = 0.00390613 loss)
I1101 08:03:12.816795 11248 sgd_solver.cpp:106] Iteration 40300, lr = 1e-007
I1101 08:03:29.476586 11248 solver.cpp:228] Iteration 40400, loss = 0.00493519
I1101 08:03:29.476586 11248 solver.cpp:244]     Train net output #0: loss = 0.00493519 (* 1 = 0.00493519 loss)
I1101 08:03:29.476586 11248 sgd_solver.cpp:106] Iteration 40400, lr = 1e-007
I1101 08:03:45.493288 11248 solver.cpp:228] Iteration 40500, loss = 0.00449714
I1101 08:03:45.493288 11248 solver.cpp:244]     Train net output #0: loss = 0.00449715 (* 1 = 0.00449715 loss)
I1101 08:03:45.493788 11248 sgd_solver.cpp:106] Iteration 40500, lr = 1e-007
I1101 08:04:01.210510 11248 solver.cpp:228] Iteration 40600, loss = 0.00250581
I1101 08:04:01.210510 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:04:01.210510 11248 sgd_solver.cpp:106] Iteration 40600, lr = 1e-007
I1101 08:04:17.021129 11248 solver.cpp:228] Iteration 40700, loss = 0.00330613
I1101 08:04:17.021129 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 08:04:17.021129 11248 sgd_solver.cpp:106] Iteration 40700, lr = 1e-007
I1101 08:04:32.842552 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_40800.caffemodel
I1101 08:04:33.108741 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_40800.solverstate
I1101 08:04:33.293874 11248 solver.cpp:337] Iteration 40800, Testing net (#0)
I1101 08:04:38.101155 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:04:38.101155 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:04:38.177172 11248 solver.cpp:228] Iteration 40800, loss = 0.00321127
I1101 08:04:38.177172 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 08:04:38.177172 11248 sgd_solver.cpp:106] Iteration 40800, lr = 1e-007
I1101 08:04:54.069608 11248 solver.cpp:228] Iteration 40900, loss = 0.00390613
I1101 08:04:54.069608 11248 solver.cpp:244]     Train net output #0: loss = 0.00390613 (* 1 = 0.00390613 loss)
I1101 08:04:54.069608 11248 sgd_solver.cpp:106] Iteration 40900, lr = 1e-007
I1101 08:05:10.492421 11248 solver.cpp:228] Iteration 41000, loss = 0.00493519
I1101 08:05:10.492421 11248 solver.cpp:244]     Train net output #0: loss = 0.00493519 (* 1 = 0.00493519 loss)
I1101 08:05:10.492421 11248 sgd_solver.cpp:106] Iteration 41000, lr = 1e-007
I1101 08:05:27.015756 11248 solver.cpp:228] Iteration 41100, loss = 0.00449714
I1101 08:05:27.015756 11248 solver.cpp:244]     Train net output #0: loss = 0.00449714 (* 1 = 0.00449714 loss)
I1101 08:05:27.015756 11248 sgd_solver.cpp:106] Iteration 41100, lr = 1e-007
I1101 08:05:43.784487 11248 solver.cpp:228] Iteration 41200, loss = 0.00250581
I1101 08:05:43.784487 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:05:43.784487 11248 sgd_solver.cpp:106] Iteration 41200, lr = 1e-007
I1101 08:06:01.010514 11248 solver.cpp:228] Iteration 41300, loss = 0.00330613
I1101 08:06:01.010514 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 08:06:01.010514 11248 sgd_solver.cpp:106] Iteration 41300, lr = 1e-007
I1101 08:06:18.072402 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_41400.caffemodel
I1101 08:06:18.284557 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_41400.solverstate
I1101 08:06:18.404642 11248 solver.cpp:337] Iteration 41400, Testing net (#0)
I1101 08:06:23.488044 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:06:23.488044 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:06:23.558714 11248 solver.cpp:228] Iteration 41400, loss = 0.00321127
I1101 08:06:23.558714 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 08:06:23.558714 11248 sgd_solver.cpp:106] Iteration 41400, lr = 1e-007
I1101 08:06:40.774022 11248 solver.cpp:228] Iteration 41500, loss = 0.00390613
I1101 08:06:40.774022 11248 solver.cpp:244]     Train net output #0: loss = 0.00390613 (* 1 = 0.00390613 loss)
I1101 08:06:40.774022 11248 sgd_solver.cpp:106] Iteration 41500, lr = 1e-007
I1101 08:06:57.997285 11248 solver.cpp:228] Iteration 41600, loss = 0.00493518
I1101 08:06:57.997788 11248 solver.cpp:244]     Train net output #0: loss = 0.00493518 (* 1 = 0.00493518 loss)
I1101 08:06:57.997788 11248 sgd_solver.cpp:106] Iteration 41600, lr = 1e-007
I1101 08:07:15.007719 11248 solver.cpp:228] Iteration 41700, loss = 0.00449713
I1101 08:07:15.007719 11248 solver.cpp:244]     Train net output #0: loss = 0.00449713 (* 1 = 0.00449713 loss)
I1101 08:07:15.007719 11248 sgd_solver.cpp:106] Iteration 41700, lr = 1e-007
I1101 08:07:31.678541 11248 solver.cpp:228] Iteration 41800, loss = 0.00250581
I1101 08:07:31.678541 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:07:31.678541 11248 sgd_solver.cpp:106] Iteration 41800, lr = 1e-007
I1101 08:07:48.625048 11248 solver.cpp:228] Iteration 41900, loss = 0.00330613
I1101 08:07:48.625048 11248 solver.cpp:244]     Train net output #0: loss = 0.00330612 (* 1 = 0.00330612 loss)
I1101 08:07:48.625048 11248 sgd_solver.cpp:106] Iteration 41900, lr = 1e-007
I1101 08:08:05.649140 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_42000.caffemodel
I1101 08:08:05.810237 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_42000.solverstate
I1101 08:08:05.910308 11248 solver.cpp:337] Iteration 42000, Testing net (#0)
I1101 08:08:10.991195 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:08:10.991195 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:08:11.062587 11248 solver.cpp:228] Iteration 42000, loss = 0.00321127
I1101 08:08:11.062587 11248 solver.cpp:244]     Train net output #0: loss = 0.00321127 (* 1 = 0.00321127 loss)
I1101 08:08:11.062587 11248 sgd_solver.cpp:106] Iteration 42000, lr = 1e-007
I1101 08:08:28.002384 11248 solver.cpp:228] Iteration 42100, loss = 0.00390613
I1101 08:08:28.002384 11248 solver.cpp:244]     Train net output #0: loss = 0.00390613 (* 1 = 0.00390613 loss)
I1101 08:08:28.002384 11248 sgd_solver.cpp:106] Iteration 42100, lr = 1e-007
I1101 08:08:44.987962 11248 solver.cpp:228] Iteration 42200, loss = 0.00493518
I1101 08:08:44.987962 11248 solver.cpp:244]     Train net output #0: loss = 0.00493518 (* 1 = 0.00493518 loss)
I1101 08:08:44.987962 11248 sgd_solver.cpp:106] Iteration 42200, lr = 1e-007
I1101 08:09:01.999008 11248 solver.cpp:228] Iteration 42300, loss = 0.00449712
I1101 08:09:01.999008 11248 solver.cpp:244]     Train net output #0: loss = 0.00449712 (* 1 = 0.00449712 loss)
I1101 08:09:01.999008 11248 sgd_solver.cpp:106] Iteration 42300, lr = 1e-007
I1101 08:09:19.069444 11248 solver.cpp:228] Iteration 42400, loss = 0.00250581
I1101 08:09:19.069444 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:09:19.069444 11248 sgd_solver.cpp:106] Iteration 42400, lr = 1e-007
I1101 08:09:36.042294 11248 solver.cpp:228] Iteration 42500, loss = 0.00330612
I1101 08:09:36.042294 11248 solver.cpp:244]     Train net output #0: loss = 0.00330612 (* 1 = 0.00330612 loss)
I1101 08:09:36.042294 11248 sgd_solver.cpp:106] Iteration 42500, lr = 1e-007
I1101 08:09:52.903420 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_42600.caffemodel
I1101 08:09:53.070683 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_42600.solverstate
I1101 08:09:53.263993 11248 solver.cpp:337] Iteration 42600, Testing net (#0)
I1101 08:09:58.353144 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:09:58.353144 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:09:58.425520 11248 solver.cpp:228] Iteration 42600, loss = 0.00321127
I1101 08:09:58.425520 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:09:58.425520 11248 sgd_solver.cpp:106] Iteration 42600, lr = 1e-007
I1101 08:10:15.425566 11248 solver.cpp:228] Iteration 42700, loss = 0.00390612
I1101 08:10:15.425566 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:10:15.425566 11248 sgd_solver.cpp:106] Iteration 42700, lr = 1e-007
I1101 08:10:32.820130 11248 solver.cpp:228] Iteration 42800, loss = 0.00493518
I1101 08:10:32.820628 11248 solver.cpp:244]     Train net output #0: loss = 0.00493518 (* 1 = 0.00493518 loss)
I1101 08:10:32.820628 11248 sgd_solver.cpp:106] Iteration 42800, lr = 1e-007
I1101 08:10:49.971658 11248 solver.cpp:228] Iteration 42900, loss = 0.00449712
I1101 08:10:49.971658 11248 solver.cpp:244]     Train net output #0: loss = 0.00449712 (* 1 = 0.00449712 loss)
I1101 08:10:49.971658 11248 sgd_solver.cpp:106] Iteration 42900, lr = 1e-007
I1101 08:11:07.058917 11248 solver.cpp:228] Iteration 43000, loss = 0.00250582
I1101 08:11:07.058917 11248 solver.cpp:244]     Train net output #0: loss = 0.00250582 (* 1 = 0.00250582 loss)
I1101 08:11:07.058917 11248 sgd_solver.cpp:106] Iteration 43000, lr = 1e-007
I1101 08:11:24.281627 11248 solver.cpp:228] Iteration 43100, loss = 0.00330612
I1101 08:11:24.281627 11248 solver.cpp:244]     Train net output #0: loss = 0.00330612 (* 1 = 0.00330612 loss)
I1101 08:11:24.281627 11248 sgd_solver.cpp:106] Iteration 43100, lr = 1e-007
I1101 08:11:41.474968 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_43200.caffemodel
I1101 08:11:41.634083 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_43200.solverstate
I1101 08:11:41.733152 11248 solver.cpp:337] Iteration 43200, Testing net (#0)
I1101 08:11:46.731289 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:11:46.731289 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:11:46.801070 11248 solver.cpp:228] Iteration 43200, loss = 0.00321126
I1101 08:11:46.801070 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:11:46.801070 11248 sgd_solver.cpp:106] Iteration 43200, lr = 1e-007
I1101 08:12:03.531570 11248 solver.cpp:228] Iteration 43300, loss = 0.00390612
I1101 08:12:03.531570 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:12:03.531570 11248 sgd_solver.cpp:106] Iteration 43300, lr = 1e-007
I1101 08:12:20.178807 11248 solver.cpp:228] Iteration 43400, loss = 0.00493517
I1101 08:12:20.178807 11248 solver.cpp:244]     Train net output #0: loss = 0.00493517 (* 1 = 0.00493517 loss)
I1101 08:12:20.178807 11248 sgd_solver.cpp:106] Iteration 43400, lr = 1e-007
I1101 08:12:37.160928 11248 solver.cpp:228] Iteration 43500, loss = 0.00449711
I1101 08:12:37.160928 11248 solver.cpp:244]     Train net output #0: loss = 0.00449711 (* 1 = 0.00449711 loss)
I1101 08:12:37.160928 11248 sgd_solver.cpp:106] Iteration 43500, lr = 1e-007
I1101 08:12:54.343307 11248 solver.cpp:228] Iteration 43600, loss = 0.00250582
I1101 08:12:54.343307 11248 solver.cpp:244]     Train net output #0: loss = 0.00250582 (* 1 = 0.00250582 loss)
I1101 08:12:54.343307 11248 sgd_solver.cpp:106] Iteration 43600, lr = 1e-007
I1101 08:13:11.275427 11248 solver.cpp:228] Iteration 43700, loss = 0.00330612
I1101 08:13:11.275427 11248 solver.cpp:244]     Train net output #0: loss = 0.00330613 (* 1 = 0.00330613 loss)
I1101 08:13:11.275427 11248 sgd_solver.cpp:106] Iteration 43700, lr = 1e-007
I1101 08:13:28.144588 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_43800.caffemodel
I1101 08:13:28.330127 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_43800.solverstate
I1101 08:13:28.431700 11248 solver.cpp:337] Iteration 43800, Testing net (#0)
I1101 08:13:33.515084 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:13:33.515084 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:13:33.587498 11248 solver.cpp:228] Iteration 43800, loss = 0.00321126
I1101 08:13:33.587498 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:13:33.587498 11248 sgd_solver.cpp:106] Iteration 43800, lr = 1e-007
I1101 08:13:50.561326 11248 solver.cpp:228] Iteration 43900, loss = 0.00390612
I1101 08:13:50.561326 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:13:50.561326 11248 sgd_solver.cpp:106] Iteration 43900, lr = 1e-007
I1101 08:14:07.549253 11248 solver.cpp:228] Iteration 44000, loss = 0.00493517
I1101 08:14:07.549253 11248 solver.cpp:244]     Train net output #0: loss = 0.00493517 (* 1 = 0.00493517 loss)
I1101 08:14:07.549253 11248 sgd_solver.cpp:106] Iteration 44000, lr = 1e-007
I1101 08:14:24.534435 11248 solver.cpp:228] Iteration 44100, loss = 0.00449711
I1101 08:14:24.534435 11248 solver.cpp:244]     Train net output #0: loss = 0.00449711 (* 1 = 0.00449711 loss)
I1101 08:14:24.534435 11248 sgd_solver.cpp:106] Iteration 44100, lr = 1e-007
I1101 08:14:41.475787 11248 solver.cpp:228] Iteration 44200, loss = 0.00250581
I1101 08:14:41.476289 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:14:41.476289 11248 sgd_solver.cpp:106] Iteration 44200, lr = 1e-007
I1101 08:14:58.859796 11248 solver.cpp:228] Iteration 44300, loss = 0.00330612
I1101 08:14:58.859796 11248 solver.cpp:244]     Train net output #0: loss = 0.00330612 (* 1 = 0.00330612 loss)
I1101 08:14:58.859796 11248 sgd_solver.cpp:106] Iteration 44300, lr = 1e-007
I1101 08:15:16.426961 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_44400.caffemodel
I1101 08:15:16.709664 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_44400.solverstate
I1101 08:15:16.863773 11248 solver.cpp:337] Iteration 44400, Testing net (#0)
I1101 08:15:22.051594 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:15:22.051594 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:15:22.123330 11248 solver.cpp:228] Iteration 44400, loss = 0.00321125
I1101 08:15:22.123330 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:15:22.123330 11248 sgd_solver.cpp:106] Iteration 44400, lr = 1e-007
I1101 08:15:39.221837 11248 solver.cpp:228] Iteration 44500, loss = 0.00390612
I1101 08:15:39.221837 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:15:39.221837 11248 sgd_solver.cpp:106] Iteration 44500, lr = 1e-007
I1101 08:15:56.184437 11248 solver.cpp:228] Iteration 44600, loss = 0.00493517
I1101 08:15:56.184437 11248 solver.cpp:244]     Train net output #0: loss = 0.00493517 (* 1 = 0.00493517 loss)
I1101 08:15:56.184437 11248 sgd_solver.cpp:106] Iteration 44600, lr = 1e-007
I1101 08:16:13.060111 11248 solver.cpp:228] Iteration 44700, loss = 0.0044971
I1101 08:16:13.060111 11248 solver.cpp:244]     Train net output #0: loss = 0.0044971 (* 1 = 0.0044971 loss)
I1101 08:16:13.060111 11248 sgd_solver.cpp:106] Iteration 44700, lr = 1e-007
I1101 08:16:29.949336 11248 solver.cpp:228] Iteration 44800, loss = 0.00250581
I1101 08:16:29.949336 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:16:29.949336 11248 sgd_solver.cpp:106] Iteration 44800, lr = 1e-007
I1101 08:16:46.834426 11248 solver.cpp:228] Iteration 44900, loss = 0.00330612
I1101 08:16:46.834426 11248 solver.cpp:244]     Train net output #0: loss = 0.00330612 (* 1 = 0.00330612 loss)
I1101 08:16:46.834426 11248 sgd_solver.cpp:106] Iteration 44900, lr = 1e-007
I1101 08:17:03.679333 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_45000.caffemodel
I1101 08:17:03.854459 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_45000.solverstate
I1101 08:17:03.962034 11248 solver.cpp:337] Iteration 45000, Testing net (#0)
I1101 08:17:09.007848 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:17:09.007848 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:17:09.079476 11248 solver.cpp:228] Iteration 45000, loss = 0.00321125
I1101 08:17:09.079476 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:17:09.079476 11248 sgd_solver.cpp:106] Iteration 45000, lr = 1e-007
I1101 08:17:25.967110 11248 solver.cpp:228] Iteration 45100, loss = 0.00390612
I1101 08:17:25.967110 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:17:25.967110 11248 sgd_solver.cpp:106] Iteration 45100, lr = 1e-007
I1101 08:17:42.832922 11248 solver.cpp:228] Iteration 45200, loss = 0.00493516
I1101 08:17:42.833423 11248 solver.cpp:244]     Train net output #0: loss = 0.00493516 (* 1 = 0.00493516 loss)
I1101 08:17:42.833423 11248 sgd_solver.cpp:106] Iteration 45200, lr = 1e-007
I1101 08:17:59.728948 11248 solver.cpp:228] Iteration 45300, loss = 0.00449709
I1101 08:17:59.728948 11248 solver.cpp:244]     Train net output #0: loss = 0.00449709 (* 1 = 0.00449709 loss)
I1101 08:17:59.728948 11248 sgd_solver.cpp:106] Iteration 45300, lr = 1e-007
I1101 08:18:16.640486 11248 solver.cpp:228] Iteration 45400, loss = 0.00250581
I1101 08:18:16.640986 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:18:16.640986 11248 sgd_solver.cpp:106] Iteration 45400, lr = 1e-007
I1101 08:18:33.530871 11248 solver.cpp:228] Iteration 45500, loss = 0.00330611
I1101 08:18:33.530871 11248 solver.cpp:244]     Train net output #0: loss = 0.00330611 (* 1 = 0.00330611 loss)
I1101 08:18:33.530871 11248 sgd_solver.cpp:106] Iteration 45500, lr = 1e-007
I1101 08:18:50.359478 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_45600.caffemodel
I1101 08:18:50.532605 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_45600.solverstate
I1101 08:18:50.642683 11248 solver.cpp:337] Iteration 45600, Testing net (#0)
I1101 08:18:55.661190 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:18:55.661190 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:18:55.732024 11248 solver.cpp:228] Iteration 45600, loss = 0.00321126
I1101 08:18:55.732024 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:18:55.732024 11248 sgd_solver.cpp:106] Iteration 45600, lr = 1e-007
I1101 08:19:12.626865 11248 solver.cpp:228] Iteration 45700, loss = 0.00390612
I1101 08:19:12.626865 11248 solver.cpp:244]     Train net output #0: loss = 0.00390612 (* 1 = 0.00390612 loss)
I1101 08:19:12.626865 11248 sgd_solver.cpp:106] Iteration 45700, lr = 1e-007
I1101 08:19:29.508708 11248 solver.cpp:228] Iteration 45800, loss = 0.00493516
I1101 08:19:29.508708 11248 solver.cpp:244]     Train net output #0: loss = 0.00493516 (* 1 = 0.00493516 loss)
I1101 08:19:29.508708 11248 sgd_solver.cpp:106] Iteration 45800, lr = 1e-007
I1101 08:19:46.420851 11248 solver.cpp:228] Iteration 45900, loss = 0.00449709
I1101 08:19:46.420851 11248 solver.cpp:244]     Train net output #0: loss = 0.00449709 (* 1 = 0.00449709 loss)
I1101 08:19:46.420851 11248 sgd_solver.cpp:106] Iteration 45900, lr = 1e-007
I1101 08:20:03.362239 11248 solver.cpp:228] Iteration 46000, loss = 0.00250581
I1101 08:20:03.362239 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:20:03.362239 11248 sgd_solver.cpp:106] Iteration 46000, lr = 1e-007
I1101 08:20:20.266803 11248 solver.cpp:228] Iteration 46100, loss = 0.00330611
I1101 08:20:20.266803 11248 solver.cpp:244]     Train net output #0: loss = 0.00330611 (* 1 = 0.00330611 loss)
I1101 08:20:20.266803 11248 sgd_solver.cpp:106] Iteration 46100, lr = 1e-007
I1101 08:20:37.082820 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_46200.caffemodel
I1101 08:20:37.265452 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_46200.solverstate
I1101 08:20:37.376531 11248 solver.cpp:337] Iteration 46200, Testing net (#0)
I1101 08:20:42.396631 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:20:42.396631 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:20:42.467948 11248 solver.cpp:228] Iteration 46200, loss = 0.00321126
I1101 08:20:42.467948 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:20:42.467948 11248 sgd_solver.cpp:106] Iteration 46200, lr = 1e-007
I1101 08:20:59.310286 11248 solver.cpp:228] Iteration 46300, loss = 0.00390611
I1101 08:20:59.310286 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:20:59.310286 11248 sgd_solver.cpp:106] Iteration 46300, lr = 1e-007
I1101 08:21:16.237695 11248 solver.cpp:228] Iteration 46400, loss = 0.00493516
I1101 08:21:16.237695 11248 solver.cpp:244]     Train net output #0: loss = 0.00493516 (* 1 = 0.00493516 loss)
I1101 08:21:16.237695 11248 sgd_solver.cpp:106] Iteration 46400, lr = 1e-007
I1101 08:21:33.163766 11248 solver.cpp:228] Iteration 46500, loss = 0.00449708
I1101 08:21:33.163766 11248 solver.cpp:244]     Train net output #0: loss = 0.00449708 (* 1 = 0.00449708 loss)
I1101 08:21:33.163766 11248 sgd_solver.cpp:106] Iteration 46500, lr = 1e-007
I1101 08:21:50.106962 11248 solver.cpp:228] Iteration 46600, loss = 0.00250581
I1101 08:21:50.107463 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:21:50.107463 11248 sgd_solver.cpp:106] Iteration 46600, lr = 1e-007
I1101 08:22:07.134763 11248 solver.cpp:228] Iteration 46700, loss = 0.00330611
I1101 08:22:07.134763 11248 solver.cpp:244]     Train net output #0: loss = 0.00330611 (* 1 = 0.00330611 loss)
I1101 08:22:07.134763 11248 sgd_solver.cpp:106] Iteration 46700, lr = 1e-007
I1101 08:22:23.940933 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_46800.caffemodel
I1101 08:22:24.109557 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_46800.solverstate
I1101 08:22:24.228641 11248 solver.cpp:337] Iteration 46800, Testing net (#0)
I1101 08:22:29.249336 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:22:29.249336 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:22:29.319401 11248 solver.cpp:228] Iteration 46800, loss = 0.00321126
I1101 08:22:29.319901 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:22:29.319901 11248 sgd_solver.cpp:106] Iteration 46800, lr = 1e-007
I1101 08:22:46.206588 11248 solver.cpp:228] Iteration 46900, loss = 0.00390611
I1101 08:22:46.206588 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:22:46.206588 11248 sgd_solver.cpp:106] Iteration 46900, lr = 1e-007
I1101 08:23:03.099313 11248 solver.cpp:228] Iteration 47000, loss = 0.00493516
I1101 08:23:03.099313 11248 solver.cpp:244]     Train net output #0: loss = 0.00493516 (* 1 = 0.00493516 loss)
I1101 08:23:03.099313 11248 sgd_solver.cpp:106] Iteration 47000, lr = 1e-007
I1101 08:23:19.981184 11248 solver.cpp:228] Iteration 47100, loss = 0.00449708
I1101 08:23:19.981184 11248 solver.cpp:244]     Train net output #0: loss = 0.00449708 (* 1 = 0.00449708 loss)
I1101 08:23:19.981184 11248 sgd_solver.cpp:106] Iteration 47100, lr = 1e-007
I1101 08:23:36.874660 11248 solver.cpp:228] Iteration 47200, loss = 0.00250581
I1101 08:23:36.874660 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:23:36.874660 11248 sgd_solver.cpp:106] Iteration 47200, lr = 1e-007
I1101 08:23:53.783720 11248 solver.cpp:228] Iteration 47300, loss = 0.00330611
I1101 08:23:53.783720 11248 solver.cpp:244]     Train net output #0: loss = 0.00330611 (* 1 = 0.00330611 loss)
I1101 08:23:53.783720 11248 sgd_solver.cpp:106] Iteration 47300, lr = 1e-007
I1101 08:24:10.585012 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_47400.caffemodel
I1101 08:24:10.760215 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_47400.solverstate
I1101 08:24:10.866291 11248 solver.cpp:337] Iteration 47400, Testing net (#0)
I1101 08:24:15.892683 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:24:15.892683 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:24:15.963148 11248 solver.cpp:228] Iteration 47400, loss = 0.00321125
I1101 08:24:15.963148 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:24:15.963148 11248 sgd_solver.cpp:106] Iteration 47400, lr = 1e-007
I1101 08:24:32.857663 11248 solver.cpp:228] Iteration 47500, loss = 0.00390611
I1101 08:24:32.857663 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:24:32.857663 11248 sgd_solver.cpp:106] Iteration 47500, lr = 1e-007
I1101 08:24:49.735738 11248 solver.cpp:228] Iteration 47600, loss = 0.00493515
I1101 08:24:49.735738 11248 solver.cpp:244]     Train net output #0: loss = 0.00493515 (* 1 = 0.00493515 loss)
I1101 08:24:49.735738 11248 sgd_solver.cpp:106] Iteration 47600, lr = 1e-007
I1101 08:25:06.625120 11248 solver.cpp:228] Iteration 47700, loss = 0.00449707
I1101 08:25:06.625120 11248 solver.cpp:244]     Train net output #0: loss = 0.00449707 (* 1 = 0.00449707 loss)
I1101 08:25:06.625120 11248 sgd_solver.cpp:106] Iteration 47700, lr = 1e-007
I1101 08:25:23.512747 11248 solver.cpp:228] Iteration 47800, loss = 0.00250581
I1101 08:25:23.512747 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:25:23.512747 11248 sgd_solver.cpp:106] Iteration 47800, lr = 1e-007
I1101 08:25:40.433332 11248 solver.cpp:228] Iteration 47900, loss = 0.0033061
I1101 08:25:40.433332 11248 solver.cpp:244]     Train net output #0: loss = 0.0033061 (* 1 = 0.0033061 loss)
I1101 08:25:40.433332 11248 sgd_solver.cpp:106] Iteration 47900, lr = 1e-007
I1101 08:25:57.115170 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_48000.caffemodel
I1101 08:25:57.289295 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_48000.solverstate
I1101 08:25:57.398372 11248 solver.cpp:337] Iteration 48000, Testing net (#0)
I1101 08:26:02.395931 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:26:02.395931 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:26:02.466981 11248 solver.cpp:228] Iteration 48000, loss = 0.00321126
I1101 08:26:02.466981 11248 solver.cpp:244]     Train net output #0: loss = 0.00321126 (* 1 = 0.00321126 loss)
I1101 08:26:02.466981 11248 sgd_solver.cpp:106] Iteration 48000, lr = 1e-007
I1101 08:26:19.236918 11248 solver.cpp:228] Iteration 48100, loss = 0.00390611
I1101 08:26:19.236918 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:26:19.236918 11248 sgd_solver.cpp:106] Iteration 48100, lr = 1e-007
I1101 08:26:35.990844 11248 solver.cpp:228] Iteration 48200, loss = 0.00493515
I1101 08:26:35.990844 11248 solver.cpp:244]     Train net output #0: loss = 0.00493515 (* 1 = 0.00493515 loss)
I1101 08:26:35.990844 11248 sgd_solver.cpp:106] Iteration 48200, lr = 1e-007
I1101 08:26:52.763782 11248 solver.cpp:228] Iteration 48300, loss = 0.00449707
I1101 08:26:52.763782 11248 solver.cpp:244]     Train net output #0: loss = 0.00449706 (* 1 = 0.00449706 loss)
I1101 08:26:52.763782 11248 sgd_solver.cpp:106] Iteration 48300, lr = 1e-007
I1101 08:27:09.519711 11248 solver.cpp:228] Iteration 48400, loss = 0.00250581
I1101 08:27:09.519711 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:27:09.519711 11248 sgd_solver.cpp:106] Iteration 48400, lr = 1e-007
I1101 08:27:26.303656 11248 solver.cpp:228] Iteration 48500, loss = 0.0033061
I1101 08:27:26.303656 11248 solver.cpp:244]     Train net output #0: loss = 0.0033061 (* 1 = 0.0033061 loss)
I1101 08:27:26.303656 11248 sgd_solver.cpp:106] Iteration 48500, lr = 1e-007
I1101 08:27:42.981560 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_48600.caffemodel
I1101 08:27:43.147678 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_48600.solverstate
I1101 08:27:43.255755 11248 solver.cpp:337] Iteration 48600, Testing net (#0)
I1101 08:27:48.258316 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:27:48.258316 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:27:48.329368 11248 solver.cpp:228] Iteration 48600, loss = 0.00321125
I1101 08:27:48.329368 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:27:48.329368 11248 sgd_solver.cpp:106] Iteration 48600, lr = 1e-007
I1101 08:28:05.000234 11248 solver.cpp:228] Iteration 48700, loss = 0.00390611
I1101 08:28:05.000234 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:28:05.000234 11248 sgd_solver.cpp:106] Iteration 48700, lr = 1e-007
I1101 08:28:21.731142 11248 solver.cpp:228] Iteration 48800, loss = 0.00493515
I1101 08:28:21.731142 11248 solver.cpp:244]     Train net output #0: loss = 0.00493515 (* 1 = 0.00493515 loss)
I1101 08:28:21.731142 11248 sgd_solver.cpp:106] Iteration 48800, lr = 1e-007
I1101 08:28:38.491072 11248 solver.cpp:228] Iteration 48900, loss = 0.00449706
I1101 08:28:38.491072 11248 solver.cpp:244]     Train net output #0: loss = 0.00449706 (* 1 = 0.00449706 loss)
I1101 08:28:38.491072 11248 sgd_solver.cpp:106] Iteration 48900, lr = 1e-007
I1101 08:28:55.238993 11248 solver.cpp:228] Iteration 49000, loss = 0.00250581
I1101 08:28:55.238993 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:28:55.238993 11248 sgd_solver.cpp:106] Iteration 49000, lr = 1e-007
I1101 08:29:11.995921 11248 solver.cpp:228] Iteration 49100, loss = 0.0033061
I1101 08:29:11.995921 11248 solver.cpp:244]     Train net output #0: loss = 0.0033061 (* 1 = 0.0033061 loss)
I1101 08:29:11.995921 11248 sgd_solver.cpp:106] Iteration 49100, lr = 1e-007
I1101 08:29:28.693805 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_49200.caffemodel
I1101 08:29:28.861927 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_49200.solverstate
I1101 08:29:28.967572 11248 solver.cpp:337] Iteration 49200, Testing net (#0)
I1101 08:29:33.979140 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:29:33.979140 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:29:34.049191 11248 solver.cpp:228] Iteration 49200, loss = 0.00321125
I1101 08:29:34.049191 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:29:34.049191 11248 sgd_solver.cpp:106] Iteration 49200, lr = 1e-007
I1101 08:29:50.799325 11248 solver.cpp:228] Iteration 49300, loss = 0.00390611
I1101 08:29:50.799325 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:29:50.799325 11248 sgd_solver.cpp:106] Iteration 49300, lr = 1e-007
I1101 08:30:07.283354 11248 solver.cpp:228] Iteration 49400, loss = 0.00493515
I1101 08:30:07.283354 11248 solver.cpp:244]     Train net output #0: loss = 0.00493515 (* 1 = 0.00493515 loss)
I1101 08:30:07.283354 11248 sgd_solver.cpp:106] Iteration 49400, lr = 1e-007
I1101 08:30:23.705632 11248 solver.cpp:228] Iteration 49500, loss = 0.00449705
I1101 08:30:23.705632 11248 solver.cpp:244]     Train net output #0: loss = 0.00449705 (* 1 = 0.00449705 loss)
I1101 08:30:23.705632 11248 sgd_solver.cpp:106] Iteration 49500, lr = 1e-007
I1101 08:30:40.099105 11248 solver.cpp:228] Iteration 49600, loss = 0.0025058
I1101 08:30:40.099105 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:30:40.099105 11248 sgd_solver.cpp:106] Iteration 49600, lr = 1e-007
I1101 08:30:56.915786 11248 solver.cpp:228] Iteration 49700, loss = 0.00330609
I1101 08:30:56.915786 11248 solver.cpp:244]     Train net output #0: loss = 0.0033061 (* 1 = 0.0033061 loss)
I1101 08:30:56.915786 11248 sgd_solver.cpp:106] Iteration 49700, lr = 1e-007
I1101 08:31:13.426769 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_49800.caffemodel
I1101 08:31:13.583642 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_49800.solverstate
I1101 08:31:13.677585 11248 solver.cpp:337] Iteration 49800, Testing net (#0)
I1101 08:31:18.626531 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:31:18.626531 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:31:18.705328 11248 solver.cpp:228] Iteration 49800, loss = 0.00321125
I1101 08:31:18.705328 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:31:18.705328 11248 sgd_solver.cpp:106] Iteration 49800, lr = 1e-007
I1101 08:31:35.302148 11248 solver.cpp:228] Iteration 49900, loss = 0.00390611
I1101 08:31:35.302148 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:31:35.302148 11248 sgd_solver.cpp:106] Iteration 49900, lr = 1e-007
I1101 08:31:51.927495 11248 solver.cpp:228] Iteration 50000, loss = 0.00493515
I1101 08:31:51.927495 11248 solver.cpp:244]     Train net output #0: loss = 0.00493515 (* 1 = 0.00493515 loss)
I1101 08:31:51.927495 11248 sgd_solver.cpp:106] Iteration 50000, lr = 1e-007
I1101 08:32:08.789427 11248 solver.cpp:228] Iteration 50100, loss = 0.00449705
I1101 08:32:08.789427 11248 solver.cpp:244]     Train net output #0: loss = 0.00449705 (* 1 = 0.00449705 loss)
I1101 08:32:08.789427 11248 sgd_solver.cpp:106] Iteration 50100, lr = 1e-007
I1101 08:32:25.497737 11248 solver.cpp:228] Iteration 50200, loss = 0.00250581
I1101 08:32:25.497737 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:32:25.497737 11248 sgd_solver.cpp:106] Iteration 50200, lr = 1e-007
I1101 08:32:42.114367 11248 solver.cpp:228] Iteration 50300, loss = 0.00330609
I1101 08:32:42.114367 11248 solver.cpp:244]     Train net output #0: loss = 0.00330609 (* 1 = 0.00330609 loss)
I1101 08:32:42.114367 11248 sgd_solver.cpp:106] Iteration 50300, lr = 1e-007
I1101 08:32:58.911751 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_50400.caffemodel
I1101 08:32:59.076869 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_50400.solverstate
I1101 08:32:59.215467 11248 solver.cpp:337] Iteration 50400, Testing net (#0)
I1101 08:33:04.264763 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:33:04.264763 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:33:04.337234 11248 solver.cpp:228] Iteration 50400, loss = 0.00321125
I1101 08:33:04.337234 11248 solver.cpp:244]     Train net output #0: loss = 0.00321125 (* 1 = 0.00321125 loss)
I1101 08:33:04.337234 11248 sgd_solver.cpp:106] Iteration 50400, lr = 1e-007
I1101 08:33:21.078071 11248 solver.cpp:228] Iteration 50500, loss = 0.00390611
I1101 08:33:21.078071 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:33:21.078071 11248 sgd_solver.cpp:106] Iteration 50500, lr = 1e-007
I1101 08:33:37.821599 11248 solver.cpp:228] Iteration 50600, loss = 0.00493514
I1101 08:33:37.821599 11248 solver.cpp:244]     Train net output #0: loss = 0.00493514 (* 1 = 0.00493514 loss)
I1101 08:33:37.821599 11248 sgd_solver.cpp:106] Iteration 50600, lr = 1e-007
I1101 08:33:54.507294 11248 solver.cpp:228] Iteration 50700, loss = 0.00449704
I1101 08:33:54.507294 11248 solver.cpp:244]     Train net output #0: loss = 0.00449704 (* 1 = 0.00449704 loss)
I1101 08:33:54.507294 11248 sgd_solver.cpp:106] Iteration 50700, lr = 1e-007
I1101 08:34:11.375844 11248 solver.cpp:228] Iteration 50800, loss = 0.00250581
I1101 08:34:11.375844 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:34:11.375844 11248 sgd_solver.cpp:106] Iteration 50800, lr = 1e-007
I1101 08:34:27.993485 11248 solver.cpp:228] Iteration 50900, loss = 0.00330609
I1101 08:34:27.993485 11248 solver.cpp:244]     Train net output #0: loss = 0.00330609 (* 1 = 0.00330609 loss)
I1101 08:34:27.993485 11248 sgd_solver.cpp:106] Iteration 50900, lr = 1e-007
I1101 08:34:44.574164 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_51000.caffemodel
I1101 08:34:44.739282 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_51000.solverstate
I1101 08:34:44.838852 11248 solver.cpp:337] Iteration 51000, Testing net (#0)
I1101 08:34:49.914029 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:34:49.914029 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:34:49.984400 11248 solver.cpp:228] Iteration 51000, loss = 0.00321124
I1101 08:34:49.984400 11248 solver.cpp:244]     Train net output #0: loss = 0.00321124 (* 1 = 0.00321124 loss)
I1101 08:34:49.984400 11248 sgd_solver.cpp:106] Iteration 51000, lr = 1e-007
I1101 08:35:06.669190 11248 solver.cpp:228] Iteration 51100, loss = 0.00390611
I1101 08:35:06.669190 11248 solver.cpp:244]     Train net output #0: loss = 0.0039061 (* 1 = 0.0039061 loss)
I1101 08:35:06.669190 11248 sgd_solver.cpp:106] Iteration 51100, lr = 1e-007
I1101 08:35:23.240880 11248 solver.cpp:228] Iteration 51200, loss = 0.00493515
I1101 08:35:23.240880 11248 solver.cpp:244]     Train net output #0: loss = 0.00493514 (* 1 = 0.00493514 loss)
I1101 08:35:23.240880 11248 sgd_solver.cpp:106] Iteration 51200, lr = 1e-007
I1101 08:35:39.689999 11248 solver.cpp:228] Iteration 51300, loss = 0.00449703
I1101 08:35:39.689999 11248 solver.cpp:244]     Train net output #0: loss = 0.00449703 (* 1 = 0.00449703 loss)
I1101 08:35:39.689999 11248 sgd_solver.cpp:106] Iteration 51300, lr = 1e-007
I1101 08:35:56.336621 11248 solver.cpp:228] Iteration 51400, loss = 0.00250581
I1101 08:35:56.336621 11248 solver.cpp:244]     Train net output #0: loss = 0.0025058 (* 1 = 0.0025058 loss)
I1101 08:35:56.336621 11248 sgd_solver.cpp:106] Iteration 51400, lr = 1e-007
I1101 08:36:12.828269 11248 solver.cpp:228] Iteration 51500, loss = 0.00330608
I1101 08:36:12.828269 11248 solver.cpp:244]     Train net output #0: loss = 0.00330608 (* 1 = 0.00330608 loss)
I1101 08:36:12.828269 11248 sgd_solver.cpp:106] Iteration 51500, lr = 1e-007
I1101 08:36:29.314205 11248 solver.cpp:454] Snapshotting to binary proto file examples/mnist/simpnet_nodrp_iter_51600.caffemodel
I1101 08:36:29.471818 11248 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/simpnet_nodrp_iter_51600.solverstate
I1101 08:36:29.568387 11248 solver.cpp:337] Iteration 51600, Testing net (#0)
I1101 08:36:34.507084 11248 solver.cpp:404]     Test net output #0: accuracy = 0.9968
I1101 08:36:34.507084 11248 solver.cpp:404]     Test net output #1: loss = 0.0132087 (* 1 = 0.0132087 loss)
I1101 08:36:34.578656 11248 solver.cpp:228] Iteration 51600, loss = 0.00321124
I1101 08:36:34.578656 11248 solver.cpp:244]     Train net output #0: loss = 0.00321124 (* 1 = 0.00321124 loss)
I1101 08:36:34.578656 11248 sgd_solver.cpp:106] Iteration 51600, lr = 1e-007
I1101 08:36:51.135313 11248 solver.cpp:228] Iteration 51700, loss = 0.00390611
I1101 08:36:51.135313 11248 solver.cpp:244]     Train net output #0: loss = 0.00390611 (* 1 = 0.00390611 loss)
I1101 08:36:51.135313 11248 sgd_solver.cpp:106] Iteration 51700, lr = 1e-007
I1101 08:37:07.684113 11248 solver.cpp:228] Iteration 51800, loss = 0.00493514
I1101 08:37:07.684113 11248 solver.cpp:244]     Train net output #0: loss = 0.00493514 (* 1 = 0.00493514 loss)
I1101 08:37:07.684113 11248 sgd_solver.cpp:106] Iteration 51800, lr = 1e-007
I1101 08:37:24.188222 11248 solver.cpp:228] Iteration 51900, loss = 0.00449703
I1101 08:37:24.188222 11248 solver.cpp:244]     Train net output #0: loss = 0.00449702 (* 1 = 0.00449702 loss)
I1101 08:37:24.188222 11248 sgd_solver.cpp:106] Iteration 51900, lr = 1e-007
I1101 08:37:40.823774 11248 solver.cpp:228] Iteration 52000, loss = 0.00250581
I1101 08:37:40.824276 11248 solver.cpp:244]     Train net output #0: loss = 0.00250581 (* 1 = 0.00250581 loss)
I1101 08:37:40.824276 11248 sgd_solver.cpp:106] Iteration 52000, lr = 1e-007
I1101 08:37:58.157469 11248 solver.cpp:22^C