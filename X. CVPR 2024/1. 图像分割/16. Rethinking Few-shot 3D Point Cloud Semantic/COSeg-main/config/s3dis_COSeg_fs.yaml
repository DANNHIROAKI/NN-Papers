DATA:
  data_name: s3dis
  data_root: # Fill in the pre-processed data path (which contains the .npy files)
  classes: 13
  fea_dim: 6
  voxel_size: 0.02
  voxel_max: 20480
  loop: 1
  cvfold: 1

TRAIN:
  #arch
  arch: stratified_transformer
  stem_transformer: True
  use_xyz: True
  sync_bn: False  # adopt sync_bn or not
  rel_query: True
  rel_key: True
  rel_value: True
  quant_size: 0.01
  downsample_scale: 8
  num_layers: 4 
  patch_size: 1 
  window_size: 4
  depths: [2, 2, 6, 2] 
  channels: [48, 96, 192, 384] 
  num_heads: [3, 6, 12, 24] 
  up_k: 3
  drop_path_rate: 0.3
  concat_xyz: True
  grid_size: 0.04
  max_batch_points: 140000
  max_num_neighbors: 34 # For KPConv
  ratio: 0.25
  k: 16

  # training
  aug: True
  transformer_lr_scale: 0.1
  jitter_sigma: 0.005
  jitter_clip: 0.02
  scheduler_update: epoch 
  scheduler: MultiStep 
  warmup: linear
  warmup_iters: 1500
  warmup_ratio: 0.000001
  use_amp: True
  optimizer: AdamW 
  ignore_label: 255
  train_gpu: [0, 1, 2, 3]
  workers: 16  # data loader workers
  base_lr: 0.00005
  epochs: 100
  start_epoch: 0
  step_epoch: 30
  multiplier: 0.1
  momentum: 0.9
  weight_decay: 0.01
  drop_rate: 0.5
  manual_seed: 123
  # manual_seed:
  print_freq: 1
  save_freq: 1
  save_path: ./saved_models # Fill the path to store the trained model
  weight:  # path to initial weight (default: none)
  resume:  # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
  eval_freq: 1

Few-shot:
  num_episode: 400
  n_way: 1
  k_shot: 1
  n_queries: 1
  num_episode_per_comb: 1000
  pretrain_backbone:
  n_subprototypes: 100
  vis: 0

Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: True
  world_size: 1
  rank: 0

TEST:
  test: False
  eval_split: val # split for eval in [val or test], should be set to 'test' if test is True

