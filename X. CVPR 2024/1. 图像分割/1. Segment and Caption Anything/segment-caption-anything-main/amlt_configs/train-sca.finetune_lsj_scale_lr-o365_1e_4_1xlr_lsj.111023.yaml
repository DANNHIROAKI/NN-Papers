env_defaults:


  # NOTE: this kind of string leaded by > will append a new line to the end of the string
  SHARED_CMD_ARGS: >-
    -m src.train
    +model=base_sca_multitask_v2
    training.do_train=True
    training.do_eval=True
    training.do_inference=True
    +data.streaming=False
    training.max_eval_samples=800
    training.max_steps=100000
    training.fp16=True
    training.output_dir=$AMLT_OUTPUT_DIR
    training.output_log_dir=$AMLT_LOGS_DIR
    model.cache_dir=/mnt/blob/weights/.model.cache/
    training.save_strategy=steps
    training.save_steps=5000
    training.save_total_limit=3
    training.optim=adamw_torch
    training.evaluate_before_train=True
    training.per_device_train_batch_size=1
    training.evaluation_strategy=steps
    training.eval_steps=5000
    training.logging_steps=1000
    training.logging_first_step=True
    training.dataloader_num_workers=4
    training.num_masks_per_sample=16
    wandb.project=$AMLT_EXPERIMENT_NAME
    wandb.name=$AMLT_JOB_NAME
    model.num_caption_tokens=8
    model.additional_num_hidden_layers=12
    model.num_task_tokens=6
    training.lr_scheduler_type=cosine
    model.lm_head_model_name_or_path=gpt2-large
    training.learning_rate=1e-4
    training.weight_decay=1e-4
    training.warmup_steps=200
    training.warmup_ratio=0.33333333
    training.compute_metrics=True



environment:

  image: nvidia/pytorch:23.07-py3 # NCCL on PHLRR4076 cannot initialized successfully
  # image: nvidia/pytorch:23.06-py3  # NCCL on PHLRR4076 cannot initialized successfully
  # image: nvidia/pytorch:22.12-py3  # Pydantic has bug: https://github.com/pydantic/pydantic/issues/545#issuecomment-1573776471 pip install pydantic==1.10.8; not support adamw_torch_fused, as it requires PyTorch 2.0 or higher
  registry: nvcr.io

code:
  local_dir: $CONFIG_DIR/../



jobs:
  - name: gpt2-large
    preemptible: True
    sku: ${NUM_NODES}xG${NUM_GPUS}-V100-IB
    process_count_per_node: 1 # Each node should run 1 process
    command:
      - . amlt_configs/setup.sh
      - source ~/.bashrc
      - . amlt_configs/setup_accelerate_on_azure.sh
      - >-
        accelerate launch --config_file amlt_configs/accelerate_deepspeed_config.yaml 
        $SHARED_CMD_ARGS 
        train_data='[vg-densecap-region_descriptions]'
        eval_data='[vg-densecap-region_descriptions]'
        model.lm_head_model_name_or_path=gpt2-large
        $EXTRA_ARGS 


    submit_args:
      env:
        SHARED_MEMORY_PERCENT: 0.5
        HYDRA_FULL_ERROR: 1
        # NCCL_IB_DISABLE: 1
        # NCCL_IBEXT_DISABLE: 1
      container_args:
        shm_size: 256g

  - name: open_llama_3b_v2
    preemptible: True
    sku: ${NUM_NODES}xG${NUM_GPUS}-V100-IB
    process_count_per_node: 1 # Each node should run 1 process
    command:
      - . amlt_configs/setup.sh
      - source ~/.bashrc
      - . amlt_configs/setup_accelerate_on_azure.sh
      - >-
        accelerate launch --config_file amlt_configs/accelerate_deepspeed_config.yaml 
        $SHARED_CMD_ARGS
        train_data='[vg-densecap-region_descriptions]'
        eval_data='[vg-densecap-region_descriptions]'
        model.lm_head_model_name_or_path=openlm-research/open_llama_3b_v2
        training.gradient_checkpointing=true
        $EXTRA_ARGS


    submit_args:
      env:
        SHARED_MEMORY_PERCENT: 0.5
        HYDRA_FULL_ERROR: 1
        # NCCL_IB_DISABLE: 1
        # NCCL_IBEXT_DISABLE: 1
      container_args:
        shm_size: 256g

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.gpt2-large-lsj-1xlr/checkpoint-100000/ training.max_steps=100000 training.learning_rate=4e-4" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --no-pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.resrch-4x8-v100-16g-no_pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.ollm3bv2-large-lsj-1xlr/checkpoint-100000 training.max_steps=100000 training.learning_rate=4e-4" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --no-pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :1=`date +"%m%d%y"`.resrch-4x8-v100-16g-no_pre.fintune-ollmv2-lr_1e_4-1xlr-lsj train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.gpt2-large-lsj-1xlr/checkpoint-100000/ training.max_steps=100000 training.learning_rate=4e-4" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.resrch-4x8-v100-16g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.ollm3bv2-large-lsj-1xlr/checkpoint-100000 training.max_steps=100000 training.learning_rate=4e-4" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :1=`date +"%m%d%y"`.resrch-4x8-v100-16g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.gpt2-large-lsj-1xlr/checkpoint-100000/ training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itphcrdellcl1 --vc hcrdell1 --sku=5xG4-V100 --no-pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.dell-5x4-v100-32g-no_pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_2 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.ollm3bv2-large-lsj-1xlr/checkpoint-100000 training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itphcrdellcl1 --vc hcrdell1 --sku=5xG4-V100 --no-pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :1=`date +"%m%d%y"`.dell-5x4-v100-32g-no_pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_2 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.gpt2-large-lsj-1xlr/checkpoint-100000/ training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_2 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111123.ckpt_backup.o365-pretrain-1e_4-1xlr-lsj-100k_steps/train-sca-pretrain-o365-lsj-scale_lr-110923/110923.octo-4x8-v100-16g-no_pre.ollm3bv2-large-lsj-1xlr/checkpoint-100000 training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :1=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_2 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023


# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7301932201.25563-cd1e6021-6ea9-4835-8578-ba26f723a708/checkpoint-100000  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_2-o365_1e_4_no_lsj_bs_64 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7301932201.25563-cd1e6021-6ea9-4835-8578-ba26f723a708/checkpoint-100000  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=4xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.rr1-4x8-v100-32g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_2-o365_1e_4_no_lsj_bs_64 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023


# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111323.backup_ckpts.pretrain-o365-no_lsj-bs_64/train-sca-pretrain-o365-lsj-scale_lr-110923/111223.rr1-4x8-v100-32g-pre.gpt2-large-no_lsj-1xlr-bs_2/checkpoint-100000  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :0=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/weights/111323.backup_ckpts.pretrain-o365-no_lsj-bs_64/train-sca-pretrain-o365-lsj-scale_lr-110923/111223.rr1-4x8-v100-32g-pre.ollm3bv2-large-no_lsj-1xlr-bs_2/checkpoint-100000  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml :1=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64 train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228594.52312-e52ba12d-9e32-41d4-9630-e8c5d3e47ca0/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :1=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228580.84789-0b1216d8-79dc-46b3-8ef2-57c112e1bd18/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :0=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-gpt2_large-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# The o365 ollm3bv2 failed due to devices. try different clusters
# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228594.52312-e52ba12d-9e32-41d4-9630-e8c5d3e47ca0/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=1" \
# -t msrresrchvc -w msrresrchws --sku=8xG4-V100-IB --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :1=`date +"%m%d%y"`.resrch-8x4-v100-16g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228594.52312-e52ba12d-9e32-41d4-9630-e8c5d3e47ca0/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=1" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --no-pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :1=`date +"%m%d%y"`.resrch-4x8-v100-16g-no_pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228594.52312-e52ba12d-9e32-41d4-9630-e8c5d3e47ca0/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=1" \
# -t msrresrchvc -w msrresrchws --sku=4xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :1=`date +"%m%d%y"`.resrch-4x8-v100-16g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023

# amlt run -d "" --extra-args "+data_transforms=lsj-0_1-2_0 model.model_name_or_path=/mnt/blob/projects/sca-xiaoke-v3/amlt-results/7300228594.52312-e52ba12d-9e32-41d4-9630-e8c5d3e47ca0/checkpoint-200000/  training.max_steps=100000 training.learning_rate=4e-4 training.per_device_train_batch_size=2" \
# -t itplabrr1cl1 -w resrchvc --sku=2xG8-V100 --pre \
# amlt_configs/train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023.yaml \
# :1=`date +"%m%d%y"`.rr1-2x8-v100-32g-pre.fintune-ollmv2-lr_1e_4-1xlr-lsj-bs_32-o365_4e_4_no_lsj_bs_64_200k.2 \
# train-sca.finetune_lsj_scale_lr-o365_1e_4_1xlr_lsj.111023